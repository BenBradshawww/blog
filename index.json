[{"categories":["Machine Learning","Trees"],"contents":" Image by Bob Brewer\nSince the introduction of gradient tree boosting methods, tree boosting methods have often been the highest performing models across many tabular benchmarks. Alongside this, they have often be the winning models in many Kaggle competitions. Today we\u0026rsquo;ll cover the most popular of these models: XGBoost.\nXGBoost popularity stems from many reasons, with the most important being its scalability to all scenarios. It is one of the fastest tree based models to train because of the algorithms used with sparse data and it\u0026rsquo;s exploitation of parallel and distributed computing.\nThis article will follow a similar format to the XGBoost paper with the following sections being covered in this article:\nThe Regularised Objective. The Split Finding Algorithms. Constructing the Regularised Objective $$\\hat{y}_i = \\phi(x_i) = \\sum_{k=1}^K f_k(x_i), \\quad f_k \\in \\mathcal{F},$$$$\\text{Prediction}(x) = \\sum_{k=1}^K w_{k, q_k}(x)$$$$\\mathcal{L}(\\phi) = \\sum_i l(\\hat{y}_i, y_i) + \\sum_i \\Omega (f_k) \\quad \\text{where } \\ \\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\| w\\|^2.$$Here, $l$ is a differentiable convex loss function (such as the mean squared error) and $\\Omega$ is the complexity penaliser term to avoid overfitting. When the penalising term is removed, this objective function is the same as gradient boosting.\nGradient Tree Boosting Tree ensemble methods cannot be optimised by traditional optimisation methods in the Euclidean space because they are discontinuos functions and are not differentiable. Consequently, this prohibits the use of gradient based optimisation methods which rely on these two properties. As a result, models need to be trained in an additive manner.\n$$\\mathcal{L}^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t)} + f_t(x_i)) + \\Omega(f_t).$$$$\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^n [l(y_i, \\hat{y}_i^{(t)}) + g_if_t(x_i)) + \\frac{1}{2}h_if_t(x_i)^2] + \\Omega(f_t),$$$$g_i = \\frac{\\delta l(y_i, \\hat{y}_i^{(t-1)})}{\\delta \\hat{y}_i^{(t-1)} } \\quad \\text{and} \\quad h_i = \\frac{\\delta l(y_i, \\hat{y}_i^{(t-1)})}{\\delta^2 \\hat{y}_i^{(t-1)^2} }.$$$$\\mathcal{L}^{(t)} = \\sum_i^n [g_if_t(x_i)) + \\frac{1}{2}h_if_t(x_i)^2] + \\Omega(f_t).$$$$\\mathcal{L}^{(t)} = \\sum_{j=1}^T [(\\sum_{i \\in I_j}g_i)w_j) + \\frac{1}{2}(\\sum_{i \\in I_j}h_i + \\lambda) w_j^2] + \\lambda T.$$$$w_j^* = -\\frac{\\sum_{i \\in I_j}g_i}{\\sum_{i \\in I_j}h_i + \\lambda}.$$$$\\tilde{\\mathcal{L}}^{(t)}(q) = -\\frac{1}{2} \\sum_{j=1}^T\\frac{(\\sum_{i \\in I_j}g_i)^2}{\\sum_{i \\in I_j}h_i + \\lambda} + \\lambda T.$$ This function can be thought of as a quality measure of the tree structure $q$ and can be thought of as the impurity score.\nThe issue with this metric is that it becomes impossible to enumerate over all tree structures because any time a new branch is added, the number of potential configurations increases combinatorially. This approach is infeasible for any large dataset.\n$$\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \\frac{(\\sum_{i \\in I_L}g_i)^2}{\\sum_{i \\in I_L}h_i + \\lambda} + \\frac{(\\sum_{i \\in I_R}g_i)^2}{\\sum_{i \\in I_R}h_i + \\lambda} - \\frac{(\\sum_{i \\in I}g_i)^2}{\\sum_{i \\in I}h_i + \\lambda} \\right] - \\lambda.$$ This formula is used for evaluating split candidates.\nShrinkage and Column Subsampling In addition to the regularised objective, two further techniques were introduced by Tianqi Chen and Carlos Guestrin. The first method, is shrinkage introduce by Friedman where newly added weights are scaled by a factor $\\mu$ after each step of tree boosting. This reduces the influence of each individual tree and leaves spaces for future trees to improve the model.\nThe second method is column subsampling (which is also used in random forest) and this method often outperforms the row. sub-sampling method.\nSplit Finding Algorithms The basic split finding algorithm (which we will call the exact greedy method) which enumerates over all the possible splits on features is computationally demanding. This becomes even more of an issue when the dataset is large and does not fit into memory or if this model is trained in a distributive setting. ![Image][/images/algorithm_1_exact.png]\nAs a result, an alternative framework is proposed. This approximate algorithm proposes candidate split points based on percentiles of a feature distribution. The algorithm maps the continuous features into buckets split by the candidate split points, aggregates the statistics and finds the best solution among the proposals.\nThere are two variants of this proposal; the global variant which propose split points during the initial tree construction phase and then uses the same proposals for splits at each level. The local variant re-proposes after each split. The local proposal tends to outperform the global proposal, however, the global method can achieve a similar performance to the global proposal given enough candidate split points are given.\nMost approximate methods for distributed tree learning also follow this framework but some construct approximations based on histograms of the gradient statistic and some use other variants of binning strategies instead of quantile. But, from these methods, Tianqi Chen and Carlos Guestrin chose to go for the quantile strategy.\nWeighted Quantile Sketch The most important step in each approximate algorithm is to find the best candidate split points. Usually, percentiles of a feature distribution are used to make candidates distribute evenly on the data. This methods did not incorporate any form of weighting and treated each data point as equally important. The limitations of this method, is that they assume uniform importance, which typically is not the case. This alternative method incorporates the hessian information into its splitting.\n$$r_k(z) = \\frac{1}{\\sum_{(x, h) \\in D_k} h} \\sum_{(x, h) \\in D_k, x \u003c z} h,$$$$|r_k(s_{k,j}) - r_k(s_{k,j+1})| \u003c \\epsilon, \\quad s_{k1} = \\min_i x_{ik}, \\quad s_{kl} = \\max_i x_{ik}.$$ Where $\\epsilon$ is an approximation factor. This means that there is roughly $1/\\epsilon$ candidate points and here, each data point is weighted by the hessian $h_i$.\nWe use the hessian as the weighting because it captures the importance uncertainty of each data point within the optimisation setting:\nCurvature information. The Hessian captures the curvature of the loss function. A higher. curvature (large $h_i$) implies that the gradient at this point is changing more rapidly and the model should focus on these points because they may impact the model\u0026rsquo;s performance. Weight Squared Loss. The rewritten loss takes the form of: $\\sum_{i=1}^n \\frac{1}{2} h_i \\left(f_t(x_i) - \\frac{g_i}{h_i} \\right)^2 + \\Omega(f_t) + \\text{constant}$, which is a weighted square loss with labels $g_i/h_i$. The gradient is insufficient. The gradients $g_i$ indiciate the direction of optimisation but does not account for the magnitude or stability of the loss function. If only the gradient points were used as weights the model would be unable to distinguish between instances with similar gradients but different curvatures (uncertainty). This could lead to suboptimal splits that do not accurately capture the structure of the data. The algorithm for the approximate splitting algorithm is below. ![Image][/images/algorithm_2_approximate_split.png]\nSparsity-aware Split Finding In many of the real world problems, the input $x$ is sparse as a result of the presence of missing values, frequent zero entries, or artefacts a result of feature engineering and one hot encoding. To make an algorithm aware of this sparsity, Tianqi Chen and Carlos Guestrin proposed to add a default direction in each tree. When a sample is missing, the instance is assigned to that direction with the optimal direction learnt from the data.\nConclusion In todays article we covered the maths behind the leading tree boosting algorithm, XGBoost. In later articles we will cover the system design behind XGBoost and its end-to-end evaluations.\n","permalink":"https://www.mathstoml.com/xgboost/","tags":["Machine Learning","Trees","Boosting"],"title":"XGBoost"},{"categories":["Probability"],"contents":" Image by Vivek Kumar\nThe Question\nA couple chooses to keep having children till they have an equal number of boys and girls. How many children will they have on average? [1]\nIn this article we will cover 2 ways to solve this problem. First using a random walk and proof by contradiction. The second is Optiver\u0026rsquo;s solution to this problem which use Dyck\u0026rsquo;s path.\nSolution 1\nLet $X_t$ denote the random variable for which child has been born at time $t$.\nGiven the first child is already born at time 0. If the first child is a boy, then let $X_t$ take the value 1 if a boy is born at time t and let $X_t$ be -1 if a girl is born at time t. Similarly, if the first child is a girl, then $X_t$ will take the value 1 if a girl is born at time t and we will take the value -1 if a girl has been born at time t.\nUsing the random variable $X_t$, we can model this question as a 1D random walk starting at the point 1. Without loss of generality, we will assume the first child to be born is a boy. If the next child born is a boy, we go up one, otherwise we go down one.\n$$X_t = \\begin{cases} 1 \\quad \u0026w.p. 1/2 \\ \\ (\\text{if a boy}) \\\\ -1 \\quad \u0026w.p. 1/2 \\ \\ (\\text{if a girl})\\end{cases}$$The random walk will finish once we hit 0 (i.e. when we have had equal number of boys and girls).\n$$\\sum_{i=x}^{W_x} X_i = 0$$ Our goal is to determine $\\mathbb{E}[W_1]$, i.e. the number of children needed to have an equal number of boys and girls, given that our first child is a boy.\n$$\\begin{align*}\\mathbb{E}[W_1] \u0026= \\mathbb{P}[X_1 = -1] \\mathbb{E}[W_1 | X_1 = -1] + \\mathbb{P}[X_1 = 1] \\mathbb{E}[W_1 | X_1 = 1] \\\\ \u0026=\\frac{1}{2}(1 + \\mathbb{E}[W_0]) + \\frac{1}{2}(1 + \\mathbb{E}[W_2]) \\\\ \u0026= 1 + \\frac{1}{2}\\mathbb{E}[W_2]\\end{align*}$$Given you are at step 2, returning back to step 1 is equivalent to being at step 1 and returning to step 0, i.e., they are identically distributed.\n$$\\mathbb{E}[W_2] = \\mathbb{E}[W_1] + \\mathbb{E}[W_1]$$$$\\begin{align*}\\mathbb{E}[W_1] \u0026= 1 + \\frac{1}{2}(\\mathbb{E}[W_1] + \\mathbb{E}[W_1]) \\\\ \u0026= 1 + \\mathbb{E}[W_1]\\end{align*}$$ This is a contradiction! As a result, the expected number of children to have before you hit 0 is infinite.\nSolution 2\n$$\\mathbb{E}[X] = \\sum x . \\mathbb{P}[X = x]$$$$\\mathbb{E}[X] = \\sum_{x=2}^\\infty x . \\mathbb{P}[X = x]$$ To solve this formula, we need to find $\\mathbb{P}[X = x]$ for each value of $x$. To do this, we will model this question as a 2D random walk. Lets take $x=10$ as an example and we\u0026rsquo;ll use the grid below. Anytime the couple gives birth to a boy, a step to the right is taken and if a girl is birthed, a step up is taken. Equality is reached when the path hits the blue line.\n$$C_n = \\frac{1}{n+1} \\binom{2n}{n}.$$$$C_4 = \\frac{1}{5} \\binom{8}{4} = 14$$ Consequently, the number of paths that start with a boy and achieves equality on the 10th child is 14. By symmetry, the number of paths that start with a girl and achieves equality on the 10th child is also 14.\n$$\\mathbb{P}[X=10] = \\frac{14 + 14}{1024} = \\frac{28}{1024}.$$$$\\begin{align*} \\mathbb{P}[X = 2k] \u0026= \\frac{2 C_{k-1}}{2^{2k}} \\\\ \u0026= \\frac{2}{k}\\binom{2k-2}{k-1}\\frac{1}{2^{2k}} \\\\ \u0026= \\frac{2}{4^kk} \\binom{2k-2}{k-1}\\end{align*}$$$$\\begin{align*}\\mathbb{E}[X] \u0026= \\sum_{k=1}^\\infty 2k . \\mathbb{P}[X = 2k] \\\\ \u0026= \\sum_{k=1}^{\\infty} 2k \\frac{2}{4^kk} \\binom{2k-2}{k-1} \\\\ \u0026= \\sum_{k=1}^{\\infty} \\frac{1}{4^{k-1}} \\binom{2k-2}{k-1} \\end{align*}$$$$\\begin{align*}\\mathbb{E}[X] \u0026= \\sum_{n=0}^{\\infty} \\frac{1}{4^n} \\binom{2n}{n} \\end{align*}$$$$4^n = 2^{2n} = (1 + 1)^{2n} = \\sum_{m=0}^{2n} \\binom{2n}{m} \u003c (2n + 1) \\binom{2n}{n}$$$$\\frac{1}{4^n} \\binom{2n}{n} \u003e \\frac{1}{2n+1}$$$$\\begin{align*}\\mathbb{E}[X] \u0026= \\sum_{n=0}^{\\infty}\\frac{1}{4^n} \\binom{2n}{n} \\\\ \u0026\u003e \\sum_{n=0}^\\infty\\frac{1}{2n+1} \\\\ \u0026\u003e \\sum_{n=0}^\\infty \\sum{1}{2n} \\\\ \u0026= \\frac{1}{2} \\sum_{n=0}^\\infty \\frac{1}{n} \\\\ \u0026\\rightarrow \\infty \\end{align*}$$As a result, the expectation is infinite!\nReferences Optiver (2024). Prove it - Ep5: Perpetual Children. [online] YouTube. Available at: https://www.youtube.com/watch?v=YllN9Pz15cU [Accessed 22 Dec. 2024]. Weisstein, Eric W. “Dyck Path.” Mathworld.wolfram.com, 18 Dec. 2024, mathworld.wolfram.com/DyckPath.html. ","permalink":"https://www.mathstoml.com/perpetual-children/","tags":["Probability"],"title":"Perpetual Children"},{"categories":["Probability"],"contents":" Image by Cameron Cress\nThe Drunkard Suppose someone has had a heavy evening in the pub and upon leaving the pub they have somehow ended up by the side of a cliff. Since he has one a one too many drinks he can\u0026rsquo;t walk well and they now has 1/3 probability to walk off the edge of the cliff or 2/3 probability to take a step backwards. What is the probability that he will eventually fall off the edge of the cliff?\nTo determine the solution to this problem, we will denote $p_{1}$ as the probability he falls off the cliff given he is one step away, $p_{2}$ the probability he falls off the cliff given he is 2 steps away, etcetera.\nWriting $p_{1}$ in terms of the forward and backward steps gives:\n$$p_{1} = \\frac{1}{3} + \\frac{2}{3}p_{2}.$$Similarly, we could write $p_{2}$ as a function of $p_{1}$ and $p_{3}$ and use substitution. However, there will always remain an unknown term. To solve this question, we have to approach this question from another angle. From position 2 we would like to know the probability that the drunkard returns to $p_{1}$ but this is equivalent to the probability the drunkard is at position 1 and reaches position 0, i.e. the structure of this sub-question is the same, just shifted to the right by 1. Next, the probability of reach the cliff from position 1 is independent of the probability of reaching position 1 from position 2.\n$$\\begin{equation}p_{1} = \\frac{1}{3} + \\frac{2}{3}p_{2} = \\frac{1}{3} + \\frac{2}{3}p^{2}_{1}. \\end{equation}$$Solving this quadratic equation gives two solutions:\n$$p_{1} = \\frac{1 \\pm \\sqrt{1 -8/9}}{4/3} = \\frac{1 \\pm 1/3}{4/3},$$$$p_{1} = \\{\\frac{1}{2}, 1\\}$$Now we need to determine which of the two probabilities are correct. To do this, we\u0026rsquo;ll look at the general solution. Let $p$ denote the probability the drunkard takes a step towards the cliff and $(1-p)$ the probability the drunkard takes a step away from the cliff. Replacing $1/3$ with $p$ and $2/3$ with $(1-p)$ in (1) gives the quadratic equation:\n$$p_{1} = p + (1-p) p^{2}_{1}.$$Finding the solutions of this equation gives:\n$$p_{1} = \\frac{1 \\pm \\sqrt{1 - 4(1-p)p}}{2(1-p)},$$$$\\begin{equation}p_{1} =\\{ 1, \\frac{p}{(1-p)} \\},\\end{equation}$$In the case $p = 1$, equation (2) gives $p_{1} = \\{1, \\infty \\}$. Due to the axioms of probability (see appendix), $p_{1}$ must be 1 when $p=1$. If $p = 0$, equation (2) gives $p_{1} = \\{0, 1\\}$. Intuitively, 0 must be the correct solution here.\nIn general, we know for any 2D random walk, every point is revisited infinitely many times, so $p_{1} = 1$ for $p \\geq 1/2$.\nPlotting the probabilities for $p_{1}$ for a range of values for $p$ gives:\nConsequently, if the drunkard had $p = 1/3$, they would have a $1/2$ chance of not falling off the edge.\nN Points on a Circle Given N points are drawn randomly on the circumference of a circle, what is the probability they are all drawn within the same semicircle.\nFor this question, we will create a random variable called $A_i$ which denotes the event where all the other N-1 points except point i fall in the clockwise semi circle starting at point i.\nFor any index i, the probability that all other points fall in this clockwise semi-circle is $\\frac{1}{2^{N-1}}$.\nThe key observation of this problem is that the probability that event $A_i$ and $A_j$ occurs when $i \\neq j$ is 0 because these events are mutually exclusive. All the points in the clockwise direction of I cannot also fall in the clockwise direction of starting from point j. Consequently, these events must be mutually exclusive.\n$$\\mathbb{P}[\\cup_{I=1}^N A_i] = \\sum_{I=1}^N \\mathbb{P}[A_i] = N . 1/ 2^{N-1} = N / 2^{N-1}.$$This same argument also applies to arcs of less than 1/2 the circumference of the circle.\nTo prove this works, they the example we have two points. Intuitively, the probability both these points fall in the same semi-circle is 1. Applying the method discussed above will also provide an output of 1.\nCars on a Road Suppose the probability of observing one or more cars on the road during a 20 minute interval is 609/625. What is the probability of observing 1 or more cars in a 5 minute interval.\nLet $X_{0:20}$ denote the random variable for the number of cars during this 20 minute interval. Similarly, $X_{0:5}$, $X_{5:10}$, $X_{10:15}$, and $X_{15:20}$ be random variable from time 0 to 5, etc and $X_{0:20} = X_{0:5} + X_{5:10} + X_{10:15} + X_{15:20}$.\nFrom the problem question, we know that the probability of not observing a car during the 20 minute interval is $1 - \\frac{609}{625} = \\frac{16}{625}$. This event only occurs when intervals $X_{0:5} =0, X_{5:10}=0, X_{10:15} = 0,$ and $X_{15:20}=0$. Using the fact that each of these intervals are i.i.d.\n$$\\begin{align*}\\mathbb{P}[X_{0:20} = 0] \u0026= \\mathbb{P}[X_{0:5} + X_{5:10} + X_{10:15} + X_{15:20} = 0] \\\\ \u0026= \\mathbb{P}[X_{0:5}=0] * \\mathbb{P}[X_{5:10} = 0] * \\mathbb{P}[X_{10:15} = 0] * \\mathbb{P}[X_{15:20} = 0] \\\\ \u0026= \\mathbb{P}[X_{0:5}=0]^4 = \\frac{16}{625} \\end{align*}$$$$\\mathbb{P}[X_{0:5}=0] = \\frac{2}{5}$$$$\\mathbb{P}[X_{0:5}\\geq 1] = 1- \\mathbb{P}[X_{0:5} = 0] = \\frac{3}{5}$$","permalink":"https://www.mathstoml.com/some-questions-in-probability-i/","tags":["Probability"],"title":"Some Questions in Probability I"},{"categories":["Statistical Modelling"],"contents":" Image by Anh Vy.\nIn this article we will cover Generalised Additive Models (GAMs). We\u0026rsquo;ll cover the base case by modelling a response variable with a univariate smooth function. We\u0026rsquo;ll then build on this by incorporating multiple exogenous variables to create additive models. After then, the GAM can be covered.\nFor more details about this methods please read the book by Simon N. Wood about Generalised Additive Models.\nGAMs GAMS: GAMs are a form of generalised linear models with linear response variables that depend on unknown smooth functions of some exogenous variables. These forms of models were initially developed to blend the properties of Generalised Linear Models (GLMs) with additive models.\n$$g(\\mathbb{E}[Y]) = \\beta_0 + f_1(x_1) + ... + f_m(x_m)$$\nwhere $f_i$ are functions with a specified parametric forms (for example, $f_i$ may follow a polynomial or an unpenalised regression spline of a variable) or may be specified non-parametrically or semi-parametrically as simply smooth functions to be estimated in a non-parametric way. Due to the relaxed assumptions of the relationship between the response and the predictors, it provides a better fit to the data than the parametric models but with a loss of interpretability.\n$$f(x) = \\sum_{q=0}^{2n} \\Phi_q\\left(\\sum_{p=1}^n \\phi_{q, p}(x_p)\\right)$$\nThis theorem states a representation of this form exists but it provides no method to construct this function in practice. Proofs to find this representation are complex and often require the use of functions with a fractal structure. While fractals are mathematically interesting, they are not practical for models due to their complexity and computational complexity. Consequently, Kolmogorov-Arnold model based methods are not suitable for practical modelling. In modelling, you tend to focus on functions which are simple and easily interpretable. Fractal based methods are the opposite to this.\n$$f(x) = \\Phi \\left(\\sum_{p=1}^n \\phi_p (x_p)\\right)$$$$g(f(x)) = \\sum_i f_i (x_i).$$$$g(\\mathbb{E}[Y]) = \\beta_0 + f_1(x_1) + ... + f_m(x_m).$$Univariate Smooth Functions $$\\begin{equation}y_i = f(x_i) + \\epsilon_i\\end{equation}$$\nwhere $\\epsilon_i$ is a random variable that follows a normal distribution.\n$$f(x) = \\sum_{i=1}^q b_i(x) \\beta_i$$\nfor some scalar parameters $\\beta_i$. Substituting this equation into the previous one gives a linear function.\n$$f = \\beta_1 + \\beta_2x + \\beta_3x^2 + \\beta_4 x^3.$$\nPolynomial bases are useful in situations which focuses on the properties of $f$ in the vicinity of a single point but when the interest relates to $f$ over the whole domain, the polynomial bases underperform. In these situations, splines are recommend to be used due to theoretic properties.\nA univariate function can be expressed as a cubic spline. A cubic spline is a curve made up of sections of cubic polynomials joined together such that they are continuous in value in the first and second derivatives. The points which join the curves together are called the knots of the spline.\nIn a conventional spline, the knots are distributed at any point where a datum exists. For regression splines, the knots are evenly spaced through a range of observed x values or placed at quantiles of the distribution of the unique x values.\nGiven the knots locations ${ x_i^* : i=1, ...q-2 }$ there are many alternative ways of writing down the basis of a cubic spline. The most common basis takes the form $b_1(x) = 1, b_2(x)=x,$ and $b_{i+2} = R(x, x_i)$ for $i=1, ..., q-2$ where\n$$\\begin{align*}R(x, z) = \\left[(z-1/2)^2 - 1/12 \\right] \\left[ (x-1/2)^2 - 1/12\\right]/4 \\ - \\left[ (|x -z| - 1/2)^4 - 1/2 (|x-z| - 1/2)^2 + 7/240\\right].\\end{align*}$$\nIf a cubic spline is used as a basis for the function $f$, then the function for the response variable $y$ can be written as a linear model $\\textbf{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ where the ith row is expressed as\n$$X_i = [1, x_i, R(x_i, x_1^*), R(x_i, x_2^*), ..., R(x_i, x_{q-2}^*)].$$As a result, the parameters of this model can be estimated using the least squares:\n$$\\min\\boldsymbol{\\beta} |\\textbf{y} - X \\boldsymbol{\\beta}|^{2},$$to give us\n$$\\boldsymbol{\\beta} = (X^T X)^{-1} X^T Y.$$The next step is to choose a suitable basis dimension $q$. One way to go about choosing $q$ is with backwards selection, however, this method will not guarantee the dots will be evenly spaced at the end of the process. An alternative method suggests keeping the base dimension fixed and adding a \u0026ldquo;wigglinss\u0026rdquo; penalty to the least squares fitting objective giving us this minimisation problem:\n$$\\min_{\\boldsymbol{\\beta}} |\\textbf{y} - X \\boldsymbol{\\beta}|^{2} + \\lambda \\int_0^1 [f''(x)]^2dx,$$The tradeoff between the model fit and model smoothness is controlled by the smoothing parameter $\\lambda$. As $\\lambda \\rightarrow \\infty$, the model leads to a straight line estimate and at $\\lambda =0$ the regression spline will be not be penalised.\nSince $f$ is linear in the parameters $\\beta_i$ , the penalty can always be written as a quadratic form in $\\boldsymbol{\\beta}$.\n$$\\int_0^1[f'(x)]^2 dx = \\boldsymbol{\\beta}^T S \\boldsymbol{\\beta},$$where $S$ is a matrix of known coefficients. Rewriting our new least squares objective with this new term gives:\n$$\\min_{\\boldsymbol{\\beta}} |\\textbf{y} - X \\boldsymbol{\\beta}|^{2} + \\lambda \\boldsymbol{\\beta}^T S \\boldsymbol{\\beta},$$Finding an estimator for the parameters gives:\n$$\\boldsymbol{\\hat{\\beta}} = (X^T X + \\lambda S)^{-1} X^T \\textbf{y}.$$Now that we have formed an objective function to minimise and a formula for our estimator $\\boldsymbol{\\hat{\\beta}}$ , we need to find an effective way to find a suitable value for the parameter $\\lambda$. In an ideal situation, we will choose $\\lambda$ such that $\\hat{f}$ (our function) is as close as possible to the true function $f$. A suitable criterion we might then choose to minimise takes the form\n$$M = \\frac{1}{n} \\sum_{i=1}^n (\\hat{f}_i - f_i)^2,$$where $f_i = f(x_i)$ and $\\hat{f}_i = \\hat{f}(x_i)$. However, $f$ is unknown so $M$ cannot be used but it is possible to find an estimate for it. Define the ordinary cross validation (OCV) score as:\n$$\\mathcal{V}_0 = \\frac{1}{n} \\sum{i=1}^n (\\hat{f}_i^{[-i]} - y_i)^2,$$where $\\hat{f}_i^{[-i]}$ is the model fitted to all data points excluding datum $x_i$. This score fits each model with all the data except the datum $x_i$, then finds the difference between the missed out datum\u0026rsquo;s predicted and true value.\n$$\\begin{align}\\mathcal{V}_0 \u0026= \\frac{1}{n} \\sum{i=1}^n (\\hat{f}i^{[-i]} - f_i - \\epsilon_i)^2 \\ \u0026= \\frac{1}{n} \\sum{i=1}^n (\\hat{f}_i^{[-i]} - f_i)^2 - (\\hat{f}_i^{[-i]} - f_i)\\epsilon_i + \\epsilon_i^2\\end{align}$$Since $\\mathbb{E}[\\epsilon_i] = 0$ and $\\epsilon_i$ and $\\hat{f}_i^{[-i]}$ are independent, the second term vanishes when taking the expectation of the OCV score. This is only possible because we fitted our model without using datum $x_i$.\n$$\\mathbb{E}[\\mathcal{V}_0] = \\frac{1}{n} \\mathbb{E} \\left[ \\sum{i=1}^n (\\hat{f}_i^{[-I]} - f_i)^2 \\right] + \\sigma^2$$Since $\\hat{f}_i^{[-i]} \\approx \\hat{f}$ and under a large sample achieves equality, we can say $\\mathbb{E}[\\mathcal{V}_0] = \\mathbb{E}[M] + \\sigma^2$ and achieves equality with a large sample. Consequently, we need to choose a lambda that minimises $\\mathbb{E}[\\mathcal{V}_0]$.\nThe issue with this method comes with estimating $\\mathcal{V}_0$ by leaving out 1 datum at a time. Luckily, there is an alternative formula for the cross validation score:\n$$\\mathcal{V}_0 = \\frac{1}{n} \\sum{i=1}^n (y_i - \\hat{f}_i)^2 / (1-A_{ii})^2.$$where $\\hat{f}$ is the estimate fitting all the data, $A$ is the influence matrix.\nAlternatively, a generalised cross validation loss (GCV) can be constructed by\n$$\\mathcal{V}_g = \\frac{n \\sum{i=1}^n (y_i - \\hat{f}_i)^2}{[tr(I-A)]^2}$$The GCV has many computational advantages over the OCV and it retains the property of minimising the $\\mathbb{E}[M]$.\nTo minimise the OCV or the GCV with respect to $\\lambda$, numerical optimisation methods will need to be applied such as grid search and quasi-newton methods like BFGS.\nAdditive Models $$y_i = f_1(x_i) + f_2(z_i) + \\epsilon_i,$$\nwhere $f_j$ are smooth functions and $\\epsilon_i$ are i.i.d $N(0, \\sigma^2)$ random variables.\nBefore continuing, it is important to mention this model suffers from identifiability problems: $f_1$ and $f_2$ are only estimable to within an additive constant. This means that a constant could be added from $f_1$ and subtracted from $f_2$ without changing the model predictions. As a result, identifiability conditions must be applied to the model before fitting.\nGiven that the identifiability issue has been addressed, the additive model can be represented using a regression spline estimating the penalised least squares and the degree of smoothing can be estimated by cross validation.\n$$f_1(x) = \\delta_1 + x\\delta_2 + \\sum_{j=1}^{q_1 - 2} R(x, x_j^*) \\delta_{j+2}$$$$f_1(x) = \\gamma_1 + z\\gamma_2 + \\sum_{j=1}^{q_2 - 2} R(z, z_j^*) \\gamma_{j+2}$$where $\\delta_j$ and $\\gamma_j$ are unknown parameters, $x_j^*$ and $z_j^*$ are knot locations.\nThe identifiable problem with the additive model mean $\\delta_1$ and $\\gamma_1$ are confounded. The simplest way to solve this problem is by constraining one of the parameters to 0. Under this constraint, the model can be written as a linear model $\\textbf{y} = X \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ where the ith row of the matrix $X$ is written as:\n$$X_i = [1, x_i, R(x_i, x_1^*), R(x_i, x_2^*), ..., R(x_i, x_{q_1 - 2}^*), z_i, R(z_i, z_2^*), ..., R(z_i, z_{q_2 -2}^*)]$$with the parameter vector $\\boldsymbol{\\beta}$ written as $\\boldsymbol{\\beta} = [\\delta_1, \\delta_2, ..., \\delta_{q_1}, \\gamma_2, \\gamma_{q_2} ]^T.$\nAs we also did in the last section, the wiggliness of a function can be measured using:\n$$\\int_0^1 [f_1''(x)]^2dx = \\boldsymbol{\\beta}^T S_1 \\boldsymbol{\\beta}, \\quad \\text{and} \\quad \\int_0^1 [f_1''(x)]^2dx = \\boldsymbol{\\beta}^T S_2 \\boldsymbol{\\beta}.$$To fit the model, we need to minimise the penalised least squares objective.\n$$| \\textbf{y} - X \\boldsymbol{\\beta}|^2 + \\lambda_1 \\boldsymbol{\\beta}^T S_1 \\boldsymbol{\\beta} + \\lambda_2 \\boldsymbol{\\beta}^T S_2 \\boldsymbol{\\beta},$$where $\\lambda_1$ and $\\lambda_2$ are the smoothing parameters.\nGeneralised Additive Models GAMs follow from the additive models and take a similar form to generalised linear models where the response variables are known to follow some distribution from the exponential family or have a known mean variance relationship.\n$$log(\\mathbb{E}[y_i]) = f_1(x_1) + f_2(x_2) \\quad \\text{where} \\quad y_i \\sim \\text{Gamma}.$$\nWith this model, we cannot take the same approach as above. The additive model was fitted with the penalised least squares and the GAM will be fit by a penalised iterative least squares.\nThe reason behind the change in our objective criterion is because our the residuals of our model is no longer assumed to be normally distributed. Consequently the least squares is no longer the optimal criterion because it does not properly account for the variance in the distribution. This means that an alternative criterion needs to be used and this will be the maximised penalised likelihood. Often this takes the form of the log-likelihood because we assume that the residuals of the model come from an exponential family such as a Gaussian, Binomial, Exponential, and Poisson distribution.\nTo fit the model with a log-likelihood objective, the penalised iteratively re-weighted least squares (P-IRLS) scheme is run till convergence.\nGiven at the kth iteration, the parameters $\\boldsymbol{\\beta}^{[k]}$ has been generated and we have the mean response vector $\\boldsymbol{\\mu}^{[k]}$, calculate: $$w_i = \\frac{1}{V(\\mu_i^{[k]} ) g'(\\mu_i^{[k]})} \\text{ and } z_i = g(\\mu_i^{[k]})(y_i - \\mu_i^{[k]}) + X_i \\boldsymbol{\\beta}^{[k]}$$where $\\text{var}(Y_i) = V(\\mu_i^{[k]}) \\phi$, $V(\\mu_i^{[k]})$ denotes the variance function, $\\phi$ is the dispersion parameter, and $X_i$ is the ith row of $X$.\nMinimise $$| \\sqrt{W} (z - X\\boldsymbol{\\beta})|^2 + \\lambda_1 \\boldsymbol{\\beta}^T S_1 \\boldsymbol{\\beta} + \\lambda_2 \\boldsymbol{\\beta}^T S_2 \\boldsymbol{\\beta}$$\nw.r.t $\\boldsymbol{\\beta}$ to obtain $\\boldsymbol{\\beta}^{[k]}$. W is the diagonal matrix such that $W_{ii} = w_i$.\nWhenever we fit a GAM model to a dataset, we choose our link function $g$ and as a result, we know what form the variance function takes. Some common examples include:\nGaussian: $V(\\mu) = 1$ Poisson: $V(\\mu) = \\mu$ Binomial: $V(\\mu) = \\mu (1 - \\mu)$ Gamma: $V(\\mu) = \\mu^2$ Inverse Gaussian: $V(\\mu) = \\mu^3$ Conclusion Today we covered the basis for general additive modelling as an alternative method to general linear models.\nReferences Wood, S.N. (2017). Generalized Additive Models. CRC Press. ‌\n","permalink":"https://www.mathstoml.com/gams/","tags":["GAMs","GLMs"],"title":"GAMs"},{"categories":["Programming Tools"],"contents":" Image by Will Turner In this article, we will cover what containerisation means and we look at Docker, a containerisation platform. Furthermore, we will cover the key commands and concepts needed to create your own containers in docker.\nWhat is Docker? Docker is an open-source software platform that assists the deployment of applications. It does this by creating standardized units called containers which are isolated environments containing the application code, runtime, libraries, and any dependencies. These containers are a type of virtual machine that has an OS but does not simulate the entire computer. It is like a sandboxed environment.\nThe goal of containers is to provide portability, meaning you can run the same container on any computer environment. This even works with environments with a different OS and hardware.\nAnother advantage of these containers is their consistency. Since they are decoupled from the environment in which they are run, they are guaranteed to run the same, regardless of where they are deployed.\nContainers vs Virtual Machines A virtual machine (VM) [1] is another way of creating a virtual environment. A VM is a computer that lives inside a host machine and they are created by virtualising [appendix 1] the host machine\u0026rsquo;s underlying hardware and OS. The hardware is virtualised and then a portion of the hardware is used to run the VM.\nFigure 1. Virtual Machines. Source found at: https://endjin.com/blog/2022/01/introduction-to-containers-and-docker\nWhat sits between the host OS and the VM is a hypervisor layer. This is a piece of software that virtualises the host\u0026rsquo;s hardware and acts as the broker which manages the resources such as CPU, memory, and OS such that multiple different VMs can be run on the same hardware simultaneously. It is important to mention each VM may require different amounts of resources and the hypervisor is used to manage these resources.\nThe main disadvantage of a VM is its duplication. For each VM, a new OS is initialised which requires CPU, memory, and storage to run to create machine level isolation between each VM. However, this makes the VM slower to boot up and more inefficient with its resource management.\nContainers aim to solve this issue. The containers virtualise only the application and its dependencies rather than the whole OS. As a result, each container doesn\u0026rsquo;t need to have its own OS which makes them more lightweight than VMs.\nContainers are best thought of as processes on the host system with isolated environments.\nFigure 2. Containers. Source found at: https://endjin.com/blog/2022/01/introduction-to-containers-and-docker\nRather than using a hypervisor layer, the docker containers use a Docker daemon which acts as a broker between the host OS and containers. Docker Daemon chooses to use the host\u0026rsquo;s kernel directly to share the host OS with all containers. This process is more efficient than VMs and allows for rapid deployment.\nDocker Images Now that we know what docker containers are, you may wonder where the containers get their files and configurations. This is when docker images step in. These images are standalone packages which include all the files, binaries, libraries, and configs required to start running a container and at runtime, the images are converted to containers.\nHere are key principal\u0026rsquo;s of images [2]:\nImages are Immutable. This means that Docker images cannot be altered after they are created. You can think of an image as a snapshot of a file system at a particular point in time. If you want to make changes (like installing new software or modifying files), you cannot alter the existing image directly. Instead, you can create a new image with those changes. Container Images are composed of layers where each layer represents a set of file system changes. These layers stack on top of each other and each command in the Dockerfile (we\u0026rsquo;ll cover this later) creates a new layer in the image. For example, in a project you have a base layer consisting of installing python. The subsequent layer may copy the application code to the container and the final layer may install additional libraries from the requirements.txt. Many docker images can be found at the Docker Hub website. Docker hub contains thousands of trusted images such as pytorch, tensorflow, postgres, neo4j, python, wordpress images and many more.\nDuring your own projects, the base images may be insufficient. In this situation a Dockerfile is needed. A Dockerfile allows the user to create highly customisable environments tailored to the application\u0026rsquo;s needs. It allows the user to build an environment by specifying the OS, configuration files, environment variables, and more.\nLets create a Dockerfile which uses the default python image which adds the current application code to the container and installs the requirements.txt.\n# Base layer: Start with an official Python image from Docker Hub FROM python:3.9-slim # Set the working directory in the container WORKDIR /app # Copy only the requirements.txt file first to leverage Docker layer caching for dependencies COPY requirements.txt . # Install the necessary libraries specified in requirements.txt RUN pip install --no-cache-dir -r requirements.txt # Copy the rest of the application code into the container COPY . . # Specify the command to run the application CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] FROM: specifies the base image to be used. The remaining commands are then used to provide additional functionality to the base image. In general, this new image is called a child image. WORKDIR: sets the current the app directory as the working directory for the container. COPY: copies the requirements.txt file from the current directory to the current directory in the container. RUN: runs a set of instructions once the container is called to be built. CMD: specifies a default command to be run when the container is started. Using a Dockerfile allows the user to create a consistent environment that behaves the same on any machine in Docker.\nTo create a Docker Image from a DockerFile run the command:\ndocker build -t my_custom_image . -t \u0026lt;my_custom_image\u0026gt;: Tags the image with the name my_custom_image. .: specifies the build context. This enables Docker to look through the current directory for a Dockerfile. Now that the Docker Image has been made, the next step is to create the container.\ndocker run -d --name my_app_container -p 3000:3000 my_custom_image -d: runs the container in detached mode, allowing the container to run in the background. \u0026ndash;name: provides a name for the container. -p: maps your device\u0026rsquo;s port 3000 to the container\u0026rsquo;s port 3000. my_custom_image: specifies the image to be used to create the container. To check the container is running:\ndocker ps This will display a list of running containers.\nThere are a few other commands that are useful to know.\nTo display the logs of the running container run:\ndocker logs my_app_container To open the shell of the running container:\ndocker exec -it my_app_container /bin/bash -it: -i (the interactive flag) keeps the session open to accept inputs. -t (the TTY flag) allocates a pseudo-TTY, allowing the the terminal to be interactve. /bin/bash: specifies a command to be run inside the container, in our case /bin/bash opens a bash shell. To stop the container:\ndocker stop my_app_container To remove the container:\ndocker rm my_app_container Docker Volumes The final concept we will cover are volumes. Whenever a docker container is stopped all the information from that container is deleted. Volumes are a solution to this issue by providing a location to store the data outside the container on the host\u0026rsquo;s system (typically at \u0026lsquo;var/lib/docker/volume\u0026rsquo;). Now when the container is stopped, the information will not be deleted.\nSome of the situations where this feature comes in handy includes:\ndatabase storage. application data. data backups. There are a few different types of volumes within Docker.\nAnonymous Volumes. These are volumes created by Docker without specifying a name. They are typically created by Docker to store data that is only required during the lifetime of the docker container and are removed when the container is terminated.\ndocker run -v /app/data my_image docker run: starts a new container from an image (my_image). -v: specifies a volume to be mounted into the container. /app/data: is used as a shorthand for an anonymous volume. Without specifying a path on the host system docker will mount the volume /app/data directory in the container. Names Volumes. As the name implies, these volumes are created with a specific name and are used when a user wants the data to persists beyond the lifetime of the container. Since these are managed independently of the container, they are not deleted once the container is removed.\nTo create a volume run\ndocker volume create my_volume docker run -v my_volume:/app/data my_image -v: mounts a volume into the container. In our case, it maps the volume we made called my_volume to the /app/data directory in the container. Host Volumes (Bind Mounts). This class of volumes are not traditional volumes, rather they are grouped with volumes. Bind mounts directly mount a directory from the host system into the container. These volumes are used when we need them.\ndocker run -v /path/on/host:/app/data my_image Some of the other important docker volume commands include:\nTo list the docker volumes:\ndocker volume ls To inspect a volume:\ndocker volume inspect my_volume Conclusion In this article, we covered the basic of docker\u0026rsquo;s containerisation design. Additionally, we discussed how to create docker images and volumes.\nAppendix Virtualisation: To virtualise something means to create a virtual version of it. This includes creating virtual version of the server, storage, and OS. Rather than having 1 piece of hardware for each virtualisation, one physical resource can act as multiple independent resources. Kernel: A kernel [3] is a mediator between an application software and the hardware components. It is responsible for managing the computer\u0026rsquo;s resources such as CPU, RAM, storage, etc and it dictates when various resources are utilised across applications. Kernels manage all the aspects of a process by scheduling when they are to run and cleaning them up. References Mooney, L. (2022). Introduction to Containers and Docker | endjin. [online] endjin.com. Available at: https://endjin.com/blog/2022/01/introduction-to-containers-and-docker. Docker Documentation. (2024). What is an image? [online] Available at: https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-an-image/. Team, H. (2024). What is a Kernel \u0026amp; How Does it Work? [online] Hostwinds Blog. Available at: https://www.hostwinds.com/blog/what-is-a-kernel-and-how-does-it-work# [Accessed 11 Nov. 2024]. ‌\n‌\n","permalink":"https://www.mathstoml.com/docker/","tags":["Docker"],"title":"Docker"},{"categories":["Probability"],"contents":" Image by sq lim\nToday\u0026rsquo;s article will look at one of the more difficult areas in probability called martingale theory. We will cover the basic theory and look at 2 examples which use martingale theory (the drunk man and the ABRACADABRA problem).\nThe Theory Before jumping into the problem, we have to cover some of the theory behind martingales. It is important to mention we will be skipping over many of the measure theory definitions and theorems. These will be covered at a later time. If you wish to read up more about this area of mathematics, I\u0026rsquo;d recommend reading the paper by T. Smith here. This paper was the basis for this article [1].\nTo work with martingales, we will need to working within a filtered space.\nDefinition. A filtered space is defined on,\n$$(\\Omega, \\mathcal{F}, \\mathcal{F}_n, \\mathbb{P}),$$where $\\Omega$ is the sample space and consists of elements $\\omega$ which are called sample points. $\\mathbb{P}$ is called the probability measure where $\\mathbb{P}(\\Omega)=1$. $\\{ \\mathcal{F} : n \\geq 0 \\}$ is called a filtration. A filtration is an increasing family of sub-$\\sigma$-algebras of $\\mathcal{F}$ such that,\n$$\\mathcal{F}_0 \\subseteq \\mathcal{F}_1\\subseteq \\cdots \\ \\subseteq \\mathcal{F}_n$$and $\\mathcal{F}_\\infty$ is defined as\n$$\\mathcal{F}_{\\infty} = \\sigma (\\cup_n \\mathcal{F}_n ) \\subseteq \\mathcal{F},$$where $\\sigma$ is a $\\sigma$-algebra on $\\mathcal{F}$. A sigma algebra is a collection of events where the sample space itself is in $\\mathcal{F}$, it is closed under complement, and it is closed under countable unions.\nA filtration $\\mathcal{F}_n$ can be thought of as all the information available up to time $n$. A random variable $X_n$ is $\\mathcal{F}_n$ measurable if ${X_n \\leq x} \\in \\mathcal{F}_n$, i.e. $\\mathcal{F}_n$ contains all the previous information and the information at time $t$.\nDefinition. A process $X = (X_{n} : n \\geq 0)$ is adapted to the filtration $\\mathcal{F}_n$ if for each $n, X_n$ is $\\mathcal{F}_{n}$-measurable.\nThis means that any stochastic process $X_n(w)$ is known at time n and the stochastic process $X_n(w)$ is only dependent on the information we have up to and including time n and the process does not depend on any information in the future.\nNow we can cover what is a martingale.\nDefinition. A process $X$ is a martingale if:\n$X$ is adapted, $\\mathbb{E}[|X_{n}|] \u003c \\infty, \\forall n,$ $\\mathbb{E}[X_{n} | \\mathcal{F}_{n-1}] = X _{n-1}, \\text{ almost surely } (n \\geq 1).$ Definition. A supermartingale is defined the same as a martingale but with (3) replaced by\n$$\\mathbb{E}[X_n | \\mathcal{F}_{n-1}] \\leq X _{n-1} $$Definition. A submartingale is defined the same as a martingale but with (3) replaced by\n$$\\mathbb{E}[X_n | \\mathcal{F}_{n-1}] \\geq X _{n-1} $$Definition. A stopping time $T:\\omega \\rightarrow 0, 1, 2, \\cdots, \\infty $ is called a stopping time if\n$$\\begin{align*}\\{T \\leq n\\} \u0026= \\{w : T(w) \\leq n \\} \\in \\mathcal{F}_{n}, \\forall n \\leq \\infty, \\\\ \\text{or } \\{T =n\\} \u0026= \\{w : T(w) \\leq n\\} \\in \\mathcal{F}_{n}, \\forall n \\leq \\infty . \\end{align*}$$This means, for a stopping time $T$, it is possible to tell whether $\\{ T \\leq n \\}$ has occurred based on a filtration $\\mathcal{F}_n$. In other words, the event $\\{T \\leq n\\}$ is $\\mathcal{F}_n$-measurable.\nTheorem. If X is a super martingale and $T$ is a stopping time, then the stopped process $X^{T} = (X_{T \\wedge n}: n \\in \\mathbb{Z}^{+})$ is a supermartingale and\n$$\\mathbb{E}[X_{T \\wedge n}] = \\mathbb{E}[X_0], \\forall n.$$This does not imply that $\\mathbb{E}[X_T] = \\mathbb{E}[X_0]$.\nThe next theorem is the most important theorem for solving martingale based problems.\nTheorem (Doob\u0026rsquo;s Optional-Stopping Theorem). a) Let $T$ be a stopping time and let $X$ be a supermatingale. Then $X_T$ is integrable and\n$$\\mathbb{E}[X_T] \\leq \\mathbb{E}[X_0]$$in each of these 3 situations:\n$T$ is bounded. (for some $N \\in \\mathcal{N}, T(w) \\leq N, \\forall w$), $X$ is bounded (for some $K \\in \\mathbb{R}^{+}, |X_n(w)| \\leq K \\text{ for every } n \\text{ and every } w$), $\\mathbb{E}[T] \u003c \\infty$ and for some $K \\in \\mathbb{R}^{+}, |X_n(w) - X_{n-1}(w)| \\leq K, \\forall (n,w)$. $$\\mathbb{E}[X_T] = \\mathbb{E}[X_0].$$The results imply that the stopped process retains the expected outcome of that process.\nDefinition. A process $C = (C_n : n \\in N) $ is previsible if $C_n$ is $\\mathcal{F}_{n-1}$ measurable for $n \\geq 1$.\nThis means that the process\u0026rsquo;s value at any time can be determined based on the information up to that time but not including that time. They help define strategies that depend only on the current and past information.\nCorollary. Suppose $M$ is a martingale, the increments $M_n - M_{n-1}$ are bounded by constant $K_1$, $C$ is a previsible process bounded by constant $K_2$ and $T$ is a stopping time such that $\\mathbb{E}[T] \u003c \\infty$. Then\n$$\\mathbb{E}[\\sum_{1\\leq k \\leq T} C_k(M_k - M_{k-1})] = \\mathbb{E}[C . M]_T = 0.$$This means that assuming you have no knowledge of future events, you cannot beat a fair game.\nLemma. If $T$ is a stopping time where for some $N \\in \\mathbb{N}$ and some $\\epsilon \u003e 0$, we have, for every $n \\in \\mathcal{N}$:\n$$\\mathbb{P}[T \u003c n + N | \\mathcal{F}_n) \u003e \\epsilon, \\text{almost surely}.$$Then $\\mathbb{E}[T] \u003c \\infty$.\nThe Drunk Man A man had too much to drink and is standing on a bridge that is 100 meters long. He is currently at the $17_{\\text{th}}$ meter but has a tendency to either move forward or backward one meter with a $50 \\%$ chance for each step he takes. What are the chances that he reaches the end of the bridge (the $100_{\\text{th}}$ meter) before he returns to the beginning (the $0_{th}$ meter)? How many steps is he expected to take to reach either end of the bridge?\nTo solve this problem, martingale theory can be used. Assume the man starts at the point 0 and stops either at -17 or 83.\nLet $X_i$ denote the random variable modelling the direction of the step at time $i$ and let $S_n$ denote the sum of the random variables $X_i$ for $i \\in \\{1, 2, \\cdots, n\\}$. Specifically, $S_n$ is the position of the man at step $n$.\nWe can prove that the random variable $S_n$ and $S_n^{2} -n$ are martingales.\n$S_n$ is adapted because it is a sum of random variables $X_i$ for $i \\in \\{1, 2, \\cdots, n\\}$. The process $S_n$ is bounded by -17 and 83, i.e. $\\mathbb{E}[|S_n|] \\leq \\mathbb{E}[|S_T|] \\leq \\max(17, 83) = 83 \u003c \\infty$, where $T$ denotes the time that the sum of the steps $S_n$ hits -17 or 83. Finally, $\\mathbb{E}[S_{n+1} | \\mathcal{F}_n] = \\mathbb{E}[S_{n} + X_{n+1} | \\mathcal{F}_n] = \\mathbb{E}[S_{n}| \\mathcal{F}_n] + \\mathbb{E}[ X_{n+1} | \\mathcal{F}_n] = S_n + \\mathbb{E}[X_{n+1}] = S_n + (-1.(1/2) + 1. (1/2)) = S_n$. Now for $S_n^{2} -n$.\n$S_n^{2} -n$ is adapted as it is the squared sum of random variables $X_i$ for $i \\in \\{1, 2, \\cdots, n\\}$. For any time step n i.e. $\\mathbb{E}[|S_n^{2} - n|] \\leq \\mathbb{E}[S_n^{2}] + n \\leq 83^{2} + n \u003c \\infty$. Consequently, $\\mathbb{E}[|S_{n}^{2} - n|] \u003c \\infty, \\forall n$. Finally, $\\mathbb{E}[S_{n+1}^{2} - (n+1) | \\mathcal{F}_n] = \\mathbb{E}[(S_{n} + X_{n+1})^{2} | \\mathcal{F}_n] - (n+1) = \\mathbb{E}[S_{n}^{2}| \\mathcal{F}_n] + 2 \\mathbb{E}[ S_{n}X_{n+1} | \\mathcal{F}_n] +\\mathbb{E}[X_{n+1}^{2} | \\mathcal{F}_n] - (n+1) = S_n^2 + 2S_n\\mathbb{E}[X_{n+1}] + \\mathbb{E}[X_{n+1}^{2}] - (n+1)= S_n^2 + 2 S_n . 0 + 1 - (n+1)= S_n^2 - n$. c Each of these two martingales will be used to solve the first and second problem in this question.\nNow that we know both random variables are martingales we can now apply Doob\u0026rsquo;s Optional-Stopping Theorem. To do this, we\u0026rsquo;ll need to satisfy one of its 3 conditions. For the first martingale, we can satisfying condition 2, by seeing that the martingale is bounded by 83.\nApplying Doob\u0026rsquo;s Optional-Stopping Theorem for the second martingale is a bit more tricky. We will choose to prove condition 3 is satisfied. For any time K, if the man is at position $x$ on the bridge between -17 and 83, there exists a probability greater than zero that in the next $83-x$ or $-17-x$ steps the man will hit either -17 or 83. Consequently, $\\mathbb{E}[T] \u003c \\infty$.\nFor the second condition,\n$$\\begin{align*}|M_n - M_{n-1}| \u0026= |(S_n^2 - n) - (S_{n-1}^2 - (n-1))| \\\\ \u0026= |S_n^2 –S_{n-1}^2 + 1| \\\\ \u0026\\leq |S_n^2| + |S_{n-1}^2| + 1 \\\\ \u0026\\leq 2 . 83^2 + 1 \\\\ \u0026\u003c \\infty\\end{align*}$$Suppose at time step $T$ the drunk man has reached -17 or 83. To determine the probability he has reached 17 or 83 we will use the first martingale and Doob\u0026rsquo;s Optional-Stopping Theorem.\n$$\\mathbb{E}[S_T] = -17.(1-p) + 83.p = S_0 = 0$$Solving this equation gives $p=17/100$.\nTo find the expected number of steps we\u0026rsquo;ll use the second martingale and Doob\u0026rsquo;s Optional-Stopping Theorem again.\n$$\\mathbb{E}[S_T - T] = (-17)^{2}.(1-p) + 83^{2}.p - \\mathbb{E}[T] = S_0^{2} - 0 = 0.$$This gives $\\mathbb{E}[T] = 1411$.\nIn general, for any two points $\\alpha$ and $-\\beta$, $p_\\alpha = \\frac{\\alpha}{\\alpha + \\beta}$ and $\\mathbb{E}[N] = \\alpha \\beta$.\nThe ABRACADABRA Problem A monkey types letters at random, one per each unit of time, producing an infinite sequence of identically independent random letters. If the monkey is equally likely to type any of the 26 letters, how long on average will it take him to produce the sequence ABRACADABRA?\nTo solve this problem, we\u0026rsquo;ll need to model these sequence of events as a gambling game. Suppose for each time $n = 1, 2, 3, \\cdots$ a gambler arrives and makes a bet on which letter will be drawn. For the first bet, he bets £1 for the first letter A. If he wins, he will receive £26 but if he loses he gets £0 and must leave. In the situation he wins, the gambler will bet all his winnings (£26) on the next letter B. If he wins again he will receive £$26^{2}$ but if he loses he gets nothing and must leave. This process is repeated till we reach the sequence ABRACADABRA where the game will finish. We will choose $T$ to denote the time that a gambler has reached the sequence ABRACADABRA. This will correspond to the time taken for a monkey to produce the sequence ABRACADABRA.\nLet $M_n^j$ denote the martingale of the payoffs after the n bets for jth gambler. To prove $M_n^j$ is a martingale we must show\n$M_n^j$ is adapted. The payoff for the gambler at time $n$ is solely dependent of the letters drawn at and before time $n$ so the process must be adapted. $\\mathbb{E}[|M_n^j|] \u003c \\infty, \\forall n$. Well, $\\mathbb{E}[|M_n^j|] = \\mathbb{E}[M_n^j] \\leq 26^n \\leq \\infty$. Consequently, this condition must be satisfied. $\\mathbb{E}[M_n^j | F_{n-1}] = M_{n-1}^j$. For this there are two situations. First, the 4th gambler loses before time $n$. Then $\\mathbb{E}[M_n^j | F_{n-1}] = M_{n-1}^j$. Now that $M_n^j$ is a martingale, we would like to use Doob\u0026rsquo;s Optional-Stopping Theorem. However, applying Doob\u0026rsquo;s Theorem to martingale $M_n^j$ will lead us to a dead end. In general, to solve this problem, we\u0026rsquo;ll want to create a function which uses the random variable $T$ such that when we take the expectation, we will have a formula for $\\mathbb{E}[T]$. To do this, we\u0026rsquo;ll actually need to create a new martingale $X_n$ based on $M_n^j$.\n$$X_n = \\sum_{j=1}^n M_n^j.$$This new random variable can be thought of as the payoff for the $n$ gamblers who have been betting up to time $n$. Using the property that the sum of a martingale is a martingale. We can now try to apply Doob\u0026rsquo;s Optional-Stopping Theorem.\nTo do so, we need to satisfy one of its 3 conditions. Let\u0026rsquo;s choose the third one where $\\mathbb{E}[T] \\leq \\infty$ for some $K \\in \\mathbb{R}^{+}, |X_n(w) - X_{n-1}(w)| \\leq K, \\forall (n,w)$. From $K=11$, there exists the probability of the event occurring w.p. $(1/26)^{11}$ and in general, for any $K$ there exists a probability greater than 0 that the next 11 letters will form $ABRACADABRA$. Therefore the first condition is satisfied.\nNext, we\u0026rsquo;ll need to prove the second condition. Notice that,\n$$|X_n - X_{n-1}| \\leq 26^{11} + 26^4 + 26.$$This works because $|X_n - X_{n-1}|$ can be thought of as the payoff at time $n$. The maximum payoff that could occur, occurs at time $T$ when the letters $ABRACADABRA$ is formed. When this happens for the jth gambler, we will also have the j+7th gambler winning $26^4$ and the j+11th gambler winning 26 because each unit of time introduces a new gambler and that gambler must always bet on $A$, then $B$, then $R$ etc.\nNow that Doob\u0026rsquo;s Optional Stopping Theorem is attained. We can use $\\mathbb{E}[X_T] = \\mathbb{E}[X_0]$. At time step $T$, the players have gained $26^{11} + 26^4 + 26$ but they have lost $T$ because $T$ players have bet £1. Consequently,\n$$\\mathbb{E}[X_T] = 26^{11} + 26^4 + 26 - \\mathbb{E}[T] = 0.$$So, $\\mathbb{E}[T] = 26^{11} + 26^4 + 26$.\nConclusion This article briefly covered martingales and showed their application in solving two probabilistic problems.\nReferences: Smith, T. (n.d.). MARTINGALES AND A BASIC APPLICATION. [online] Available at: https://math.uchicago.edu/~may/REU2012/REUPapers/Smith.pdf [Accessed 12 Oct. 2024]. ","permalink":"https://www.mathstoml.com/martingale-theory/","tags":["Probability"],"title":"Martingale Theory"},{"categories":["Non-Convex Optimisation"],"contents":" Image by Joshua Sukoff One of the most crucial hyperparameters in any machine learning (ML) model is the learning rate. A small learning rate often results in longer training times and can lead to overfitting. Conversely, a large learning rate may accelerate initial training but risks hindering the model’s convergence to the global minimum and can even cause divergence. Therefore, selecting the appropriate learning rate is a critical step in training any ML model.\nOver the years there have been a host of methods used to find the ideal learning rate with each of these methods tailored to different settings. This article will focus on the learning rate strategies in the non-convex optimisation setting. These non-convex functions are what’s hidden behind the of all Deep Learning (DL) models used today such as ChatGPT, Claude, AlphaFold, and many more models. The objective in training any deep learning model is to find the parameters that best minimize the loss function. This requires finding the minimum of a non-convex function, which is challenging due to the presence of multiple local minima. Consequently, optimisation methods must be carefully designed to ensure convergence to a global minimum rather than getting trapped in a local minimum.\nFigure 1. Convex vs non-convex functions (Image from author)\nFixed Learning Rate A straightforward approach is to use a fixed step size, which often works well for simpler machine learning models like linear regression and logistic regression, where the loss function is convex. However, in deep learning, where models are typically non-convex, the training process can be much more unstable. As a result, a fixed learning rate is seldom sufficient in deep learning settings. Instead, a variable learning rate is almost always employed to better handle the complexities and instabilities of the training process.\nDiminishing Learning Rate There are several strategies for applying a variable learning rate. One common approach is to use a diminishing learning rate, where the learning rate decreases according to a fixed schedule over time. Well-known methods for diminishing learning rates include step decay, exponential decay, and polynomial decay.\nStarting with a high learning rate allows the model to explore the “loss landscape” more effectively, helping it to escape local minima and increase the likelihood of finding the global minimum. As the learning rate decreases over time, the model should ideally have explored enough of the loss landscape to identify the location of the global minimum.\nDecaying learning rates have been successfully applied to optimizers such as SGD and momentum-based methods. However, they come with several limitations, including:\nA learning rate that decays too quickly can leave the model with insufficient time to thoroughly explore the “loss landscape,” potentially causing it to converge to a local minimum with a suboptimal loss. Additionally, decaying learning rates involve several hyper-parameters, and the learning rate can be highly sensitive to some of these parameters. Finding the optimal settings often requires extensive experimentation and tuning, which can be a time-consuming process. Moreover, these methods are inflexible because they do not adapt dynamically to the current gradient information during training. This lack of adaptability can hinder their effectiveness in some scenarios. Adaptive Learning Rates This leads naturally to the next group of methods: adaptive gradient optimizers. These optimizers adjust their learning rates based on the gradients observed during training, making them less sensitive to the initial learning rate. They adapt the learning rates dynamically by incorporating gradient information into the calculation of the current step size. Classic deep learning optimisers such as Adam and RMSProp are prominent examples of this category.\nEach of these methods has its own advantages and disadvantages, which are beyond the scope of this discussion. However, it is highly recommended to explore these algorithms further, as they remain widely used in many deep learning applications. For instance, a variant of the Adam optimiser, known as AdamW, was employed to train the LLM model LLama3.\nCyclical Learning Rates Another common approach involves Cyclical Learning Rates (CLRs). Unlike traditional methods that monotonically decrease the learning rate over time, CLRs oscillate the learning rate between a minimum and maximum boundary value. High learning rates in this scheme help the optimizer escape local minima and explore the entire loss landscape, while lower learning rates assist in fine-tuning.\nDetermining the appropriate maximum and minimum learning rates is crucial when using cyclical learning rates. If the maximum learning rate is too high, the model may diverge, while a learning rate that is too low can result in slow convergence. To address this, CLRs are often paired with the Learning Rate Range Test (LRRT) to identify suitable boundary values. Additionally, incorporating momentum can help smooth out the oscillations caused by the high learning rates, enhancing convergence. Common CLR methods include Triangular and Triangular2.\nTo perform the Learning Rate Range Test (LRRT), start with a small learning rate and choose a maximum learning rate. Gradually increase the learning rate from the minimum to the maximum over a set number of epochs. By plotting the loss as the learning rate increases, you’ll typically observe a period where the loss decreases significantly before it begins to rise again due to the learning rate becoming too high. The minimum learning rate for CLRs is identified as the rate just before the sharp drop in loss, while the maximum learning rate is the rate just before the loss starts increasing again.\nCyclical Learning Rates (CLRs) were introduced by mathematician Leslie Smith and continue to be widely used. Smith also introduced other learning rate strategies, such as 1Cycle. Similar to CLRs, 1Cycle involves cyclic changes in the learning rate, but it uses a single cycle (hence the name). The 1Cycle method starts with a low learning rate to initialize training, gradually increases it to a maximum, and then decreases it to a value lower than the initial learning rate.\nMomentum plays a crucial role in the 1Cycle learning rate strategy. It starts high and decreases as the learning rate increases, encouraging the optimizer to explore the loss landscape. As the learning rate decreases, momentum is increased to help refine the convergence.\nFigure 2. Example of a 1Cycle Learning Rate Policy (Image by author)\nSimilar to CLRs, the 1Cycle method is sensitive to the choice of maximum and minimum learning rates, as well as the cycle length. To determine the appropriate learning rate range, the Learning Rate Range Test (LRRT) can be employed.\nConclusion\nThis article provided an overview of various learning rate strategies used in non-convex optimization, including diminishing learning rates, adaptive learning rates, Cyclical Learning Rates (CLRs), and 1Cycle.\nThere is no one-size-fits-all learning rate procedure; each has its own advantages and disadvantages. It is also crucial to recognize that the effectiveness of these methods can vary depending on the specific model, as each model’s loss landscape is unique. Therefore, some learning rate strategies may perform better or worse depending on the model they are applied to.\nReferences:\nSmith, L. (n.d.). Cyclical Learning Rates for Training Neural Networks. [online] Available at: https://arxiv.org/pdf/1506.01186. ‌Smith, L. (n.d.). A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 -LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY. [online] Available at: https://arxiv.org/pdf/1803.09820. Kingma, D. and Lei Ba, J. (2015). ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION. [online] Available at: https://arxiv.org/pdf/1412.6980. Ruder, S. (n.d.). An overview of gradient descent optimization algorithms *. [online] Available at: https://arxiv.org/pdf/1609.04747. ‌\n‌\n‌\n","permalink":"https://www.mathstoml.com/non-convex-optimisation-learning-rate-scheduling/","tags":["Optimisation"],"title":"Non-Convex Optimisation Learning Rate Scheduling"},{"categories":["Probability"],"contents":" Image by mi_shots Today we\u0026rsquo;ll be looking at a classic problem in probability: \u0026ldquo;The Ant Problem\u0026rdquo;.\nThe Question: An ant leaves its anthill in order to forage for food. It moves with the speed of 10cm per second, but it doesn\u0026rsquo;t know where to go, therefore every second it moves randomly 10cm directly north, south, east or west with equal probability.\na) If the food is located on east-west lines 20cm to the north and 20cm to the south, as well as on north-south lines 20cm to the east and 20cm to the west from the anthill, how long will it take the ant to reach it on average?\nb) What is the average time the ant will reach food if it is located only on a diagonal line passing through (10cm, 0cm) and (0cm, 10cm) points?\nc) Can you write a program that comes up with an estimate of average time to find food for any closed boundary around the anthill? What would be the answer if food is located outside an defined by ( (x – 2.5cm) / 30cm )2 + ( (y – 2.5cm) / 40cm )2 \u0026lt; 1 in coordinate system where the anthill is located at (x = 0cm, y = 0cm)? Provide us with a solution rounded to the nearest integer.\nThe question can be found here.\nSolutions: Part A: Let\u0026rsquo;s demonstrate the solution of this problem with probability.\nFigure 1.\nGiven that the ant starts at the origin, we can see from Figure 1 that the ant has four options: up, right, down, and left (corresponding to the red dots in Figure 1). Notice that taking any of these four routes places the ant in one of four equivalent positions. These positions are considered equivalent because, from any of them, the ant has the same possibilities for the next step. Similarly, if the ant lands on any blue points, there are three other equivalent positions within the grid where the possible next steps are exactly the same.\nLet $X_{\\text{black}}, X_{\\text{red}},$ and $X_{\\text{blue}}$ denote the random variables representing the number of steps required to reach the yellow box from the black, red, and blue points, respectively. These can also be expressed as:\n$$X_{\\text{black}} = 1 + X_{\\text{red}}.$$$$X_{\\text{red}} = \\begin{cases} 1 + X_{\\text{black}} \u0026 \\text{w.p. } \\frac{1}{4}, \\\\ 1 + X_{\\text{blue}} \u0026 \\text{w.p. } \\frac{1}{2}, \\\\ 1 \u0026 \\text{w.p. } \\frac{1}{4}. \\end{cases}$$$$X_{\\text{blue}} = \\begin{cases} 1 + X_{\\text{red}} \u0026 \\text{w.p. } \\frac{1}{2}, \\\\ 1 \u0026 \\text{w.p. } \\frac{1}{2}. \\end{cases}$$Taking the expectation of the 3 random variables gives:\n$$\\mathbb{E}[X_{\\text{black}}] = 1 + \\mathbb{E}[X_{\\text{red}}],$$$$\\mathbb{E}[X_{\\text{red}}] = 1 + \\frac{1}{4}\\mathbb{E}[X_{\\text{black}}] + \\frac{1}{2}\\mathbb{E}[X_{\\text{blue}}],$$$$\\mathbb{E}[X_{\\text{blue}}] = 1 + \\frac{1}{2}\\mathbb{E}[X_{\\text{red}}].$$Solving the above formulas by substitution gives:\n$$\\mathbb{E}[X_{\\text{black}}] = 4.5$$Therefore, on average it will take the ant 4.5 steps to reach the food.\nThis solution can be verified using the code from the last section.\nPart B: The premise behind this question is to model the ants 2D random walk as a 1D random walk.\nFigure 2.\nLet the yellow line denote where the food is located and let $X_{\\text{black}}, X_{\\text{red}}, X_{\\text{blue}}, X_{\\text{green}},$ and $X_{\\text{purple}}$ denote the random variables modelling the ants walks starting from the black, red, blue, green, and purple points.\n$$X_{\\text{black}} = \\begin{cases} 1 + X_{\\text{red}} \u0026 \\text{w.p. } \\frac{1}{2}, \\\\ 1 \u0026 \\text{w.p. } \\frac{1}{2}. \\end{cases}$$$$X_{\\text{red}} = \\begin{cases} 1 + X_{\\text{black}} \u0026 \\text{w.p. } \\frac{1}{2}, \\\\ 1 + X_{\\text{blue}} \u0026 \\text{w.p. } \\frac{1}{2}. \\end{cases}$$$$X_{\\text{blue}} = \\begin{cases} 1 + X_{\\text{red}} \u0026 \\text{w.p. } \\frac{1}{2}, \\\\ 1 + X_{\\text{green}} \u0026 \\text{w.p. } \\frac{1}{2}. \\end{cases}$$$$\\vdots$$Notice at each colour, the ant either moves 1 step further from the line w.p. 1/2 or 1 step closer to the line w.p. 1/2. Consequently, this 2 dimensional walk could be represented as 1D walk.\nFigure 3.\nThere are several ways to prove the expected number of steps, and we\u0026rsquo;ll use a proof by contradiction.\nSuppose that the average number of steps to move from the black point to the line exists and is equal to $x$. Next, we\u0026rsquo;ll construct an equation for $x$ based on the expected number of steps required to return from our starting point. Taking a step toward the yellow lines gives us 1 step, and from there, no further steps are needed. On the other hand, taking a step toward the red point initially adds 1 step. From the red point, we then need to find the expected number of steps to return to the black point and subsequently reach the yellow line. The expected number of steps from the red point to the black point is equivalent to the number of steps from the black point to the yellow line. From our initial assumption, we know that the expected number of steps for this is $x$. Similarly, starting from the black point, it takes, on average, another $x$ steps to reach the yellow line.\n$$X_{\\text{black}} = \\begin{cases} 1 + X_{\\text{red}} \u0026 \\text{w.p. } \\frac{1}{2}, \\\\ 1 \u0026 \\text{w.p. } \\frac{1}{2}. \\end{cases}$$$$\\begin{align*}\\mathbb{E}[X_{\\text{black}}] = x \u0026= \\frac{1}{2} (1) + \\frac{1}{2}(1 + \\mathbb{E}[X_{\\text{red}}]) \\\\ \u0026= 1 + \\frac{1}{2}(x + x) \\\\ \u0026= 1 + x \\end{align*}$$This leads to a contradiction. Consequently, there cannot be a finite average number of steps to hit the yellow line.\nThe original proof by contradiction can be found here and this solution will be verified in the last section.\nPart C: For this section we will be using python. We\u0026rsquo;ll create a base class and 3 separate subclasses to verify solutions for the last 3 questions. Each subclass has a custom function called \u0026ldquo;check\u0026rdquo; which checks if there is food located at the ant\u0026rsquo;s current location.\nclass base_ants: def __init__(self, n_iters=100000, max_iters=2000): self.n_iters = n_iters self.max_iters = max_iters def estimate(self): walk_storage = [self.walk((0,0), 0) for _ in range(self.n_iters)] return round(sum(walk_storage) / self.n_iters, 1) def check(self, pos): pass def walk(self, pos, count): if count \u0026gt;= self.max_iters: return float(\u0026#39;inf\u0026#39;) x, y = pos dx, dy = random.choice([(0,10), (10,0), (0,-10), (-10,0)]) nx, ny = x + dx, y + dy if self.check((nx, ny)): return count + 1 else: return self.walk((nx, ny), count + 1) class ants_q1(base_ants): def __init__(self): super().__init__() def check(self, pos): x, y = pos return abs(x) \u0026gt;= 20 or abs(y) \u0026gt;= 20 class ants_q2(base_ants): def __init__(self): super().__init__() def check(self, pos): x, y = pos return (x + y) == 10 class ants_q3(base_ants): def __init__(self, ellipse_center, horizontal_radius, vertical_radius): super().__init__() self.ellipse_center = ellipse_center self.horizontal_radius = horizontal_radius self.vertical_radius = vertical_radius def check(self, pos): x, y = pos x0, y0 = self.ellipse_center ellipse_eq = ((x - x0) / self.horizontal_radius) ** 2 + ((y - y0) / self.vertical_radius) ** 2 return ellipse_eq \u0026gt;= 1 Running the ants_q3 class gives an answer of 14.0.\nReferences: Random (2021). Random walk 1D with a single bound. [online] Mathematics Stack Exchange. Available at: https://math.stackexchange.com/questions/4320314/random-walk-1d-with-a-single-bound [Accessed 28 Aug. 2024].\n‌\n","permalink":"https://www.mathstoml.com/the-ant-problem/","tags":["Probability"],"title":"The Ant Problem"},{"categories":["Programming Tools"],"contents":" Image by Ante Hamersmit\nWhat are APIs Before we just into the class of Rest APIs it is best to understand what is an API. An API is simply an acronym for application programming interface, a software intermediary that allows two applications to talk to each other.\nThey are usually explained in terms of a client and a server. The application that sends a request is called the client and the application that sends the response is called the server.\nIn general when we refer to an API, we often are referring to web APIs. These APIs are accessed via a HTTP protocol (see the appendix).\nAPIs fall into 4 different categories [1]:\nPublic APIs: These APIs are available to anyone to use with little to no restriction through registration and authentication. These includes services such as the Spotify API, OpenWather API, and Google Maps API. Internal (Private) APIs: These APIs are designed for use by a closed group of API consumers. These often have much greater security than public APIs. Partner APIs: These sit between private and public APIs. These often share data between two companies. Open APIs: Most Public APIs follow the OpenAI standard which provides guidelines on endpoint naming, data formats, and error messaging. API Protocals Every API has a protocol. This defines how your API will connect to internet and how it will communicate information. There are many different type each with their own set of advantages and drawbacks.\nSOAP APIs: Simple Object Access Protocol (SOAP) is another API protocol which can connect over TCP and SMTP alongside HTTP. As a result, it is more flexible than REST, however it is also more restrictive because it only works with XML data and has more rigid requirements for requests. One advantage of SOAP is its requirement for metadata about the files in the request which results in a standardised protocol able to communicate complex data easily. Consequently, SOAP tends to be a good fit for large and sophisticated applications where reliability takes priority. RPC APIs: Remote Procedure Call (RPC) protocols return either XML or JSON responses. RPC calls a method rather than a data source and RPC returns confirmation on whether the function was triggered. RPC API can be thought of as a method to create actions. GraphQL APIs: This one isn\u0026rsquo;t really a specific protocol, however, it has a distinct query language. In essence, GraphQL APIs provide clients with higher amounts of flexibility with each query but to achieve this higher flexibility, extensive documentation is needed. REST APIs: We\u0026rsquo;ll cover these below. REST APIs The Representational State Transfer (REST) protocol [3] is the most well known protocol. REST simply defines routes with a URL rather than typically having to wrap routes with XML (see the appendix).\nAPIs that follow the REST architecture style are called REST APIs and the web services that implement REST architectures are called RESTful web services.\nWhat are the key features of the REST architecture? Uniform Interface. Any RESTful web-service must transfer information in a standard format. There are 4 structural constraints on the request. The request must identify resources. Clients should have enough information in the resuource representation to modify or delete the resource if need be. Meta data must be attached to the received information to help the client process the information. The client must also receive information about all other related resources to finish their task. Statelessness. Every request is completed independently of all previous request. Layered System. The client can connect to intermediaries between the client and server and still be able to receive responses from the server. Cacheability. The RESTful web services must support caching where some responses are stored on the clients or intermediary to improve server response time. Code on demand. In the REST architecture, servers can transfer software programming code to the client. What are the benefits? There are 3 main benefits of REST APIs.\nScalability. RESTful APIs scale efficiently because REST optimises client-server interactions. The REST APIs stateless and well managed caching can reduce load on the server. Flexibility. RESTful web services support client-server separation. Consequently, server and client changes will not require changes to the API. Independence. REST APIs are independent of the technologies they use. Both the client and server application can change without affecting the API design. How does it work? In general, a rest API follows 4 steps.\nThe client sends a request to the server. The server authenticates the request. The server receives and processes the request. The server returns the response to the client. What consists of a Restful API Client Request? Each client request consists of a Uniform Resource Loader (URL) which specifies the path to the resource. This is similar to the website address to enter any browser to visit any webpage and the Restful URL is referred to as the request endpoint.\nSince the Restful APIs are implemented with HTTP, the HTTP method will tell the server what to do with the online resource. There are 4 different methods which can be used:\nGET: Clients use GET to access resources at the URL. POST: Clients use POST to send data to the server. PUT: Clients use PUT to update resources on a server. DELETE: Clients use DELETE to delete resources from the server. Finally, the client request also consists of HTTP headers which consists of the metadata exchanged between the client and server. This can include information about the format of the request and response, information about the request status, and more. Some HTTP headers include data for the POST and PUT HTTP methods. While other headers consist of a list of parameters for what the server needs to do.\nHow are Restful APIs verified? Any web service which uses Restful APIs need to authenticate the requests to verify the identity of the individual who has sent the request. Typically, there are 4 ways to do this.\nBasic authentication: Here the client sends a username and password in the request header. These details are encoded with base64 for a safe transmission. Bearer authentication: Bearer authentication refers to process of giving access to the token bearer. The token bearers token is usually a string of characters. API keys: Here the server assigns a unique generate value for a first-time client which is used to verification. API keys are less secure because the client has to transmit the key which leaves them vulnerable to network theft. OAuth: OAuth combines passwords and tokens for a highly secure login. The server will identify the first request from the passwords and then will ask for additional tokens for authorisation. What does the RESTful request contain? REST responses consists of:\nA status line to communicate whether the communication was a success or failure. Some common status codes include 200 (generic success response), 201 (POST method success response), 400 (incorrect request the server cannot process), 404 (resource not found). A message body which contains the resource. Clients can request information in an XML or JSON format. Headers or metadata which provides additional information about the response. Appendix HTTP: A Hypertext Transfer Protocol (HTTP) is used to load webpages using hyperlinks. It is designed to transfer information between network devices.\nInternet browsers use HTTP requests as communications platforms to ask for the information needed to load a website. Each HTTP request carries encoded data that carries different types of information, typically this includes\nAn HTTP version type. A URL. An HTTP method. An HTTP request header. An optional HTTP body. XML: XML is a software and hardware independent tool for storing or transferring data [2]. In essence, XML is data wrapped around tags.\n\u0026lt;events\u0026gt; \u0026lt;event\u0026gt;sale\u0026lt;/event\u0026gt; \u0026lt;user\u0026gt;Jani\u0026lt;/user\u0026gt; \u0026lt;item\u0026gt;123456789\u0026lt;/item\u0026gt; \u0026lt;time\u0026gt;11:59:03\u0026lt;/time\u0026gt; \u0026lt;/events\u0026gt; XML looks similar to HTML, however they were designed with different goals. HTML is designed to display data, focusing on how the data looks while XML is designed to carry data.\nThe key difference between the two is that HTML has predefined tags while XML does not.\nConclusion This article introduces the idea of web APIs and briefly covered many of their variants before focussing on the REST API.\nReferences Stoplight. (n.d.). Types of APIs | Types Of API Calls \u0026amp; REST API Protocol. [online] Available at: https://stoplight.io/api-types. w3schools (2015). XML Introduction. [online] W3schools.com. Available at: https://www.w3schools.com/xml/xml_whatis.asp. Amazon Web Services (2024). What is RESTful API? - RESTful API Beginner’s Guide - AWS. [online] Amazon Web Services, Inc. Available at: https://aws.amazon.com/what-is/restful-api/. ‌\n‌\n‌\n","permalink":"https://www.mathstoml.com/rest-apis/","tags":["REST APIs"],"title":"Rest APIs"},{"categories":["Time Series"],"contents":" Image by Elijah Grimm In this article we will cover the ARMA and ARIMA based statistical models, cover the maths behind these models, and look at the statistical test required prior and the post the model fitting.\nI\u0026rsquo;d recommend reading the previous article here before reading the remainder of this article. In that article we cover the basics of what is a time series and how we could model it.\nDefinitions: Recall from our previous article that given a sequence of data points $\\{ X_{t}, t \\in \\mathbb{Z} \\}$, the sequence is strongly stationary or strictly stationary if\n$$\\left( X_{t_{1}}, \\dots, X_{t_{k}}\\right) \\overset{d}{=} \\left( X_{t_{1} + h}, \\dots, X_{t_{k} + h}\\right),$$for all the time points $t_{1}, \\dots, t_{k}$ and integer $h$ [1]. A sequence is weakly stationary or second order stationary if\n$$\\begin{aligned} \\mathbb{E}(X_{t}) \u0026= \\mu \\\\ \\text{and} \\quad \\text{Cov}(X_{t}, X_{t+k}) \u0026= \\mathbb{E}[(X_{k} - \\mu)(X_{t+k} - \\mu)] = \\gamma_{k}, \\end{aligned}$$where $\\mu$ is constant and $\\gamma_{k}$ is independent of $t$ and the sequence $\\{ \\gamma_{k}, k \\in \\mathbb{Z} \\}$ is called the autocovariance function. At the lag 0, $\\gamma_{0} = \\mathbb{E}[(X_{k} - \\mu)^{2}] = \\text{Var}(X_{k})$.\nIn general, a process is $X_t$ is stationary if the autocovaraince function $\\gamma_k$ does not depend on $t$.\nThese definitions will be useful later in the article.\nIndeterministic Processes There are many other processes in time series other than stationarity, one of these processes are indeterministic processes. A process is purely-indeterministic if the regression on $X_{t-q}, X_{t-q-1}, \\cdots$ has explanatory power tending to 0 as $q \\rightarrow \\infty$, i.e. the ability for the regressors to explain the variable $X_t$ diminishes to 0. Alternatively, this can be expressed as the residual variance of the series tends to $X_t$.\nThe key feature of these processes is that no matter how much past data is acquired, there will always be a sense of randomness in the data.\nIn any forecasting in the real world, there will alway be a random element introducing uncertainty into the model. ARMA based models account for this randomness by incorporating error terms which we will now cover below.\nARMA Given a stationary process $\\{X_t\\}$, an autoregressive moving average process ARMA(p, q) is defined as\n$$X_t - \\sum_{r-1}^p \\phi_r X_{t-r} = \\sum_{s=0}^{q} \\theta_s \\epsilon_{t-s},$$where $\\{ \\epsilon_t \\}$ is white noise. The parameter p corresponds to the autoregressive component where the current value depends on its previous p values and the parameter q corresponds to the moving average component which is influenced by the last q noisy terms.\nFinding the parameters p and q of the ARMA(p, q) model can be found by plotting the partial autocorrelation functions (PACF) and autocorrelation functions (ACF) respectively. Alternatively, both of these parameters can be determined using the extended autocorrelation function (EACF) which excels in situations where the parameters indicated by the PACF and ACF plots fail to provide clear indication on which parameters should be used.\nPACF\nGiven a time series $X_t$, the partial autocorrelation of lag k, denoted as $\\phi_{k,k}$ is the autocorrelation between $X_t$ and $X_{t+k}$ with $X_{t+k-1}$ removed. In other words, it is the autocorrelation between $X_t$ and $X_{t+k}$ which is not accounted by lags 1 to k-1. This stems from the idea that the terms $X_t$ and $X_{t-k}$ are indirectly dependent on $X_{t-1}, X_{t-2}, \\cdots, X_{t-k+1}$ and if they are not removed, the partial autocorrelated will not capture the true correlation between $X_t$ and $X_{t+k}$.\n$$\\phi_{1,1} = corr(X_{t+1}, X_t), \\quad \\text{for } k= 1, ...$$$$\\phi_{k,k} = corr(X_{t+k} - \\hat{X}_{t+k}, X_t) - \\hat{X}_t, \\quad \\text{for } k \\geq 2,$$where $\\hat{X}_{t+k}$ and $\\hat{X}_{t}$ are linear combinations of $\\{ X_{t+1}, X_{t+2}, \\cdots, X_{t+k-1}$ that minimise the mean square error of $X_{t+k}$ and $X_t$ respectively. Specifically,\n$$\\hat{X}_{t+k} = \\beta_1 X_{t+k-1} + \\cdots + \\beta_{k-1} X_{t+1} \\quad \\text{and} \\quad \\hat{X}_t = \\beta_1 X_{t+1} + \\cdots + \\beta_{k-1} X_{t+k-1}.$$These parameters can be calculated with the Durbin-Levinson Algorithm which is an algorithm that recursively calculates the solution to an equation using the Toeplitz matrix. More details about this algorithm can be found here.\nEACF\nThe EACF [2] is a two dimensional table that compares the lags of different AR and MA components. The rows correspond to the different values for the AR component p and the columns correspond to the different MA components q.\nEach entry in the table is given a 0 if the residuals at ARMA(p, q) are approximately white noise and are given an X if there is an indication of a poor fit.\nTo assign 0 to a parameter combination, we can approximate the autocorrelation of the residuals to a $N(0, 1/(n-p-q))$ distribution where we subtract the parameter p and q to reduce the bias in our estimation of the variance. These parameters are subtracted because there are p and q parameters which are unknown. If the sample autocorrelation surpasses $1.96/\\sqrt{n-p-q}$ then there is sufficient evidence to suggest the residuals are not white noise and the model has failed to capture the process\u0026rsquo;s information.\nEstimating the parameters of an ARMA model\nOnce the correct hyperparameters p and q have been chosen, the next step is to determine the parameters of the model. The most common method for this is Maximum Likelihood Estimation (MLE), which finds the parameters that maximize the likelihood of observing the actual time series data. This process begins with an initial guess for the parameters, followed by computing the likelihood of the observed data given those parameters. Optimization algorithms such as Gradient Descent, Newton-Raphson, or BFGS are then used to iteratively adjust the parameters to improve the likelihood. These algorithms rely on the gradient and sometimes the second derivative (Hessian) of the likelihood function to guide parameter updates until convergence is achieved. Statistical packages such as \u0026ldquo;statsmodels\u0026rdquo; in Python employ these methods, and once the model is fitted, diagnostics like residual autocorrelation and parameter uncertainty are checked to validate the model.\nIt is worth mentioning other estimation methods could be used instead of MLE such as the Generalised Least Squares (GLS) or Method of Moments (MoM). However, in practice, these methods are often disregarded in favour for the MLE.\nThese diagnostic methods include checking the residual autocorrelations and assessing the normality of the residuals. If the model is well-fitted, the residuals should be uncorrelated, which can be tested using the Ljung-Box test or by inspecting the ACF of the residuals. To evaluate the normality of the residuals, you can use a Shapiro-Wilk test, a Q-Q plot, or a Jarque-Bera test (see the appendix).\nEvaluating your ARMA model\nNow that the model has been fitted, the next step is to evaluate the residuals of the model. If the model has been fit well, the residuals should behave like white noise (i.e. the mean of the residuals should be zero and should not have a pattern). There are a few different tests to test the residuals\nFirst is the Ljung-Box test[4] which test the group of autocorrelations of a time series are different from 0. Rather than testing the randomness at each lag, it test the \u0026ldquo;overall\u0026rdquo; randomness based on a number of lags. The test goes as follows.\nNull Hypothesis (H0): The data is independently distributed.\nAlternative (H1): The data is not independently distributed and exhibit serial correlation.\nThe test statistic is written as\n$$Q = n(n+2) \\sum_{k=1}^h \\frac{\\hat{\\rho}_k^2}{n-k},$$where n is the sample size, $\\hat{\\rho}_k$ is the autocorrelation at a lag k, and h is the number of lags being tested. Under H0, the statistic should asymptotically follows a chi squared distribution with degrees of freedom h $\\mathcal{X}^2_h$. Consequently, to reject the null:\n$$Q \u003e \\mathcal{X}^2_{1-\\alpha, h}.$$It is important that the degrees of freedom $h$ must be adjusted based on the parameters $p$ and $q$ to $h -p - q$.\nAlternatively to the Ljung-Box test, a Jarque-Bera test could be used [5]. A Jarque-Bera test is a goodness-of-fit test which tests whether the residuals follow a normal distribution. In general, if residuals are not normally distributed, the current model may not be accounting for all the patterns in the data. For more details about this test, please refer to the appendix.\nARIMA In many situations, the modelled process will not be stationary. Consequently, the process will need to be made stationary using a differencing process. A first order differencing operation for distribution $\\{X_t\\}$ is expressed as\n$$Y_t = \\nabla X_t = (X_t - X_{t-1}).$$Similarly, a second order differencing is expressed as\n$$Y_t = \\nabla^2 X_t = (X_t - X_{t-1}) - (X_{t-1} - X_{t-2}) = X_t - 2X_{t-1} + X_{t-2}.$$A process $Y_t$ is said to be an autoregressive integrated moving average process, ARIMA(p,d,q) if $Y_t = \\nabla^d X_t$ is an ARMA(p,q) process.\nThese ARIMA models have some well known cases [1] such as\nAn ARIMA(0,1,0) model $X_t = X_{t-1} + \\epsilon_t$ is a random walk. An ARIMA(0,1,0) model with a constant $X_t =\\mu + X_{t-1} + \\epsilon_t$ is a random walk with a drift. ARIMA(0,0,0) is a white noise model. ARIMA(0,1,2) is the Damped Holt\u0026rsquo;s model. ARIMA(0,1,1) is a basic exponential smoothing model. In general, ARMA models assumes the data is stationary while for ARIMA models the time series may not be stationary. ARIMA will use this differencing procedure to make the data stationary and from here follow the ARMA based modelling procedure.\nConclusion Today we covered a few key definition within time series. Additionally, we covered ARMA based models and the process required to ensure the correct parameters have been chosen for our model.\nAppendix Gaussian Processes: A Gaussian process (GP) is a process where $X_{t_1}, \\cdots, X_{t_n}$ has a joint normal distribution for all $t_1, \\cdots, t_n$. No two distinct Gaussian processes will have the same autocovariance function.\nGPs are non-parametric model used to forecast time series data by modelling the time series as a collection of random variables that follow a Gaussian distribution. Not only will these model create predictions but will also provide uncertainty estimates. These uncertainty estimates can then be applied to create confidence intervals for each prediction.\nAs mention in the last paragraph, these models are non-parametric because they take no explicit assumptions on the structure of the timer series. Instead, they use a kernel function to model the relationship between data points.\nThis kernel function can take many forms, one common kernel function is the squared exponential kernel which is used to model smooth continuous trends in time series data.\nGPs excel in situations where the data\u0026rsquo;s relationship is complex or unknown and when uncertainty measures are required.\n**Jarque-Bera test **. A Jarque-Bera test is a goodness-of-fit test which tests whether the residuals have skewness and kurtosis matching a normal distribution. The test statistic (called JB) is non-zero and when it is far from 0, the residuals do not follow a normal distribution.\n$$JB = \\frac{n}{6}(S^2 + \\frac{1}{4}(K-3)^2 ),$$where n is the number of observations, S the sample skewness, and K the sample kurtosis.\n$$S=\\frac{\\hat{\\mu}_3}{\\hat{\\sigma}^3} = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^3}{(\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2)^{3/2}},$$$$S=\\frac{\\hat{\\mu}_4}{\\hat{\\sigma}^4} = \\frac{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^4}{(\\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2)^{2}},$$where $\\hat{\\mu}_3$ and $\\hat{\\mu}_4$ correspond to the estimates of the 3rd and 4th central moments. If the residuals follow a normal distribution, than the JB statistic will asymptotically achieve a chi squared distribution. In general, samples from a normal distribution should have expected skewness of 0 and an expected kurtosis of 0 (same as a kurtosis of 3).\nSome example rejection regions for this test are below.\nFigure 2. Rejection Regions in JB test. Source found at: https://en.wikipedia.org/wiki/Jarque–Bera_test\nReferences TIME SERIES Contents. (n.d.). Available at: https://www.statslab.cam.ac.uk/~rrw1/timeseries/t.pdf. Chapter 6: Model Specification for Time Series. (n.d.). Available at: https://people.stat.sc.edu/hitchcock/stat520ch6slides.pdf. Wikipedia Contributors (2019). Autoregressive integrated moving average. [online] Wikipedia. Available at: https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average. Wikipedia Contributors (2023). Ljung and Annelund. [online] Wikipedia. Available at: https://en.wikipedia.org/wiki/Ljung. Wikipedia. (2020). Jarque. [online] Available at: https://en.wikipedia.org/wiki/Jarque. ‌\n‌\n‌\n‌\n","permalink":"https://www.mathstoml.com/arma--arima/","tags":["Time Series","ARMA","ARIMA"],"title":"ARMA \u0026 ARIMA"},{"categories":["Time Series"],"contents":" Image by Oğuzhan Kıran\nThis article will cover what is a time series, what does it mean for a time series to be stationary, and how could an autoregressive process or moving average process be used to model the time series.\nBefore starting this article, I would like to mention the Cambridge notes from here form the foundation of this content.\nWhat is a Time Series? In short, time series is area of statistics which focuses on describing a set of data points which are collected on regular intervals by fitting low-dimensional models and making forecasts.\nAny time series data can be broken down into 4 components:\nTrend $(T_{t})$: The long term movements of the mean. Seasonal effects $(I_{t})$: Calendar based fluctuations. Cycles $(C_{t})$: Cyclical fluctuations. Residuals $(E_{t})$: The random fluctuations. Using these 4 components, any time series sample $X_{t}$ can be decomposed into these 4 components either additively or multiplicatively:\n$$X_{t} = T_{t} + I_{t} + C_{t} + E_{t},$$$$X_{t} = T_{t} \\cdot I_{t} \\cdot C_{t} \\cdot E_{t}.$$The distribution of a sequence of time series $(X_{t}, \\dots, X_{t+k})$ can take many forms with some examples including:\nNormal Distribution: The samples could follow a normal distribution where most data is concentrated around the mean and has fewer points on the tail. Skewed Distribution: The time series exhibits skewness where the distribution is skewed to the left or right and so the data is not symmetric around the mean. Heavy-Tailed: Here the time series has a greater number of outliers. Discrete Distribution: In this example, the time series could only take discrete values. Time series can be split into two categories depending on the properties of the time series: stationary or non-stationary. A stationary time series has a fixed mean, variance, and autocorrelation which helps us capture the time series\u0026rsquo;s patterns and relationships allowing for more accurate forecasts. A non-stationary time series may have trends, seasonality, or changing volatility, making it harder to create forecasts.\nStationary Time Series Given a sequence of data points $\\{ X_{t}, t \\in \\mathbb{Z} \\}$, the sequence is strongly stationary or strictly stationary if\n$$\\left( X_{t_{1}}, \\dots, X_{t_{k}}\\right) \\overset{d}{=} \\left( X_{t_{1} + h}, \\dots, X_{t_{k} + h}\\right),$$for all the time points $t_{1}, \\dots, t_{k}$ and integer $h$. Note that when we use $X \\overset{d}{=} Y$, this means the CDF of the two probability distributions $X$ and $Y$ are the same, formally, $\\mathbb{P}(X \\leq x) = \\mathbb{P}(Y \\leq x)$. This implies the mean, variance, and higher moments are the same.\nA sequence is weakly stationary or second order stationary if\n$$\\begin{aligned} \\mathbb{E}(X_{t}) \u0026= \\mu \\\\ \\text{and} \\quad \\text{Cov}(X_{t}, X_{t+k}) \u0026= \\mathbb{E}[(X_{k} - \\mu)(X_{t+k} - \\mu)] = \\gamma_{k}, \\end{aligned}$$where $\\mu$ is constant and $\\gamma_{k}$ is independent of $t$ and the sequence $\\{ \\gamma_{k}, k \\in \\mathbb{Z} \\}$ is called the autocovariance function. At the lag 0, $\\gamma_{0} = \\mathbb{E}[(X_{k} - \\mu)^{2}] = \\text{Var}(X_{k})$.\nFurther define,\n$$\\begin{aligned}\\rho_{k} = \\gamma_{k} / \\gamma_{0} \u0026= \\frac{\\mathbb{E}[(X_{k} - \\mu)(X_{t+k} - \\mu)]}{\\sqrt{\\text{Var}(X_{t})\\text{Var}(X_{t+k})}} \\\\ \u0026= \\frac{\\mathbb{E}[(X_{k} - \\mu)(X_{t+k} - \\mu)]}{\\text{Var}(X_{t})} \\\\ \u0026= \\text{Corr}(X_{t}, X_{t+k}),\\end{aligned}$$where $\\rho_{k}$ is the autocorrelation at lag k and the plot of the autocorrelations $\\{\\rho_0, \\dots, \\rho_k \\}$ is referred to as the autocorrelation function. The autocorrelation function helps identify patterns in the time series, such as periodicity (repeating cycles) or trends. If the autocorrelation decays slowly as the lag h increases, it suggests a trend or long-term dependence. If the autocorrelation exhibits periodic patterns, it indicates seasonality or cycles in the data.\nIn general, weak stationarity is a weaker condition than strong stationarity because the weakly stationary time series may have a fixed mean, variance, and autocorrelation, however the time series may not follow the same probability distribution and the weakly stationary time series moments (see the appendix) may vary overtime.\nProperties of a stationary time series:\nA strictly stationary process is weakly stationary. If $X_{t}$ is truly random, then $\\gamma_{0} = \\text{Var}(X_{t}) \u003e 0$. By symmetry $\\text{Cov}(X_t, X_{t-k}) = \\text{Cov}(X_{t-k}, X_t)$, so $\\gamma_{k} = \\gamma_{-k} \\ \\forall k$. Now that we know the difference between the type of time series data, we can start to model it.\nAutoregressive Process The first way to model any stationary time series is as an autoreggresive process where the current value of the series is expressed as a linear combination of its past values and a random error term (or white noise). Specifically, an autoregressive process of order $p$ is denoted as AR($p$) and can be written as:\n$$X_{t} = \\sum_{r=1}^{p} \\phi_{r} X_{t-r} + \\epsilon_{t},$$where $\\phi_{1}, \\dots, \\phi_{r}$ are fixed constants referred to as autoregressive coefficients which determine how much influence each lagged coefficient has on the current and $\\{\\epsilon_{t}\\}$ is a sequence of independent random variables with mean 0 and variance $\\sigma^{2}$, this random variable is often referred to as white noise.\nTo find the parameters of this model, we\u0026rsquo;ll need to use the autocovariance function. For simplicity, we\u0026rsquo;ll use the AR(1) process:\n$$\\begin{equation} X_{t} = \\phi X_{t-1} + \\epsilon_{t},\\end{equation}$$First by multiplying (1) by $X_{t-k}$ and taking the expected value gives:\n$$\\begin{aligned} \\mathbb{E}[X_{t}X_{t-k}] \u0026= \\mathbb{E}[\\phi X_{t-1}X_{t-k}] + \\mathbb{E}[\\epsilon_{t}X_{t-k}] \\\\\\mathbb{E}[X_{t}X_{t-k}] \u0026= \\phi \\mathbb{E}[X_{t-1}X_{t-k}] + \\mathbb{E}[\\epsilon_{t}] \\cdot \\mathbb{E}[X_{t-k}] \\\\ \\gamma_k \u0026= \\phi \\gamma_{k-1} + 0 \\cdot \\mathbb{E}[X_{t-k}] \\\\ \\gamma_{k} \u0026= \\phi \\gamma_{k-1}.\\end{aligned}$$Since the terms $\\gamma_{k}$ can be solved using $\\gamma_{k} = \\frac{1}{N}\\sum_{t=1}^{N-k}\\mathbb{E}[(X_{t} - \\bar{X})(X_{t+k} - \\bar{X})]$, we now have a way to find the parameters $\\phi$.\nThe final term to estimate is the white noise $\\epsilon_{t}$. To find an equation for this term, we\u0026rsquo;ll start by squaring (1) and then taking the expected value. This gives:\n$$\\begin{aligned} \\mathbb{E}[X_{t}^{2}] \u0026= \\mathbb{E}[(\\phi X_{t-1} + \\epsilon_{t})^{2}] \\\\ \\mathbb{E}[X_{t}^{2}] \u0026= \\phi_{1}^{2}\\mathbb{E}[X_{t-1}^{2}] + 2 \\phi_{1} \\mathbb{E}[X_{t-1}\\epsilon_{t}] + \\mathbb{E}[\\epsilon_{t}^{2}] \\\\ \\gamma_{0} \u0026= \\phi_{1}^{2}\\gamma_{0} + 0 + \\sigma^{2}, \\end{aligned}$$so $ \\gamma_{0} (1 - \\phi_{1}^{2}) = \\sigma^{2}$.\nNow that we know how to find the parameters of an AR(1) model, we can define this for any AR process. Defining AR(p) as\n$$\\begin{equation}X_{t} = \\phi_{1} X_{t-1} +\\phi_{1} X_{t-1} \\dots \\phi_{p} X_{t-p} + \\epsilon_{t}.\\end{equation}$$If we were to multiply (2) by $X_{t-k}$, take the expected value, and divide by $\\gamma_{0}$ we will end up producing the Yule-Walker equations:\n$$\\rho_{t} = \\phi_{1} \\rho_{t-1} +\\phi_{1} \\rho_{t-1} \\dots \\phi_{p} \\rho_{t-p}, \\quad k=1, 2, \\dots$$This can be solved using the autocovariance matrix (a Toeplitz matrix):\n$$\\begin{pmatrix} \\gamma_0 \u0026 \\gamma_1 \u0026 \\cdots \u0026 \\gamma_{p-1} \\\\ \\gamma_1 \u0026 \\gamma_0 \u0026 \\cdots \u0026 \\gamma_{p-2} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\gamma_{p-1} \u0026 \\gamma_{p-2} \u0026 \\cdots \u0026 \\gamma_0 \\end{pmatrix} \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\vdots \\\\ \\phi_p \\end{pmatrix} = \\begin{pmatrix} \\gamma_1 \\\\ \\gamma_2 \\\\ \\vdots \\\\ \\gamma_p \\end{pmatrix}$$Now that the AR coefficients are estimated. The variance for the white noise $\\sigma^{2}$ can be computed using:\n$$\\sigma^{2} = \\gamma_0 - \\sum_{i=1}^{p}\\phi_i \\gamma_i$$Moving Average Process Given a centred time series, an alternative way to model this time series is by using a moving average process of order $q$ denoted by $MA(q)$ is defined as\n$$X_{t} = \\sum_{i=0}^{q} \\theta_{i} \\epsilon_{t-i},$$where $\\theta_{1}, \\dots, \\theta_{q}$ are fixed constants where $\\theta_{0} = 1$ and $ \\{\\epsilon_{0},\\dots ,\\epsilon_{t} \\}$ is an independent random variable with mean 0 and variance $\\sigma^{2}$, we\u0026rsquo;ll call this term the white noise term.\nA moving average process uses the idea that time series data contains fluctuations which can be exploited to model the short-term variations in the time series.\nFitting the moving average model is no more complicated than fitting an autoregressive model. Given an MA(1) model:\n$$\\begin{equation} X_{t} = \\theta_{1} \\epsilon_{t-1} + \\epsilon_{t}\\end{equation}$$First we\u0026rsquo;ll create an equation for $\\theta_1$ and $\\sigma^{2}$ using $\\gamma_0$ by multiplying (3) by $X_{t}$ and taking the expectation giving:\n$$\\gamma_{0} = \\mathbb{E}[X_{t}^{2}] = \\mathbb{E}[(\\epsilon_t + \\epsilon_{t-1}\\theta_1)^{2} ] = \\sigma^{2}(1 + \\theta_{1}^{2})$$The second equation will be formed by multiplying (3) by $X_{t-1}$ and taking the expectation:\n$$\\gamma_1 = \\mathbb{E}[X_{t} X_{t-1}] = \\mathbb{E}[(\\epsilon_t + \\epsilon_{t-1}\\theta_1) (\\epsilon_{t-1} + \\epsilon_{t-2}\\theta_1)] = \\theta_1 \\sigma^{2}.$$Next using the autocorrelation at lag 1:\n$$\\rho_1 = \\frac{\\gamma_{1}}{\\gamma_0} = \\frac{\\theta_1 \\sigma^2}{\\sigma^2(1 + \\theta_1^2)} = \\frac{\\theta_1}{1 + \\theta_1^2}.$$For larger models, with more parameters, the combination of the Yule Walkers equations and the Toeplitz matrix will be the easiest way to find the model\u0026rsquo;s parameters.\nMoving average processes are particularly effective when a time series exhibits short-term dependencies, as they are designed to capture the immediate correlations between shocks. However, their primary limitation lies in this very reliance on short-term relationships, making them less suited to modelling long-term dependencies in the data.\nConclusion Today we covered what is a time series and how we could model a time series using basics processing techniques such as AR and MA. The next article will build on these methods introducing the ARMA models which combine both the AR and MA processes.\nAppendix Moments: Moments are quantitative measures of distribution used to describe its shape and characteristics.\nFirst Moment: The first moment (mean) is a measure to determine the central tendency of data. For a random variable from a continuous probability distribution, the first moment is written as:\n$$\\text{MEAN} = \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f(x) d x.$$Second Moment: The second moment measures the spread of the distribution. It is written as:\n$$\\text{VARIANCE} = \\text{Var}[X] = \\mathbb{E}[(X - \\mathbb{E}[X])^{2}].$$Third Moment: The third moment measures the asymmetry of a distribution. It is written as:\n$$\\text{SKEWNESS} = \\frac{\\mathbb{E}[(X - \\mathbb{E}[X])^{3}]}{\\sigma^{3}}.$$If the skewness if positive, than the distribution has a longer tail on the left and the bulk of the data is concentrated on the right. If the skewness is negative, then the distribution has a longer tail on the right and the bulk of the data is concentrated on the left. If the skewness is 0, the data is symmetric.\nFourth Moment: The fourth moment measures the peakedness of a distribution and tells us how of the variance is a result of extreme outliers. It can be written as:\n$$\\text{KURTOSIS} = \\frac{\\mathbb{E}[(X - \\mathbb{E}[X])^{4}]}{\\sigma^{4}}.$$Interpretation:\nHigh Kurtosis (Leptokurtic): The kurtosis is greater than 3, than the distribution has more extreme values (fatter tails). Low Kurtosis (Platykurtic): The kurtosis is less than 3, than the distribution has fewer extreme values (thinner tails). Normal Kurtosis (MesoKurtic): A kurtosis of 3 is the same as a normal distribution. Fixed Form Solution: A fixed-form solution (also known as a closed-form solution) refers to a solution to a problem that can be written down in a finite number of operations, without the need for iterative methods, approximation, or numerical optimization.\nReferences: TIME SERIES Contents. (n.d.). Available at: https://www.statslab.cam.ac.uk/~rrw1/timeseries/t.pdf.\n‌\n","permalink":"https://www.mathstoml.com/what-is-a-time-series--how-can-we-model-it/","tags":["Statistics","Time Series"],"title":"What is a Time Series \u0026 How can we Model it?"},{"categories":["Graphs"],"contents":" Image by BoliviaInteligente\nOver the past 15 years we have seen a surge of use of Graph Neural Networks (GNNs) being used to model social networks, recommendations systems, transportation networks, and many more systems. With the ever growing use of GNNs, this has naturally led to the questions about the use of GNNs within the medical sector. More specifically, the question of using GNNs to predict the properties of molecules was brought up. Back in the early 2010s, this idea was in its infancy with few successful applications.\nSimilarly, in the 2010s, we observed significant advancements in large-scale quantum chemistry calculations and molecular dynamics simulations (see the appendix for more details). Coupled with the increased speed of experimentation, these advancements have led to the generation of vast amounts of data. To effectively harness this data, more powerful models became necessary. One promising approach was the use of Neural Network (NN) architectures, which have demonstrated great success in handling large datasets and modelling complex interactions.\nHowever, molecules present a unique challenge. They tend to be irregular in structure and do not fit well into traditional grid or sequential formats, making them non-Euclidean in nature. To effectively model molecules, a graph-based representation is needed, where atoms are treated as nodes and chemical bonds as edges. This naturally leads to the consideration of GNNs.\nFigure 1. An example molecule from the PyTorch MoleculeNet dataset.\nThe second main drive behind using GNNs to model molecules comes from the GNN\u0026rsquo;s inductive bias of permutation invariance. In short, this means that if a different permutation of the same molecule entered the model the same prediction would be given. Most other classes of Neural Networks lacked this property making the GNNs preferable.\nEven with these preferences towards GNNs, there was one limitation present. Many of the GNNs before 2017 did not incorporate edge information in their message passing system. Consequently, a new GNN-like architecture had to constructed, this led to Gilmer et al. constructing the class of NNs called the Message Passing Neural Networks (MPNNs).\nThe MPNN Framework Given a undirected graph $G$, we will denote with node features of node $v$ as $x_v$ and edge features as $e_{vw}$ where $w \\in \\mathcal{N}(v)$. The MPNN framework can be broken down into two phases: the message passing phase and the readout phase.\nThe message passing phase is run for T steps and is defined by the differentiable message function $M_t$ and differentiable vertex update function $U_t$. This message passing phase updates the hidden states of node $v \\ (h_{v}^{t})$ using the message $m_{v}^{t+1}$:\n$$m_{v}^{t+1} = \\sum_{w \\in \\mathcal{N}(v)} M_{t}(h_{v}^{t}, h_{w}^{t}, e_{vw}),$$$$h_{v}^{t+1} = U_{t}(h_{v}^{t}, m_{v}^{t+1}).$$The readout phase is implemented in the final layer of the MPNN. This computes a feature vector for the whole graph using a differentiable readout function $R$ according to:\n$$\\hat{y} = R(\\{h_{v}^{T} | v \\in G\\} ).$$These outputs $\\hat{y}$ of the readout function are then fed into a neural network to solve the classification or regression problem.\nGG-NN Before jumping into Gilmer et al.\u0026rsquo;s model, it is worth mentioning many other GNNs could be considered suitable MPNNs. Specifically, the GNN called Gated Graph Neural Networks (GG-NN) by Li et al. was used as the baseline for Gilmer et al. choice of model.\nIn the GG-NN architecture, the message passing system and the update function is defined as:\n$$M_{t}(h_{v}^{t}, h_{w}^{t}, e_{vw}) = A_{e_{vw}}h_{w}^{t},$$$$U_{t} = GRU(h_{v} ^{t}, m_{v} ^{t+1})$$where $A_{e_{vw}}$ is a learned matrix and GRU stands for the Gated Recurrent Unit by Cho et al. This work implemented weight tying which constricts the update function to remain the same for every time step t.\nFinally the readout function takes the form:\n$$\\begin{equation}R = \\sum_{v\\in V} \\sigma \\left( i(h_{v}^{T}, h_{v}^{0}) \\right) \\odot \\left( j(h_{v}^{T})\\right),\\end{equation}$$where $i$ and $j$ and neural networks, $\\odot$ denotes the element wise multiplication, and $\\sigma$ denotes an activation function (often being the sigmoid activation function).\nGilmer et al.\u0026rsquo;s MPNN The first modification made to the GG-NN model was in the message-passing function. Here, Gilmer et al. defined the message function as $M_{t}(h_{v}, h_{w}, e_{vw}) = A(e_{vw}) h_{w}^{t}$, where $A(e_{vw})$ is a neural network that maps edge vectors $e_{vw}$ to a $d \\times d$ matrix referred to as the edge network. This design introduced the feature that the message produced depends solely on the embedding of node $w$, $h_{w}^{t}$, and not on the current embedding of node $v$, $h_{v}$. This design choice was made to simplify the computational process. Alternatively, a more computationally expensive method would involve passing the message from node $w$ to node $v$ along their edge using $m_{wv} = f(h_{w}^{t}, h_{v}^{t}, e_{vw})$, where $f$ is a neural network.\nFor the readout function, two options were explored. The first was identical to the one implemented in the GG-NN model (equation 1) and the second option was inspired by the set2set model by Vinyals et al. In the second approach, the model performs a linear projection on each tuple $(h_{v}^{t}, x_{v})$, then stores the results of these projected tuples $T = \\{(h_{v}^{T}, x_{v})\\}$. After $M$ steps of computation, the graph generates a graph-level embedding, $q_{t}^{\\star}$ from these tuples. This final operation is invariant to the order of the tuples and the resulting embedding is then fed into a neural network.\nDuring Gilmer et al\u0026rsquo;s training, the best performing model used the set2set readout function.\nThe final innovations centred around the introduction of virtual graph elements, aimed at improving the message-passing capabilities of the model by facilitating the transfer of information over longer distances. The first idea introduced a virtual edge type for each pair of nodes that were not directly connected. The second method involved adding a master node that was connected to every node in the graph via a unique edge type. This master node had a separate dimensionality and separate weights for its update function.\nThe limitation of this method revolves around its complexity. A single step in the MPNN for a dense graph requires $\\mathcal{O}(n^{2} d^{2})$ operations where $n$ in the number of nodes and $d$ is the dimension of the node embeddings. This is a consequence of the message passing step where each node embedding message suffers from complexity $\\mathcal{O}({d}^{2})$ and this operation is performed $n^{2}$ resulting in the complexity of $\\mathcal{O}(n^{2} d^{2})$. To solve this issue, the node embeddings can be broken down into k different embeddings of size d/k denoted by $h_{v}^{t+1, k}$. The propagation step is running on each k copies generating node embeddings $\\tilde{h}_{k} ^{t, k}$ and then these embeddings are then mixed together via a neural network:\n$$(h_{v}^{t, 1}, h_{v}^{t, 2}, ... , h_{v}^{t, k}) = g(\\tilde{h}_{v}^{t, 1}, \\tilde{h}_{v}^{t, 2}), ... , \\tilde{h}_{v}^{t, k}),$$where $g$ denotes the neural network and $(x, y, ...)$ denotes the concatenation. Using this method, each k embeddings part achieves a complexity of $\\mathcal{O} (n^{2} (d/k)^{2})$ and multiplying this result by the k different embedding parts gives a complexity of $\\mathcal{O} (n^{2} d^{2} / k)$.\nFigure 2. The performance of Gilmer el al\u0026rsquo;s MPNN model with a set2set readout function. enn-s2s denotes the best model and enn-s2s-ens5 denotes the ensemble model with an ensemble of 5 different predictions. The right side corresponds to the benchmarks for GNNs in molecule prediction tasks such as polarity and energy level predictions. Source found here: Gilmer, J., Schoenholz, S., Riley, P., Vinyals, O. and Dahl, G. (n.d.). Neural Message Passing for Quantum Chemistry. [online] Available at: https://arxiv.org/pdf/1704.01212v2.\nLimitations of MPNNs Looking at figure 2 we can see that the Gilmer et al\u0026rsquo;s MPNN exceeded the performance of the previous state of the art model on the tested benchmarks, however, even with this strong performance, there were some notable limitations.\nThe MPNN struggled to generalise effectively to graphs that are larger than those encountered during training. This is particularly problematic in domains like chemistry, where molecules of varying sizes need to be analysed. Any approaches that utilize spatial information in MPNNs tend to create fully connected graphs, where each node is connected to every other node. As a result, computing the number of message can be computationally expensive. Conclusion This article introduced the idea of the MPNN, a form of GNN with applications in molecule prediction problems.\nAppendix Quantum Chemistry: Quantum chemistry use the quantum mechanical principles to calculate the properties and behaviours of molecules at the atomic level. These used to be computationally expensive, but due to recent advancements in computational power, these form of calculations are now possible to be performed at a large scale.\nMolecular dynamics simulations: Molecular dynamics (MD) simulations model the physical movements of atoms and molecules over time. These simulations are used to predict how a molecule will evolve within different conditions. Similar to the quantum chemistry, advancements in computational power has made these calculations possible.\nReferences Gilmer, J., Schoenholz, S., Riley, P., Vinyals, O. and Dahl, G. (n.d.). Neural Message Passing for Quantum Chemistry. [online] Available at: https://arxiv.org/pdf/1704.01212v2.\nLi, Y., Zemel, R., Brockschmidt, M. and Tarlow, D. (n.d.). Published as a conference paper at ICLR 2016 GATED GRAPH SEQUENCE NEURAL NETWORKS. [online] Available at: https://arxiv.org/pdf/1511.05493 [Accessed 14 Aug. 2024].\nChung, J., Gulcehre, C. and Cho, K. (2014). Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. [online] Available at: https://arxiv.org/pdf/1412.3555.\nVinyals, O., Bengio, S., Kudlur, M. and Brain, G. (n.d.). ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS. [online] Available at: https://arxiv.org/pdf/1511.06391 [Accessed 4 Sep. 2024].\n‌\n","permalink":"https://www.mathstoml.com/message-passing-neural-networks/","tags":["GNNs"],"title":"Message Passing Neural Networks"},{"categories":["Statistics"],"contents":" Image by Daniela Turcanu\nIntroduction\nThis article will cover the key features of Kullback-Leibler Divergence (KL divergence), a formula invented in 1951 by the mathematicians Soloman Kullback and Richard Leibler. This formula is used in the background of many of the modern day machine learning models focused around probabilistic modelling. These including Variational Autoencoders (VAEs), Generative Models, Reinforcement Learning, and Natural Language Processing. Additionally, this article will cover some of KL divergence\u0026rsquo;s key properties and briefly cover one of its applications.\nKL Divergence\nIn essence, KL divergence gives us a way to tell how much one probability distribution varies from another. KL divergence quantifies this result as the amount of information lost when approximating one distribution from another.\nIn the discrete setting, for probability distributions $P$ and $Q$ defined over sample space $\\mathcal{X}$, the formula takes the form:\n$$D_{\\text{KL}}(P || Q) = \\sum_{x\\in\\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)}.$$This formula quantifies how close distribution $Q$ is to the approximating distribution $P$. More specifically, it measures the divergence of the distribution $Q$ from distribution $P$. This is interpreted as the amount of extra information required to encode samples from $P$ using distribution $Q$.\nKL divergence for continuous probability distributions $P$ and $Q$ follow:\n$$D_{\\text{KL}}(P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx,$$where $p$ and $q$ refer to the probability densities of distribution $P$ and $Q$ respectively.\nBefore continuing, it is important to mention that in other texts, KL divergence is often referred to relative entropy. This must not be confused with Shannon entropy which measures the uncertainty in just one probability distribution.\nKey Properties of KL Divergence\nAsymmetry\nThe first key property of KL divergence is that this metric is asymmetrical,\n$$D_{\\text{KL}}(P || Q) \\neq D_{\\text{KL}}(Q || P).$$This indicates that the divergence of distribution $P$ from $Q$ often gives a different result to the divergence of distribution $Q$ from $P$.\nThe lack of symmetry, along with the fact that the triangle inequality is not satisfied, are key reasons why this measure is not referred to as a distance. Instead, in information theory, it is classified as a type of divergence.\nAn alternative symmetric measure to quantify the difference between two probability distributions is Jeffery\u0026rsquo;s Divergence. This divergence is defined as the sum of divergences between distributions $P$ and $Q$ in both directions.\n$$J(P, Q) = \\frac{1}{2} D_{\\text{KL}}(P \\parallel Q) + \\frac{1}{2} D_{\\text{KL}}(Q \\parallel P)$$Jeffery\u0026rsquo;s measure provides a way to measure the total difference between two distributions.\nOn initial inspection, Jeffery\u0026rsquo;s may seem more appropriate than KL divergence, however there\u0026rsquo;s a few reasons why Kl divergence may be preferred. First KL divergence has a higher sensitivity to differences in distributions while Jefferys is a more smoothed version of KL divergence, which is less sensitive to these finer differences. The finer tuning ability is often preferred in practice.\nSecondly, in situations where one distribution is a \u0026ldquo;target\u0026rdquo; or \u0026ldquo;true\u0026rdquo; distribution and the second is one is a distribution trying to approximate the first. KL divergence encapsulates this problem well, while Jeffery\u0026rsquo;s does not. The reasons for this is because Jeffery\u0026rsquo;s will incorporate information about the divergence of the true distribution from the approximating distribution. This is unwanted information, which hinders are inferences.\nNon-Negativity\nThe second important property of KL divergence is that it is non-negative and equals zero iff (if and only if) the two distributions are identical almost everywhere. In this case, there is no divergence between the two distributions. Mathematically, this non-negativity property is written as $D_{\\text{KL}}(P | Q) \\geq 0$. This result is known as Gibbs’ inequality.\nThe term “almost everywhere” refers to the distributions being identical except on a subset of the domain with measure zero. This idea is analogous to the concept of “almost surely” in probability theory. Essentially, KL divergence is zero iff the two distributions are identical everywhere except on a small subset of measure zero.\nConvexity\nKL divergence is convex for any pair of probability measures $P$ and $Q$. For example, choose $(P_{1}, P_{2})$ and $(Q_{1}, Q_{2})$ to be two pairs of probability measures. Then the following property is satisfied.\n$$D_{\\text{KL}}(\\lambda P_{1} + (1 - \\lambda) P_{2} \\| \\lambda Q_{1} + (1-\\lambda)Q_{2}) \\leq \\lambda D_{\\text{KL}}(P_{1} \\| Q_{1} ) + (1 - \\lambda) D_{\\text{KL}}(P_{2} \\| Q_{2} ), \\\\ \\text{ for } 0 \\leq \\lambda \\leq 1.$$No upper bound exists\nAnother important property of KL divergence is that, in general, it does not have an upper bound. However, an upper bound does exist in a specific case: when $P$ and $Q$ are discrete probability distributions over the same discrete set of outcomes. In this situation, a maximum value for $D_{\\text{KL}}(P | Q)$ can be determined.\nExample:\nGiven probability distributions $P$ and $Q$ with discrete outcomes from the sample space $\\{0,1,2\\}$. The distributions\u0026rsquo;s outcome probabilities are in the table below.\nx 0 1 2 Distribution P(x) 1/2 1/3 1/6 Distribution Q(x) 1/3 1/3 1/3 The KL divergence for $Q$ from $P$ and $P$ from $Q$ is written as:\n$$\\begin{aligned} D_{\\text{KL}}(P\\|Q) \u0026= \\sum_{x \\in \\mathcal{X}} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right) \\\\ \u0026= 1/2 \\log(\\frac{1/2}{1/3}) + 1/3 \\log(\\frac{1/3}{1/3}) + 1/6 \\log(\\frac{1/6}{1/3}) \\\\ \u0026= 0.038 \\\\ D_{\\text{KL}}(Q\\|P) \u0026= \\sum_{x \\in \\mathcal{X}} Q(x) \\log \\left( \\frac{Q(x)}{P(x)} \\right) \\\\ \u0026= 1/3 \\log(\\frac{1/3}{1/2}) + 1/3 \\log(\\frac{1/3}{1/3}) + 1/3 \\log(\\frac{1/3}{1/6}) \\\\ \u0026= 0.042\\end{aligned} $$In many real-world scenarios, the distributions $P$ and $Q$ are derived from frequency distributions. As a result, there may be instances where one distribution contains events that have not been observed in the other. To account for these unobserved events, a smoothing method is applied. This method assigns a small probability to the events missing from both distributions, ensuring that all possible outcomes are represented.\nx 0 1 2 Distribution P(x) 1/2 1/2 0 Distribution Q(x) 0 2/3 1/3 Here a small smoothing operator $\\epsilon=10^{-3}$ smoothen $P$ and $Q$. This gives us the following table.\nx 0 1 2 Distribution P(x) 1/2 - ε/2 1/2 - ε/2 ε Distribution Q(x) ε 2/3 - ε/2 1/3 - ε/2 Then the KL divergence of $P$ from $Q$ and $Q$ from $P$ can be calculated using the same formula as above.\nApplications of KL\nOne application of KL divergence is in Variational Inference (VI), a method in Bayesian Machine Learning used to approximate complex posterior distributions. The process involves selecting a simpler family of distributions, $Q$, to approximate the true but intractable posterior distribution, $P$. This approximating distribution $Q$ is referred to as the variational distribution.\nThis approximation is crucial since directly finding the true posterior distribution is often infeasible. To estimate $P$, we first express the distribution in terms of the latent variable ($Z$) and the observed data ($X$), leading to the following equation:\n$$ P(Z|X) = \\frac{P(X|Z)P(Z)}{P(X)} = \\frac{P(X|Z)P(Z)}{\\int_{Z'}P(X|Z') dZ'}.$$The challenge with this formula arises from the marginal term $P(X)$. Integrating over all possible values of $Z$ is often intractable, especially when $Z$ is a high-dimensional vector, as this makes the integration computationally expensive. To address this intractability, we introduce an approximating function $Q$, which serves as a simpler and more tractable version of the true posterior $P$.\nThe difference between the distributions $Q$ and $P$ is measured by a dissimilarity function, $d(Q, P)$. This function can take various forms, with KL divergence being one of the most commonly used.\nAs a result, the goal becomes minimizing the divergence between $Q$ and $P$. This optimization problem is typically framed as maximizing the Evidence Lower Bound (ELBO), which can be expressed as follows:\n$$\\begin{aligned} \\text{ELBO} \u0026= \\mathbb{E}_{Q(Z, \\phi)}\\left[\\log(P(X,Z)) - \\log(Q(Z,\\phi))\\right]\\\\ \u0026= D_{KL}(P\\|Q(Z, \\phi)) - \\log(P(X)) \\end{aligned} $$where $\\phi$ are the parameters of distribution $Q$, these are the parameters which need to be optimised to make distribution $Q(Z, \\phi)$ approximate the true posterior. Since the term $\\log(P(X)$ is constant, maximising the ELBO is equivalent to minimising the KL divergence.\nConclusion\nIn conclusion, KL divergence is a useful tool used in information theory, statistics, and even machine learning. This measure of divergence is a crucial for quantifying the difference between two probability distributions. Understanding KL divergence will undoubtedly enhance your ability to interpret probabilistic models and it will serve as a good stepping stone into many key ML models and algorithms used today.\nReferences:\nWikipedia. (2024). Kullback–Leibler divergence. [online] Available at: https://en.wikipedia.org/wiki/Kullback_Leibler_divergence [Accessed 26 Jul. 2024].\nWikipedia. (2022). Variational Bayesian methods. [online] Available at: https://en.wikipedia.org/wiki/Variational_Bayesian_methods.\nShannon Entropy and Kullback-Leibler Divergence. (n.d.). Available at: https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf [Accessed 23 Apr. 2019].\n2.4.8 Kullback-Leibler Divergence. (n.d.). Available at: https://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf.\n‌\n‌\n","permalink":"https://www.mathstoml.com/kullback-leibler-divergence/","tags":["Machine Learning","Statistics"],"title":"Kullback-Leibler Divergence"},{"categories":["Diffusion Models"],"contents":" [Image by Justin Lim]\nIntroduction This article will delve into diffusion models, a group of latent variable (see definitions) generative models with applications in image generation, audio synthesis, and denoising. More specifically, this article will mostly focus on the derivations and the ideas behind diffusion models, with a heavy enthuses on the ideas introduced in Ho et al. in his Denoising Diffusion Probabilisitic Models paper (DDPMs). The applications of these models will not be covered today.\nIntroduction to probabilistic modelling Before delving into diffusion models, it\u0026rsquo;s best to briefly cover some of the common issues in probabilistic modelling. As explained by J.Sohl-Dickstein et al., in probabilistic modelling, there has always been the trade-off between two conflicting objectives. These objectives are tractability and flexibility.\nTractable models are those that fit easily to data and that can be analytically evaluated. However, these models often lack the complexity required to model complex datasets.\nOn the contrary, flexible models have the ability to model any arbitrary data. For example, any distribution of data $\\mathcal{X}$ can be represented by a scoring function $\\phi(\\bm{x}) \\geq 0$, $\\forall x \\in \\mathcal{X}$. To construct a probabilistic distribution from this function, we need to divide by a normalizing constant $C$, where $C = \\int \\phi(x) \\ \\delta x$, giving us $p(x) = \\phi(x) / C$. The challenge with this method arises from the intractability of this normalisation term. We will explore this limitation further later in this article.\nDiffusion models sit in the middle between these two objectives. They provide extreme flexibility while remaining tractable. To achieve this a generative Markov chain is used. This chain iteratively converts a simple known distribution (often a Gaussian distribution) to the target distribution. The model learns in each step by estimating the small perturbations in the sample rather than estimating the target function from a single non-analytically-normalisable potential function.\nThis idea stems from the fact that a diffusion process exists for any smooth target function. Consequently, a diffusion process can model distributions of arbitrary form.\nWhat are diffusion models? Diffusion models are broken down into 2 key processes. The forward process and the reverse process. The forward process gradually adds Gaussian noise to an image, slowly corrupting the image. On the other hand, the reverse process takes this corrupted image and iteratively tries to remove the noise from the corrupted image. In the process of removing the noise, a function (often a neural network) is trained to remove this noise. After a long training process, this new function will have the ability to generate samples from random noise.\nDiffusion models are inspired by two key processes. The forward and backward processes in diffusion models are rooted in the concept of diffusion in physics, where particles naturally move from areas of high concentration to low concentration. This is where the term “diffusion” in the model’s name comes from.\nThe second, slightly more complex, inspiration comes from Sequential Monte Carlo (SMC) methods. SMC methods are simulation-based techniques used to approximate the posterior distribution, which is particularly useful for modelling dynamic systems where states evolve over time.\nThese models have been shown to be competitive and even surpass the performance GANs and VAEs architectures in many high quality image generation tasks. They\u0026rsquo;re also the backbone architecture used by many of the modern day image generation models such as OpenAI Sora, Stable Diffusion, DALL-E 2, Imagen, Latent Diffusion Models, and many more.\nForward Process The forward process starts with starting point $\\bm{x}_{0} \\sim q$, where $\\bm{x}_{0}$ is an uncorrupted image and $q$ is the marginal probability distribution. We iteratively add Gaussian noise to the sample image over T steps producing a set of increasing corrupted samples $\\{\\bm{x}_{1}, \\bm{x}_{2}, ... \\bm{x}_{T}\\}$, with each of these step sizes controlled by a variance scheduler parameter $\\{ \\beta_{t} \\in (0,1)\\}^{T}_{t=1}$ . Under this assumption, the marginal probability distribution $q$ given the corrupted image $\\bm{x}_{t-1}$ can be modelled with a Gaussian distribution:\n$$ q(\\bm{x}_{t}|\\bm{x}_{t-1}) = \\mathcal{N}(\\bm{x}_{t}; \\sqrt{1 - \\beta{t}}\\bm{x}_{t-1}, \\beta_{t}\\bm{I}).$$\nFigure 1. An example forward diffusion process. Source: Nichol \u0026amp; Dhariwal\nThe iterative nature of progressively adding noise to the images is inspired by Markovian processes where the subsequent state of an event $\\bm{x}_{t+1}$ is only dependent on the current state of the event $\\bm{x}_{t}$. This can be shown of mathematically with the probability of getting the sequence of images $\\bm{x}_{1}, ..., \\bm{x}_{T}$ given start image $\\bm{x}_{0}$ is:\n$$p(\\bm{x}_{1}, ..., \\bm{x}_{T} | \\bm{x}_{0}) = \\prod _{t=0}^{T} p(\\bm{x}_{t} | \\bm{x}_{t-1}).$$For any given time step t, in order to avoid having to iteratively calculate the previous samples from $\\bm{x}_{0}$, to $\\bm{x}_{t-1}$ to apply the noise to the sample at step $t$, we can instead use the following reparameterization trick. First let $\\alpha_{t} = 1 - \\beta_{t}$ and $\\overline{\\alpha} = \\prod_{i=1}^{t}\\alpha_{i}$. If we define $\\bm{\\epsilon}_{t-1}$, \u0026hellip; to be random variables from a normal distribution $\\mathcal{N}(\\bm{0}, \\bm{I})$, then we can write $\\bm{x}_{t}$ as a function of $\\bm{x}_{0}$:\n$$\\begin{aligned}\\bm{x}_t \u0026= \\sqrt{\\alpha_t} \\bm{x}_{t-1} + \\sqrt{1 - \\alpha_t} \\bm{\\epsilon}_{t-1} \\\\ \u0026= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}\\bm{x}_{t-2} + \\sqrt{1 - \\alpha_{t-1} }\\bm{\\epsilon}_{t-2}) + \\sqrt{1 - \\alpha_t} \\bm{\\epsilon}_{t-1} \\\\ \u0026= \\sqrt{\\alpha_t \\alpha_{t-1}}\\bm{x}_{t-2} + \\sqrt{\\alpha_t (1 - \\alpha_{t-1})}\\bm{\\epsilon}_{t-2} + \\sqrt{1 - \\alpha_t} \\bm{\\epsilon}_{t-1} \\\\ \u0026= \\sqrt{\\alpha_t \\alpha_{t-1}} \\bm{x}_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bm{\\bar{\\epsilon}}_{t-2} \\\\ \u0026= \\cdots \\\\ \u0026= \\sqrt{\\bar{\\alpha}_t} \\bm{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\bm{\\epsilon}\\end{aligned}$$where $\\bm{\\epsilon}$ merges the $n$ gaussian distributions and the third to fourth line uses the fact that if $X \\sim \\mathcal{N}(0, \\sigma^{2}_{1} \\bm{I})$ and $Y \\sim \\mathcal{N}(0, \\sigma^{2}_{2} \\bm{I})$, then $X + Y \\sim \\mathcal{N}(0, (\\sigma^{2}_{1} + \\sigma^{2}_{2}) \\bm{I})$.\nUsing this formula, we now have a tractable way to apply the noise for the sample at any time step.\nBackward Diffusion Process Now that we know how to add noise to a sample, we need to know how to remove noise from a sample. This process is called the backward diffusion process. The goal of the backward process is to learn the conditional probability distribution of $q(\\bm{x}_{t-1}| \\bm{x}_{t})$. However, learning $q(\\bm{x}_{t-1}| \\bm{x}_{t})$ turns out to be intractable (see the definitions section) and needs to be approximated. To show why $q(\\bm{x}_{t-1}| \\bm{x}_{t})$ is intractable we\u0026rsquo;ll start by writing the backward process using Bayes theorem:\n$$q(\\bm{x}_{t-1}| \\bm{x}_{t}) = \\frac{q(\\bm{x}_{t}| \\bm{x}_{t-1}) q(\\bm{x}_{t-1})}{q(\\bm{x}_{t})} = \\frac{q(\\bm{x}_{t}| \\bm{x}_{t-1}) q(\\bm{x}_{t-1})}{\\int q(\\bm{x}_{t}|\\bm{x}_{t-1})q(\\bm{x}_{t-1}) d\\bm{x}_{t-1}}.$$Computing the marginal distribution $q(\\bm{x}_{t})$ requires integrating over a high dimensional space which is generally intractable and the distribution $q(\\bm{x}_{t})$ is often complex and doesn\u0026rsquo;t have a closed form solution.\nConsequently, we need to learn a model $p_{\\theta}$ to approximate the conditional distribution $q(\\bm{x}_{t-1}| \\bm{x}_{t})$ to reverse the noise. This model will take two arguments $\\bm{x}_{t}$ and $t$. $\\theta$ corresponds to the model\u0026rsquo;s parameters the model will learn.\nW .Feller proved in his paper on the theory of stochastic processes that for Gaussian and binomial diffusion processes (with small step sizes $\\beta$), the reversal of these diffusion process also has an identical form to the forward process. This forms the foundation of our theory here. If the forward process uses a small enough step sizes $\\beta_{t}$, then the reverse process $q(\\bm{x}_{t-1} \\| x_{t})$ also follows a Gaussian distribution. Consequently, we only need to create a model to find the parameters $\\mu_{\\theta}(x_{T}, t)$ and $\\sum_{\\theta}(x_{t}, t))$ to estimate the backward diffusion process.\n$$p_{\\theta}(\\bm(x)_{t-1}|\\bm{x}_{t}) = \\mathcal{N}(\\bm{x}_{t-1}|\\mu_{\\theta}(x_{t}, t), \\sum_{\\theta}(x_{t},t)).$$There are many different models we could choose to estimate these parameters. In J, Sohl-Dickstein first application of diffusion models, he used a MultiLayer Perceptron, but more of the modern methods tend to use a Neural Network inspired by U-Net. Using either of these models, the next steps are finding the correct parameters of our NN which minimise the divergence of $p_{\\theta}(\\bm{x}_{0})$ from distribution $q(\\bm{x}_{0})$. This is done by borrowing some of the ideas around Variational Inference (VI) from Bayesian Statistics.\nFigure 2. The reverse diffusion process. Source: Ho et al. https://arxiv.org/pdf/2006.11239\nVariational Inference: In short, VI revolves around a family of techniques used for approximating intractable integrals arising in Bayesian statistics. The goal of variational inference is to find a way to approximate the posterior probability of unobserved variables by using the Evidence lower bound (ELBO). The ELBO provides us with a lower bound on the loss of our approximate distribution. This lower bound is often known as the variational free energy. We won\u0026rsquo;t delve any more into this lower bound here but more details can be found here. The ELBO inequality applied to our situation gives:\n$$\\ln p_{\\theta}(\\bm{x}_{0}) \\geq \\mathbb{E}_{\\bm{x}_{1:T} \\sim q(\\cdot | \\bm{x}_{0})} \\left[ \\ln p_{\\theta} (\\bm{x}_{0:T}) - \\ln q(\\bm{x}_{1:T} | \\bm{x}_{0}) \\right] .$$Taking expectation wrt to $x_{0}$ gives:\n$$\\mathbb{E}_{\\bm{x}_{0} \\sim q} \\left[ \\ln p_{\\theta} (\\bm{x}_{0}) \\right] \\geq \\mathbb{E}_{\\bm{x}_{0:T} \\sim q(\\cdot \\mid \\bm{x}_{0})} \\left[ \\ln p_{\\theta} (\\bm{x}_{0:T}) - \\ln q(\\bm{x}_{1:T} \\| \\bm{x}_{0}) \\right]$$Since the term on the left is fixed, we now have an upper bound. Consequently, we want to maximise the terms on the right by minimising the divergence of $\\ln p_{\\theta} (\\bm{x}_{0:T}) $ from $q(\\bm{x}_{1:T} | \\bm{x}_{0})$. Using this formula, a loss function can be created by multiplying the formula above by negative 1.\n$$L(\\theta) = -\\mathbb{E}_{\\bm{x}_{0:T} \\sim q} \\left[ \\ln p_{\\theta} (\\bm{x}_{0:T}) - q(\\bm{x}_{1:T} | \\bm{x}_{0}) \\right].$$Minimising this loss function is equivalent to minimising the divergence between the two functions. This can be re-written to:\n$$L(\\theta) = \\sum_{t=1}^{T} \\mathbb{E}_{\\bm{x}_{t}, \\bm{x}_{t-1} \\sim q} \\left[ -\\ln p{\\theta} (\\bm{x}_{t-1} | \\bm{x}_{t}) \\right] + \\mathbb{E} \\left[ D_{\\text{KL}} (q(\\bm{x}_{T} | \\bm{x}_{0}) | p_{\\theta} (\\bm{x}_{T})) \\right].$$Since the term $p_{\\theta} (\\bm{x}_{T})$ is completely independent of any parameters, this KL divergence term can be ignored. This gives the simpler loss function to be minimised:\n$$L(\\theta) = \\sum_{t=1}^{T} L_{t} \\ \\text{ where } L_{t} = \\mathbb{E}_{\\bm{x}_{t-1}, \\bm{x}_{t} \\sim q} \\left[ -\\ln p_{\\theta} (\\bm{x}_{t-1} | \\bm{x}_{t}) \\right]. $$Some form of the gradient descent algorithm will be applied to this function in hopes to minimise the loss (and consequently be able to identify the noise). Many lines of code are skipped in this proof (for my sanity), however, I\u0026rsquo;d recommend reading this GitHub post which covers the proofs behind the whole process.\nMuch of this section follows the work from here.\nBack to the backward diffusion step Now that we know what we are trying to minimise, the next step is determining how we\u0026rsquo;re going to do this.\nDeriving a formula for $q(\\bm{x}_{t-1} | \\bm{x}_{t})$ is impossible because this distribution relies on the value $\\bm{x} _{0}$ which is not available during inference. If you think about this intuitively, it will be incredibly difficult to predict the noise of an image without knowing the end image with no noise. However, if we were to condition the distribution on $\\bm{x}_{0}$, we will now have a tractable way to predict the noise. Most the proof for the formulas below will be skipped for now but can be found here.\n$$q(\\bm{x}_{t-1}|\\bm{x}_{t}, \\bm{x}_{0}) = q(\\bm{x}_{t}|\\bm{x}_{t-1}, \\bm{x}_{0}) \\frac{q(\\bm{x}_{t-1} | \\bm{x}_{0})}{q(\\bm{x}_{t} | \\bm{x}_{0}) }$$Since each of these distributions on the left can be expressed as a normal distribution, they can be multiplied together creating another normal distribution with parameters $\\tilde{\\bm{\\mu}}_{t}(\\bm{x}_{t}, \\bm{x}_{0})$ and $\\tilde{\\beta_{t}}$.\n$$\\begin{align*} q(\\bm{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) \u0026= \\mathcal{N}\\left(\\bm{x}_{t-1}; \\tilde{\\bm{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I}\\right) \\\\ \\tilde{\\beta}_t \u0026= \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t \\\\ \\tilde{\\bm{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) \u0026= \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0 + \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t \\end{align*}$$The only issue with conditioning on $\\bm{x}_{0}$ is that it defeats the process of generating samples from random noise. When you generate samples in practice you won\u0026rsquo;t have the clean sample $\\bm{x}_{0}$ . Consequently, we can\u0026rsquo;t use the true sample of $\\bm{x}_{0}$, however it can be estimated. Using the simplified formula from the forward diffusion process we can write $\\bm{x}_{0}$ as a function of $\\bm{x}_{t}$.\n$$\\begin{aligned} \\bm{x}_{t} \u0026= \\sqrt{\\bar{\\alpha}_t} \\bm{x}_{0} + \\sqrt{1 - \\bar{\\alpha}_t} \\bm{\\epsilon} \\\\ \\bm{x}_{0} \u0026= \\frac{\\bm{x}_{t} - \\sqrt{1 - \\bar{\\alpha}_t} \\bm{\\epsilon}}{\\sqrt{\\bar{\\alpha}_t}}. \\end{aligned}$$Substituting this estimation of $\\bm{x}_{0}$ into the function $\\mu$ gives:\n$$\\tilde{\\mu}_t(\\mathbf{x}_t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\bm{\\epsilon} \\right).$$Next we will write $\\bm{\\epsilon}$ as a function of $\\mathbf{x}_{t}$ and $t$:\n$$\\tilde{\\mu}_{\\theta}(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta}(\\mathbf{x}_t, t) \\right).$$Substituting these terms into our loss function from earlier. We get:\n$$\\begin{aligned}L_t \u0026= \\mathbb{E}_{\\mathbf{x}_0, t, \\epsilon} \\left[ \\frac{1}{2 \\|\\Sigma_{\\theta}(\\mathbf{x}_t, t)\\|_2^2} \\|\\tilde{\\mu}_t - \\mu_{\\theta}(\\mathbf{x}_t, t)\\|_2^2 \\right] \\\\ \u0026= \\mathbb{E}_{\\mathbf{x}_0, t, \\epsilon} \\left[ \\frac{\\beta_t^2}{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\|\\Sigma_{\\theta}\\|_2^2} \\|\\epsilon_t - \\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)\\|_2^2 \\right]\\end{aligned}$$With these substitutions and rearrangements, we have simplified the problem. Now we only need to estimate the variance term ($\\Sigma_{\\theta}$) and the noise term ($\\bm{\\epsilon}$) term for every timestep $t \\in \\left[1, T\\right]$.\nThese loss functions was further simplified in the paper by Ho et al., where he removed the weighting term giving the final loss function:\n$$L_{t}^{\\text{simple}} = \\mathbb{E}_{x_{0},t,\\epsilon} \\left[ \\left| \\epsilon - \\epsilon_{\\theta} \\left( \\sqrt{\\bar{\\alpha}t} \\bm{x}_{0} + \\sqrt{1 - \\bar{\\alpha}_{t}} \\bm{\\epsilon}, t \\right) \\right|^{2} \\right].$$Note that of the math in this section follows the website here.\nDDPM Specifics During testing Ho et al. found that when they tried estimating the variance term $\\beta_{t}$, model training became unstable and model sample quality deteriorated. Ho et al. observed better performance when $\\sum_{\\theta} (\\bm{x}_{t}, t) = \\sigma^{2}_{t} \\bm{I}$ and $ \\sigma^{2} _{t}$ were fixed to the hyper-parameter $\\beta_{t}$.\nTo ensure the reverse process could be modelled as a gaussian distribution Ho et al. decided to use a linearly increasing step size $\\beta_{t}$ from $\\beta_{0} = 10^{-4}$ to $\\beta_{T} = 0.02$ with $T$ chosen to be 1000 steps. It\u0026rsquo;s worth mentioning many believe these choices of $\\beta$ were quite low given the fact that the image pixel values were in the interval $\\left[-1, 1\\right]$. This parameter was changed on the next iterations of this model.\nTo estimate the noise $\\bm{\\epsilon}_{t}$ at each timestep $t \\in \\left[1, T\\right]$ a U-Net inspired architecture from Ronneberger et al. was used.\nFigure 3. The U-Net Architecture. Source: Ronneberger et al.\nHowever, there were a few key differences to the generic U-Net model.\nResidual Blocks: The first difference were in the type of convolution blocks used. Unlike typical U-Net blocks which do not incorporate skip-connections, Ho et al. added skip-connections to these blocks. Skip connections are often used to help improve gradient flow and avoid the vanishing or exploding gradient problem. Attention Mechanisms: Second, self-attention layers were incorporated into the U-Net. Attention mechanism help the model capture long-term relationships. Group Normalization: Instead of Batch Normalization, Group Normalization was used to stabilize the training process. In most situations, Batch Normalisation should be used instead of Group Normalisation. However in the setting where the input data is not IID, then group normalisation is preferred. A common example where the data is not IID occurs when the data comes from different classes. Sinusoidal Position Embeddings: Finally a sinusoidal position embedding is used to encode the timesteps. This information is then added to the input of every residual block. This of course takes inspiration from the positional encodings used in Transformers. Conclusion This article introduced the idea of diffusion models and delved into their mathematical foundation. The diffusion model DDPM was used as an example.\nDefinitions Intractability: A problem which can be solved in theory (given large enough but finite resources, and time) but requires too many resources to be useful. These group of problems are known as intractable. On contrary, a problem that can be solved in practice is known as a tractable problem.\nLatent Variables: Latent variables are inspired by the latin word \u0026ldquo;lateo\u0026rdquo; which means \u0026ldquo;lie hidden\u0026rdquo;. These are a class of variables that are not directly observed or measured. They are only inferred through the observed variables. It\u0026rsquo;s best thinking of these variables as hidden variables.\nReferences:\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S. and Edu, S. (n.d.). Deep Unsupervised Learning using Nonequilibrium Thermodynamics. [online] Available at: https://arxiv.org/pdf/1503.03585.\nHo, J., Jain, A. and Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. [online] Available at: c.\nAdaloglou, S. K. . N. (2022, September 29). How diffusion models work: the math from scratch | AI Summer. AI Summer. https://theaisummer.com/diffusion-models/\nWikipedia contributors. (2024, July 25). Diffusion model. Wikipedia. https://en.wikipedia.org/wiki/Diffusion_model\nWeng, L. (2021, July 11). What are Diffusion Models? Lil’Log. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n‌\n‌\n","permalink":"https://www.mathstoml.com/diffusion-models-ddpm/","tags":["Diffusion Models","Auto Encoders"],"title":"Diffusion Models (DDPM)"},{"categories":["Algorithms"],"contents":" Image by Danny Greenberg\nSummary This article explores dynamic programming (DP), a technique used to tackle complex problems in computer science. We will specifically apply DP to two problems involving palindrome partitioning.\nRequirements\nA basic understanding of dynamic programming and some experience with DP problems are recommended before reading this article.\nDefinitions\n• Palindrome: A string is a palindrome if it reads the same backward as forward. For example, “aba” is a palindrome, while “aab” is not.\n• Substring: A substring is a contiguous sequence of characters within a string.\nQ1: Palindromic Substrings Before delving into this question I\u0026rsquo;d recommend attempting it on Leetcode here.\nThis question is as follows. Given a string s, return the number of palindromic substrings in it.\nBrute Force Approach\nAt first glance, a brute force approach might seem viable. For a string like abc, you would check each of its 6 substrings (a, b, c, ab, bc, and abc) to see if they are palindromes. Given a string of length $n$, there are $\\frac{n(n-1)}{2}$ possible substrings to check. Each palindrome check operation is $\\mathcal{O}(n)$, leading to an overall complexity of $\\mathcal{O}(n^3)$ for the brute force algorithm. While this approach works for small values of $n$, it becomes computationally expensive as $n$ grows larger.\nTo achieve a lower complexity, we can consider more efficient strategies. Dynamic programming is one such approach that can significantly reduce the time complexity. By storing intermediate results and avoiding redundant calculations, dynamic programming can optimise the process of checking palindromes and solving related problems.\nDP Approach\nGiven that dynamic programming (DP) can be applied to this problem, let’s explore how it can be utilized effectively. To apply DP, we need to ensure the problem meets the two key requirements: optimal substructure and overlapping sub-problems.\nOverlapping Sub-Problems\nTo illustrate overlapping sub-problems, consider a string like aabc. If we know whether a substring like ab is a palindrome, this information can help determine if aabc is a palindrome. Specifically, since ab is not a palindrome, aabc cannot be a palindrome either. This principle also extends to longer strings. For example, if ab is not a palindrome, then aaabca cannot be a palindrome. This shows that smaller sub-problems overlap and their solutions contribute to solving the larger problem.\nIn general, if the middle portion of a string is not a palindrome, the whole string cannot be a palindrome. Conversely, if the first and last characters of a string are the same, there is a possibility that the string could be a palindrome, provided the substring between them is also palindromic. If the first and last characters do not match, the string is definitely not a palindrome.\nOptimal Substructure\nOptimal substructure means that the solution to a problem can be constructed from the solutions to its sub-problems. For palindrome checking, the problem can be broken down by determining whether a substring is palindromic or not. The optimal solution to any of the subproblem is either a True or False boolean. Consequently, any solution to the subproblem is the optimal solution. Given the solution to this problem can be constructed using overlapping sub-problems and optimal substructure, DP can be used, more specifically we\u0026rsquo;ll be implementing bottom-up DP. We\u0026rsquo;ll illustrate this example by using the string abaa as the example problem for the rest of this section.\nWe\u0026rsquo;ll create bottom-up DP table where each index in the array corresponds to a substring of the string abaa. Specifically, row index i and column index j corresponds to the substring from index i to j .\n0 1 2 3 0 1 2 3 Let’s start by filling out the DP table for substrings of length 1. Since every single character is inherently a palindrome, the cells corresponding to substrings of length 1 will all be True.\n0 1 2 3 0 T 1 T 2 T 3 T For substrings of length 2, we check whether each pair of adjacent characters in the string are the same. A substring of length 2 is a palindrome if and only if both characters are identical.\n0 1 2 3 0 T F 1 T F 2 T T 3 T Now this is where the algorithm could get tricky to understand. Starting at string from index 0 to index 2 (aba). In this example, we first check that the first and last letter align. Next we would need to check if the substring between the first and last letter is a palindrome (substring b). Since we have computed smaller subproblems and stored their values within the grid we can check position row 1 and column 1. This is True. Hence (aba) is a palindrome.\nNext we\u0026rsquo;ll look at substring from index 1 to 3 (baa). Since the first and last letters do not align, we know this substring cannot be a palindrome. So we fill the corresponding grid position with False.\n0 1 2 3 0 T F T 1 T F F 2 T T 3 T The final substring to check is from index 0 to 3 (abaa). Since the first and last letters align, we\u0026rsquo;ll next check if the substring ba is a palindrome. To determine this we check the cell in the grid corresponding to row 1 and column 2 (substring ba). This gives a value of False, so we know abaa is not a palindrome.\n0 1 2 3 0 T F T F 1 T F F 2 T T 3 T Summing over the number of True values in the grid above tells us that there are 6 substring palindromes.\nThis application of DP may seem trivial however, we have now reduced the run-time complexity of the algorithm to $\\mathcal{O}(n^{2})$. This is a consequence of storing the smaller subproblems which are used to solve the larger palindromes later on.\ndef countSubstrings(self, s: str) -\u0026gt; int: n = len(s) dp = [[False]*n for _ in range(n)] ans = n for i in range(n): dp[i][i] = True for j in range(i): if s[j] == s[i] and (i-j\u0026lt;=2 or dp[j+1][i-1]): dp[j][i] = True ans += 1 return ans Q2: Palindromic Partitioning II Given a string s, partition s such that every substring of the partition is a palindrome.Return the minimum cuts needed for a palindrome partitioning of s.\nSimilar to the question above, I\u0026rsquo;d recommend trying out this question beforehand here.\nThere are 3 different approaches to solving this problem. First the brute force approach, second the DP approach, and finally the optimised DP approach.\nBrute Force Approach\nGiven a string of length n, there are n-1 potential places to cut the string, which gives $2 ^{n}$ ways we could cut the string. Checking that each cut has produced palindromes is an $\\mathcal{O}(n)$ operation. As a result, this method has a complexity of $\\mathcal{O}(n 2 ^{n})$. For large n this method is computationally expensive.\nDP Approach\nSimilar to the last question, we\u0026rsquo;ll check to see if DP is appropriate here by checking for optimal substructure and overlapping sub-problems.\nOptimal Substructure\nThe optimal substructure is somewhat harder to recognize. For a substring abc of string abcd, the optimal number of cuts for abc (2 cuts) can be used to determine the optimal number of cuts for abcd (3 cuts). However, this approach has a complication due to edge cases where palindromic strings contain non-palindromic substrings. For example, the substring ab of string abba is not palindromic, however its parent string is a palindrome. Therefor, the number of cuts for ab does not need to be used in string abba.\nOverlapping Subproblems\nThe overlapping subproblems should be easy to spot. For example, if we have a non-palindromic string abcd, the number of cuts needed for the substring abc (2 cuts) could be used to help determine the number of cuts needed to cut string abcd (3 cuts). Similarly, the cuts from the substring abc can help determine the number of cuts needed for the substring aabcd. Consequently, there are overlapping subproblems.\nTo implement DP for this problem, we need to create two arrays. The first array will indicate whether substrings are palindromic or not, using the same method as in the first question. This array helps handle edge cases where the entire string is a palindrome but its substrings are not.\nThe second array is a 2D DP array that will determine the number of cuts needed for any substring. Similar to the previous question, each cell in this array corresponds to a substring starting at index x and ending at index y in the original string. The value in the cell represents the optimal number of cuts required for that substring.\nTo demonstrate this method, we will be trying to figure out the optimal number of cuts needed for string abaa such that each cut substring is palindromic.\n0 1 2 3 0 1 2 3 Similar to the last question, we\u0026rsquo;ll start with the substrings of length 1. Since every string of length 1 is palindromic, 0 cuts are needed.\n0 1 2 3 0 0 1 0 2 0 3 0 Now we move onto the strings of length two starting from indexes 0 to 1 (substring ab). Checking the corresponding index positions in the first DP array tells us that this string is not a palindrome. So a cut is needed. Perform the same procedure for the two other substrings of length 2 gives:\n0 1 2 3 0 0 1 1 0 1 2 0 0 3 0 The final substring between indexes 2 and 3 (aa) required 0 cuts. This is because when we checked the corresponding indexes in our first DP array, it indicated that the substring was a palindrome, so no cuts were needed.\nNext, we check substrings of length 3, starting from indexes 0 to 2 (aba). By referencing the first array, we see that this substring is also a palindrome, so 0 cuts are needed.\nFor the next substring (baa), we know it is not a palindrome. To determine the optimal cuts, we loop through the potential cut positions and evaluate which cut yields the best result. Let’s start with a cut between b and aa. Checking row 1 and column 1 in the DP array, we find that the substring b requires 0 cuts to be a palindrome. Similarly, checking row 2 and column 3 shows that aa also requires 0 cuts. Therefore, making a cut here would result in 1 cut to make the entire substring a palindrome.\nNext, we check the cut between ba and a. The corresponding indexes in the array show that ba requires 1 cut to become a palindrome, while a requires 0 cuts.\nConsequently, the minimum number of cuts for the substring baa is 1, which occurs when we cut between b and aa. We fill the DP cell with this value, as it represents the fewest additional cuts needed.\n0 1 2 3 0 0 1 0 1 0 1 1 2 0 0 3 0 For the final string we again check the first DP array to see if it\u0026rsquo;s a palindrome. It is not, so now we have to go through the 3 cut positions to see which produces the fewest remaining cuts.\nThe cut positions we\u0026rsquo;ll check are a and baa, ab and aa, and aba and a. Checking the corresponding substring positions in the DP array tells us that substring aba and a require the fewest number of additional cuts (0 additional cuts). So we\u0026rsquo;ll fill in the cell with 1 (for our first cut).\n0 1 2 3 0 0 1 0 1 1 0 1 1 2 0 0 3 0 Checking the final value at row 0 column 3 gives us 1. So 1 cut is needed to make all substrings palindromic.\ndef minCut(self, s: str) -\u0026gt; int: n = len(s) dp_pali = [[False]*n for _ in range(n)] for i in range(n): dp_pali[i][i] = True for j in range(i): if s[j] == s[i] and (i-j\u0026lt;=2 or dp_pali[j+1][i-1]): dp_pali[j][i] = True dp_cuts = [[float(\u0026#39;inf\u0026#39;)]*n for _ in range(n)] for i in range(n): dp_cuts[i][i] = 0 for j in range(1, n): for i in range(j, -1, -1): if dp_pali[i][j]: dp_cuts[i][j] = 0 for k in range(0, j): dp_cuts[i][j] = min(1+dp_cuts[i][k]+dp_cuts[k+1][j], dp_cuts[i][j]) return dp_cuts[0][-1] This method works but under time complexity requirements, this method needs to be further optimised. This algorithm suffers from complexity $\\mathcal{O}(n^{3})$ as a consequence of $\\mathcal{O}(n^{2})$ substrings being checked, with each check being an $\\mathcal{O}(n)$ operation.\nI\u0026rsquo;d recommend watching the video by Tushar here to gain a deeper understanding of this question.\nOptimised DP Approach\nThis optimised DP approach reduces the complexity of the previous DP approach to $\\mathcal{O}(n^{2})$ by reducing the complexity of the partitioning cuts from $\\mathcal{O}(n^{3})$ to $\\mathcal{O}(n^{2})$.\nRather than checking every substring to determine the minimum number of cuts needed, only the $n$ substrings starting from index 0 will be checked. For example, for the string abaa we will only check string a, ab, aba, and abaa to determine the minimum number of cuts needed.\nWe\u0026rsquo;ll create a 1 dimensional DP array, where index x corresponds to the minimum cuts needed to make a substring from index 0 to x a palindrome and we\u0026rsquo;ll also be using our DP array from question 1 to determine whether the substring is a palindrome or not.\nIterate through the $n$ substrings from smallest to largest. If the substring is a palindrome (found using our first DP array) then we know 0 cuts are needed. So we\u0026rsquo;ll set the corresponding column index to 0. If we are at index x and the substring from index 0 to x is not a palindrome then we iterate through the indexes from 0 to x checking to see if the substring from our new index to x is a palindrome. If so we know 0 cuts are needed to convert the subsequent string to a palindrome and we\u0026rsquo;ll know the minimum cuts needed to make the previous substring a palindrome (since it was stored in our DP array in a previous iteration of this method).\nThis is best explained with an example. Let\u0026rsquo;s use the final substring abaa. Since this is not a palindrome we\u0026rsquo;ll iterate from 0 to x, checking to see if the subsequent substring is a palindrome. baa is not, so we go to the next index. aa is a palindrome. With this information we know that no more cuts are needed to make the subsequent substring (aa) a palindrome and we have stored the minimum cuts needed to make ab a palindrome from a previous iteration.\ndef minCut(self, s: str) -\u0026gt; int: n = len(s) dp_pali = [[False]*n for _ in range(n)] for i in range(n): dp_pali[i][i] = True for j in range(i): if s[j] == s[i] and (i-j\u0026lt;=2 or dp_pali[j+1][i-1]): dp_pali[j][i] = True dp_cuts = [float(\u0026#39;inf\u0026#39;)]*n for i in range(n): if dp_pali[0][i]: dp_cuts[i] = 0 else: for j in range(i): if dp_pali[j+1][i]: dp_cuts[i] = min(dp_cuts[i], 1 + dp_cuts[j]) return dp_cuts[-1] Conclusion This article delved into 2 DP approaches used to solve problems related to palindrome partitions.\n","permalink":"https://www.mathstoml.com/dynamic-programming-in-solving-palindrome-partitioning/","tags":["Dynamic Programming","Algorithms"],"title":"Dynamic Programming in Solving Palindrome Partitioning"},{"categories":["Graphs"],"contents":" Image by Cajeo Zhang Graphs have been used across many fields due to their ability to represent relationships between entities with applications including social networks, search engines, and protein-protein interaction networks. However, one growing limitation of these graphs are the amount of computational resources they require with some large-scale graphs having millions of nodes each with their own set of features and their set of edges.\nThis has led to the creation of graph embedding methods, more specifically the deep embedding methods. These embedding methods aim to create a high-quality representation of the nodes and their edges. Rather than just incorporating the graph structural information into an embedding, these methods also include node and edges features and other hierarchical information. This results in a complicated model which are able to learn very rich representations of nodes.\nDeep embedding methods use neural networks called graph neural networks (GNNs) to create these deep embeddings. GNNs consists of many layers, with each layer consisting of a message passing (neighbourhood aggregation) system and an update function. The message passing aggregates the information from its neighbours while the update function use these aggregations to update the node\u0026rsquo;s current representation within the layer.\nThere are many different GNNs with many specialised for different tasks. Some incorporate temporal dynamics, others incorporate attention mechanisms, some are built for different scales, etcetera. From these GNNs, today we will be looking at GraphSage by Hamilton et al..\nInductive vs Transductive Methods From the GNN node embeddings methods, they can often be split into 2 categories: transductive and inductive methods. Transductive methods create embeddings from the nodes it has seen during training but they are unable to generalise to nodes they have not seen. To create embeddings for new nodes, the GNN would have to be retrained on these new nodes. On the other hand, the inductive methods can generalise to nodes they have never encountered before. As a result the inductive methods are computationally efficient compared to transductive methods and perform better with dynamic graphs.\nGraphSage (SAmple and aggreGatE) was one of the first successful inductive node embedding algorithms. It was inspired by a variant of the Graph Convolution Network (GCN), another inductive GNN method.\nRather than training a distinct embedding vector for each node, a set of aggregate functions are trained to incorporate the structural and node features.\nGraphSage introduced 2 methods: the embedding generation method (called the forward propagation algorithm) and model parameter training method.\nForward Propagation Algorithm Given a Graph $\\mathcal{G}(V, \\mathcal{E})$ where $V$ corresponds to the domain of nodes and $\\mathcal{E}$ corresponds to the domain of edges. The embedding generation method follows:\nFigure 1. The forward propagation algorithm. Source: https://arxiv.org/pdf/1706.02216\nThe intuition behind this approach is as each iteration continues, the nodes continue to aggregate information from their local neighbours and stores the information for node $v$ in layer $k$ of the algorithm in an embedding vector $h_{v}^{k}$. As this process continues, more information about the whole graph is aggregated.\nModel Parameter Training To allow this method to work in the mini batch setting, given a mini batch of size $m$ and it\u0026rsquo;s batch of nodes $\\{ v_{1}, ..., v_{m} \\}$, rather than iterating over all nodes in the graph, only the nodes needed up to a depth K are used.\nTo train the parameters of the model, Hamilton et al. used a loss function inspired by vector calculus. Given a node $u$ and it\u0026rsquo;s final node embedding of $z_{u}$, the loss function can be defined as:\n$$J_{\\mathcal{G}}(z_{u}) = -\\log(\\sigma(z_{u}^{T} z_{v})) - Q\\cdot \\mathbb{E}_{v _{n} \\sim P_{n}(v)} \\log (\\sigma(-z_{u}^{T} z_{v_{n}})).$$where $v$ is a node that co-occurs near $u$ on a fixed-length random walk, $\\sigma$ is the sigmoid activation function, $P_{n}$ is the negative sampling distribution, and $Q$ defines the number of negative samples. This form of loss encourages nodes which close to $u$ to have similar embeddings, while nodes far away to be distinguishable. To see this, we\u0026rsquo;ll write the loss function using the sigmoid activation function.\n$$J_{\\mathcal{G}}(z_{u}) = \\log(1 + e^{-z_{u}^{T} z_{v}})) + Q\\cdot \\mathbb{E}_{v _{n} \\sim P_{n}(v)} \\log (1 + e^{z_{u}^{T} z_{v_{n}}})).$$The term on the left: $1 + e^{-z_{u}^{T} z_{v}}$ achieves a maximum of 2 when the vectors are orthogonal and when the vector embeddings are parallel, the minimum of $1 + e^{-1}$ is achieved.\nSimilarly, the term $1 + e^{z_{u}^{T} z_{v_{n}}}$ is minimised with the value of 1 when the vectors are orthogonal and maximised when the vectors are parallel.\nThe key difference to previous method which have implemented this form of loss is that here, we restrict the embeddings $z_{u}$ to be the embeddings generated from the features contained within a node\u0026rsquo;s local neighbourhood, unlike generating a unique embedding for each node.\nGraphSage Aggregation Functions Similar to other GNN algorithms, GraphSage\u0026rsquo;s aggregator function had to be symmetric, trainable, and have the ability to provide a rich representational capacity. So GraphSage experimented with 3 different aggregator functions: the mean aggregator, LSTM aggregator, and the pooling aggregator.\nMean Aggregator\nGiven embedding vectors$\\{ h_{u}^{k-1}, \\forall u \\in \\mathcal{N}(v) \\}$, the mean aggregator for the embedding for node v and layer k is written as:\n$$ h_{v}^{k} \\leftarrow \\sigma (W \\cdot \\text{MEAN}( \\{h_{v}^{k-1}\\} \\cup \\{ \\{h_{u}^{k-1}, \\forall u \\in \\mathcal{N}(v) \\})).$$This method is often referenced as the mean based aggregator convolutional since it is rough linear approximation of the local spectral convolution (see the appendix).\nLSTM Aggregator\nAnother experimented aggregator function was the LSTM (see the appendix) aggregator. Here LSTMs have the advantage of having more expressive capabilities but suffers from its main limitation of being inherently non-symmetric due to their sequential processing. To help the LSTM overcome this limitation, a random permutation of the node\u0026rsquo;s neighbours are inputted to LSTM.\nPooling Aggregator\nThe final of the 3 approaches is the pooling approach where a neighbouring embeddings are fed independently through a neural network:\n$$\\text{AGGREGATE}_{k}^{\\text{pool}} = \\max(\\{ \\sigma(W _{\\text{pool}} h_{u_{i}}^{k} + b), \\forall u_{i} \\in \\mathcal{N}(v) \\}),$$where the max is the element-wise max operator and sigma is a non-linear activation function.\nResults The authors Hamilton et al. tested the performance on 3 benchmark datasets, classification of academic papers, classifying Reddit posts, and classifying protein functions. For each dataset, the 3 variants of GraphSage were trained along with using a variant inspired by the GCN. Each of these methods were compared to DeepWalk by Perozzi et al. (see the appendix).\nTwo further variants of the GraphSage model were tested: the unsupervised model which was trained on the loss function defined above and the supervised model which was trained on the cross-entropy loss function. For further insights into the experimental details, visit Hamilton et al.\u0026rsquo;s paper.\nFrom the results, each of the tested aggregators exceeded the performance of the previous state of the art methods with the GraphSage-LSTM and GraphSage-pool achieving the highest results.\nConclusion This article delved into GraphSage, an inductive node embedding method with applications in large scale graph.\nAppendix Spectral Convolution: In spectral graph convolution, an eigen decomposition of the Laplacian matrix is performed, which gives us the information related to the underlying structure of the graph. This operation is performed on the Fourier space and finds the smallest eigenvalues as these explain the graph structure better in Spectral Convolution. ChebNet is an example of a method which has applied the idea spectral convolutions with a focus on local spectral convolutions.\nLocal Spectral Convolution: The difference between spectral convolutions and local spectral convolutions is that the local methods focus more on the informations from a node\u0026rsquo;s immediate neighbourhood rather than the entire graph. As a result these methods are more computationally efficient and often provide more practical relevance. The Graph Convolution Network (GCN) used a first-order approximation of the spectral convolution which lead to localised operations. Here the full-spectral convolution can be written as:\n$$h_{v} = \\sum_{u\\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{d_{u}d_{v}}} h_{j},$$where $d_{v}$ and $d_{u}$ correspond to the degrees of nodes v and u.\nSpatial Convolution: These methods work on local neighbourhoods of nodes to understand their properties based on the k neighbours. Unlike spatial convolutions, these methods are much simpler to implement. GraphSage is an example of Spatial Convolution.\nNegative Sampling: Negative sampling is a technique in machine learning which is designed to enhance the efficiency of the models by selecting a small subset of negative samples from a pool of negative samples. This form of loss function is defined the maximise the similarity between the positive pairs and minimise the similarity between the negative pairs. As a result, this form of model should be able to distinguish between positive and negative samples, in our case, between nodes close and far apart.\nDeepWalk: DeepWalk is a shallow node embeddings method which uses a random walk to capture the nodes structure. These random walks sequences are then applied to the Skip-Gram model to predict nearby nodes. Using the Skip-Gram model, dense vector embeddings for the nodes are generated where nodes that co-occur have a similar embedding.\nLSTM: An LSTM is a type of recurrent neural network (RNN) that is designed to better capture and retain information while avoiding the vanishing gradient problem. There LSTM is composed of 3 gates, the forget gate, input gate, and output gate, with each gate controlling the amount of information to be stored or forgotten during each step.\nReferences Hamilton, W., Ying, R. and Leskovec, J. (n.d.). Inductive Representation Learning on Large Graphs. [online] Available at: https://arxiv.org/pdf/1706.02216 [Accessed 11 Sep. 2024].\nPerozzi, B., Al-Rfou, R. and Skiena, S. (n.d.). DeepWalk: Online Learning of Social Representations. [online] doi:https://doi.org/10.1145/2623330.\nHochreiter, S. and Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), pp.1735–1780. doi:https://doi.org/10.1162/neco.1997.9.8.1735.\nin (2019). What is the difference between graph convolution in the spatial vs spectral domain? [online] Artificial Intelligence Stack Exchange. Available at: https://ai.stackexchange.com/questions/14003/what-is-the-difference-between-graph-convolution-in-the-spatial-vs-spectral-doma [Accessed 11 Sep. 2024].\n‌\n","permalink":"https://www.mathstoml.com/graphsage/","tags":["GNN"],"title":"Graph Sage"},{"categories":["Graphs"],"contents":"\nImage by Elena Mozhvilo\nSummary Graphs are incredibly useful for modelling a range of relationships and interactions. Using nodes to represent entities and edges to represent connections between these entities, they have become a very useful representation tool. Nowadays they are used to model social networks, protein-protein interactions, recommendations systems, knowledge graphs, supply chains, and so much more. However, as these graphs scale up and add more nodes and edges, a range of issues start to arise. They start to become computationally expensive to process, noisy, and difficult to interpret.\nTo overcome some of these issues, embedding algorithms were invented. In essence, these algorithms reduce the graph data to a lower dimension while also creating a rich lower dimensional representation by incorporating structural and semantic information in the form of a vector. These methods are scalable to massive graphs and are compatible with standard machine learning models.\nToday\u0026rsquo;s article will look into these embeddings methods. We\u0026rsquo;ll first start by briefly covering the graph embedding problem and the role that encoder-decoder models play it this setting. Next, we will quickly glance over the difference between shallow and deep embeddings methods before delving deeply into the graph factorisation shallow embedding methods. Some of the graph factorisation methods we will be covering include Laplacian Eigenmaps, Graph Factorisation, GraRep, and more. The other group of shallow embedding methods, namely the random walk methods will be covered in a later article.\nThis article should serve as an introduction into some of the shallow graph embedding techniques. I\u0026rsquo;d highly recommend reading the papers attached in the references section. There they delve into these methods at a much greater detail then we will be diving into today.\nWhat are Node Embeddings? Let\u0026rsquo;s start with the basics. What are node embeddings? In short, they are a lower dimensional representation of the node\u0026rsquo;s position and the structure of their local graph. The process of mapping these nodes to this lower dimension is often referred to as encoding nodes.\nThese node embedding strategies fall into 2 groups: shallow embeddings and deep embeddings.\nShallow embedding methods focus on learning low-dimensional embeddings without using complex mechanisms. As a result, shallow embedding methods are often simpler and more scalable than the deep learning-based methods. Shallow embedding methods focus on node proximity, first-order proximity, second-order proximity, and edge weights (for weighted graphs). But they lack the complexity requires to capture complex patterns. Some of the disregarded pieces of information include node features, edge features, edge types, graph global structure, and higher-order relationships.\nThe basic strategy around these methods involve factoring the adjacency matrix or some variant of the adjacency matrix. The factored matrices are then mapped to a lower dimensional representation. Some of the well known shallow embedding methods include DeepWalk, LINE, and node2vec.\nDeep embedding methods use neural network to learn graph representations. These methods are more expressive and are able to learn rich embeddings from incorporating graph information and structure. Methods which fall into this category are called Graph Neural Networks (GNNs). Example GNNs include Graph Convolution Networks, GraphSAGE, Graph Attention Networks (GAT), and Variational Graph Autoencoders (VGAE). These methods will not be covered today but will be covered in a later article.\nTo create these lower-dimensional embeddings, an encoder-decoder framework is needed. The encoder takes the graphs node and maps the nodes to a the latent space as a lower dimensional vector. The decoder does the opposite, it takes the lower-dimensional vector from the latent space and maps it back to its higher dimensional vector.\nEncoder Given nodes in vector space $ v \\in \\mathcal{V}$ and an encoder $\\text{ENC}:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{d}$, where $d \u003c n$, the encoder maps these vectors $v$ to a lower dimensional space $\\text{ENC}(v) = \\bm{z}_{v}$.\nFigure 1. Example encoder creating node embeddings. Source: William, W.L. (2022). Graph Representation Learning. Springer Nature.\nDecoder The decoder goal is opposite to the encoder. It tries to reconstruct the node and graph properties from the node embeddings. For example, the decoder $DEC$ may try to determine the vector $v$\u0026rsquo;s neighbour given the embedded vector $\\bm{z}_{v}$.\nIn practice, the decoders are used to determine the similarity between two pairs of nodes. More specifically, given two lower dimensional vectors $\\bm{z}_{u}$ and $\\bm{z}_{v}$, we say the decoder is trying to reconstruct the relationship between nodes $u$ and$v$.\nFigure 2. An example encoder-decoder procedure. Source: William, W.L. (2022). Graph Representation Learning. Springer Nature.\nThe general goal of encoder-decoder based shallow embedding methods is to minimise the encoder and decoder reconstruction loss. In other words, minimise the difference between the original data $\\bm{S}$ and reconstructed data. Mathematically, this is written as:\n$$\\text{DEC}(\\text{ENC}(u), \\text{ENC}(v)) = \\text{DEC}(\\bm{z}_u, \\bm{z}_v) \\approx \\bm{S}[u, v], $$where $\\bm{S}[u, v]$ is the graph-based similarity measure between nodes and tells us how nodes are connected. There are many different definitions for the similarity matrix based on which algorithm is applied. Common similarity matrices include the adjacency matrix, laplacian matrix, and the node transition probability matrix. For the rest of this paper we will use the matrix $\\bm{S}$ to denote the similarity matrix.\nIn the problem of finding whether nodes are neighbours, the similarity measure would correspond to the adjacency matrix $\\bm{A}$, i.e. $\\bm{S}[u, v] \\stackrel{\\triangle}{=} \\bm{A}$.\nNote that the symbol $\\stackrel{\\triangle}{=} $ should be read as \u0026ldquo;defined as\u0026rdquo;.\nOptimisation To optimise our encoder-decoder model we need to minimise the empirical reconstruction loss over all the set of node pairs $\\mathcal{D}$. In other words, we need to minimise the error between the original data and its reconstruction.\n$$\\mathcal{L} = \\sum_{(u, v) \\in \\mathcal{D}} \\ell (\\text{DEC}(\\bm{z}_{u}, \\bm{z}_{v}), \\bm{S}[u, v]),$$where $\\ell$ denotes the loss function. Common loss functions include the mean square error and the cross entropy loss.\nNow that we have set the problem, the next step is to choose the decoder, similarity measure, and loss function. There are 2 groups of methods to choose from: factorisation based approaches and random walk embeddings. Today we will be focusing on the factorisation based approaches.\nFactorisation Based Approaches Some of the earliest graph based embedding methods were based on using factorisation to approximate a pair of node\u0026rsquo;s similarity. Here, a factorisation of the similarity matrix can be used to encode the data to a lower dimension. These factorisation methods include:\nSingular Value Decomposition (SVD). Non-Negative Matrix Factorisation (NMF). Alternating Least Squares (ALS). Within the factorisation based approaches, we\u0026rsquo;ll look into 4 methods: Locally Linear Embeddings (LLE), Laplacian Eigenmaps, Graph Factorisation, and GraRep.\nLocally Linear Embedding\nThe oldest of these methods is the Locally Linear Embeddings (LLE). This non-linear dimensionality reduction technique assumes each node lies near a smooth non-linear manifold of lower dimension. Consequently, there exists a translation, rotation, or rescaling method that maps the high dimensional data to the lower dimension manifold\u0026rsquo;s neighbourhood.\nFigure 3. An example of LLE mapping a three dimensional space to a 2 dimensional space. Source: Roweis, S.T. (2000). Nonlinear Dimensionality Reduction by Locally Linear Embedding. Science, 290(5500), pp.2323–2326. doi:https://doi.org/10.1126/science.290.5500.2323.\nLLE starts by acquiring the k nearest neighbours for each node. With the node\u0026rsquo;s neighbours found, weights are then assigned to these neighbouring nodes that best reconstruct each node from its neighbours in the high dimensional space. Specifically, for node $x_{i}$, the goal is to find the weights $w_{ij}$ such that $x_{i} \\approx \\sum_{j \\in \\mathcal{N}_{i}} w_{ij} x_{j}$, where $\\mathcal{N}_{i}$ denotes the neighbourhood for node $x_{i}$ . This problem is referred to as finding the weights which minimise the reconstruction error.\n$$\\argmin_{w}\\sum_{i}\\|x_{i} - \\sum_{j} w_{ij} x_{j}\\|^{2}$$, under the constraints that $\\sum_{j} w_{ij} = 1$.\nMinimising the reconstruction error for all data points gives weight matrix $W \\in \\mathbb{R}^{n\\times n}$, where $n$ denotes the number of nodes. This weight matrix is next used to create the Laplacian matrix by $L = (I-W)^{T}(I-W)$ where $I$ is the identity matrix. More details about Laplacian matrices can be found in the appendix.\nNow that we have found our Laplacian $L$, the next step is to find the eigenvectors of our Laplacian $L$ with the corresponding d smallest non-zero eigenvalues. Solving this eigenvalue problem gives us the smallest eigenvectors to map the data points to a lower dimensional space, i.e. $L$ is approximated by $L_{d}$ where the columns of $L_{d}$ are the eigenvectors with the first d non-zero eigenvalues.\nIn most dimensional reduction algorithms, the algorithms tend to search for the largest eigenvalues, however in LLE they search for the d smallest non-zero eigenvalues. This is a consequence of the Rayleitz-Ritz theorem where the smallest eigenvalues correspond to smooth variations across the graph. This assumption implies that the connected nodes will have similar eigenvectors.\nThis is very different from the PCA method which captures the largest eigenvalues. PCA aims to reduce the dimensionality of the data by capturing the most variance. To do this PCA finds the largest eigenvalues as they correspond to the greatest variance. This is different to LLE which focuses on preserving smoothness in the data.\nLaplacian Eigenmaps\nThe second of these methods we\u0026rsquo;ll look into today are called the Laplacian Eigenmaps. Unlike LLEs, Laplacian Eigenmap methods define the decoder on the L2 distance between the node embeddings. More specifically,\n$$\\text{DEC}(\\bm{z}_{u}, \\bm{z}_{v}) = \\| \\bm{z}_{u} - \\bm{z}_{v} \\|_{2}^{2}.$$This decoder punishes node embeddings which are far apart. Next, the loss function is given by the product of the decoder and the similarity matrix:\n$$\\mathcal{L} = \\sum_{(u, v) \\in \\mathcal{D}} \\text{DEC}(\\bm{z}_{u}, \\bm{z}_{v}) \\cdot \\bm{S}[u, v].$$The similarity matrix is defined as the normalised Laplacian matrix. To create this normalised Laplacian matrix, first you need to create an $\\epsilon$-neighbourhood for each node or find the nodes k nearest neighbours. With the neighbourhood found, next the Laplacian matrix is divided by weights signifying the distance between the nodes. There are two common weighting strategies used to find our weight matrix $W$. First is by using a heat kernel. Second simply assigns the weight of 1 if the nodes are connected, otherwise, the weight is assigned the value 0. Each have their own set of advantages and disadvantages. More details of this process can be found here.\nUsing this weighting matrix, we can construct the Laplacian $L$:\n$$L = D - W,$$where D is a diagonal degree matrix with $D_{ii} = \\sum_{j} w_{ij}$.\nNow that we have constructed our normalised Laplacian. The next step is to solve the generalised eigenvalue problem.\nThe optimal solution to minimise the loss is given by the eigenvectors with d smallest non-zero eigenvalues.\nInner Product Methods The second group of methods are called the inner product methods. These use the assumption that the similarity between two nodes is proportional to their dot product.\n$$\\text{DEC}(\\bm{z}_{u}, \\bm{z}_{v}) = \\bm{z}_{u}^{T} \\bm{z}_{v}.$$Methods such as Graph Factorisation (GF), GraRep, and HOPE used this decoder. This type of decoder is often applied to minimise the mean squared error loss between the decoder and the chosen similarity matrix.\n$$\\mathcal{L} = \\sum_{(u, v) \\in \\mathcal{D}} \\| \\text{DEC}(\\bm{z}_{u}, \\bm{z}_{v}) - \\bm{S}[u, v] \\|_{2}^{2}.$$Some methods defined the adjacency matrix as the similarity matrix (GF) while other variants used powers of the adjacency matrix as the similarity matrix. In practice, there is no default strategy to go about choosing the similarity matrix and many experts have used different similarity matrices with goals of optimising different problems. We\u0026rsquo;ll delve deeper into some of these methods in the next sections.\nGraph Factorisation\nAs mention in the last paragraph, the GF factorisation approach defines its similarity matrix as the adjacency matrix. To approximate this adjacency matrix, GF used two matrices $U, V \\in \\mathbb{R}^{n \\times d}$, where each row of $U$ and $V$ corresponds to a low-dimensional embedding of a node in the graph. The goal of the factorisation is to minimise the difference between the product $U \\times V^{T}$. This gives the following loss function:\n$$\\min_{U,V} f(U,V) = \\min_{U,V} \\frac{1}{2}\\sum_{(i,j) \\in E} (A_{ij} - U_{i}V^{T}_{j})^{2} + \\lambda (\\| U_{i}\\|^{2} + \\| V_{j}\\|^{2}).$$The $\\lambda$ term acts as a regularisation to prevent overfitting. This optimisation problem is solved using stochastic gradient descent (SGD). SGD is an optimisation process that involves iterating over the edges in the graph and updating the embedding vectors $U_{i}$ and $V_{j}$ to minimize the error between the actual adjacency value $A_{ij}$ and our reconstruction. It requires differentiating wrt to the edge from node i to j to give:\n$$\\frac{\\delta f}{\\delta U_{i, j}} = - (A_{i,j} - U_{i}V^{T}_{j})V_{j} - \\lambda U_{i}.$$Using the SGD formula, this gradient is inserted below:\n$$U_{i} \\leftarrow U_{i} - \\mu \\frac{\\delta f}{\\delta U_{i}},$$where $\\mu$ denotes the step size. This process is iterated till the Frobius norm between the matrix produced in each iteration is less than the stopping criterion.\nThe initial reason for two different matrices $U$ and $V$ was to help the matrix generalise to a wide range of different graphs (directed, undirected, weighted, etc.). Note that in the scenario the graph is undirected the adjacency matrix is symmetric so $U\\approx V$.\nMore details can be found by Ahmed and his team here.\nGraRep\nUnlike GF, GraRep uses a different similarity matrix. GraRep starts by using a row normalised variant of the adjacent matrix. To create this matrix, it defines the probability transition matrix $P$ as $P = D^{-1}A$, where A is the adjacency matrix and D is the degree matrix. A degree matrix is a diagonal matrix where the ith row\u0026rsquo;s non-zero diagonal element corresponds to the degree of the ith node. This matrix $P$ is not our similarity matrix however it\u0026rsquo;s used in the calculation of our similarity matrix.\nNext we use the concatenation of k adjacency matrices, $P^{k} = P....P$. We\u0026rsquo;ll denote $P^{k}_{w, c}$ to correspond to the probability of transitioning from vertex $w$ to vertex $c$ in the kth step. In other literature this transition probability is written as $p_{k}(c|w) = P^{k} _{w,c}$.\nThis transition matrix is used to create our objective function. This gives:\n$$L_{k}(w,c) = P^{k}_{w,c} \\cdot \\log \\sigma(w\\cdot c) + \\frac{\\lambda}{N} \\sum_{w'} P^{k}_{w', c} \\cdot \\log \\sigma(-w' \\cdot c).$$The derivation and reasonings behind this loss function will not be covered today but it can be found here. To minimise this loss we differentiate this loss function wrt $w\\cdot c$ and equate to zero. This tells us that the best parameters for $w\\cdot c$ are:\n$$w\\cdot c = \\log(\\frac{P^{k}_{w,c}}{\\sum_{w'}P ^{k}_{w',c}}) - \\log(\\beta),$$where $\\beta = \\lambda / N$. Consequently, if we were to find a lower dimensional representation for our similarity matrix $S$ of dimension d (denoted as $S_{d}$), we would need to find a factorisation $W$ and $C$ of $S$ where\n$$S_{i,j}^{k} = W^{k}_{i} \\cdot C _{j}^{k} = \\log \\left( \\frac{P_{i,j}^{k}}{\\sum_{t} P_{t,j}^{k}} \\right) - \\log (\\beta).$$To further improve stability of this method, the negative entries within matrix $S$ are assigned the value 0.\nNow that we have our similarity matrix, we can next find our low dimensional representation using SVD.\n$$S^{k} = U^{k} \\textstyle \\sum^{k}_{d} V^{k}.$$Using the d largest singular values of $S$, we can approximate $S^{k} \\approx S^{k}_{d} = U^{k}_{d} \\sum^{k}_{d} V^{k} _{d}$ where $\\sum^{k}_{d}$ corresponds to the first diagonal matrix composed of the top d singular values and $U^{k}_{d}$ and $V^{k} _{d}$ correspond to the first d eigenvectors of $S S^{T}$.\nConstructing $S$ as a function of $W$ and $C$ will minimise our loss function above.\n$$S^{k} \\approx S^{k} = W^{k}_{d}C^{k}_{d}$$$$W^{k} = U^{k}_{d}( \\textstyle \\sum_{d}^k) ^{\\frac{1}{2}}, \\quad C^{k} = ( \\textstyle \\sum _{d}^{k}) ^{\\frac{1}{2}}V^{k^{T}}_{d}$$This matrix $W^{k}$ gives our d-dimensional representations of the data which captures the k-step global structural information.\nThe authors mentioned alternative approaches to SVD could be used for the dimensionality reduction but had yet to be investigated. These include incremental SVD, independent component analysis, and deep neural networks.\nLimitations of Shallow Embedding Methods Before concluding we\u0026rsquo;ll briefly mention some of the important limitations of shallow embeddings methods.\nFirst, the parameters between the nodes are not shared. Parameter sharing can greatly improve efficiency and can act as a form of regularisation. Shallow embeddings do not leverage node features in the encoder process. Shallow embeddings are transductive. This means that methods are only able to generate embeddings for nodes that they have trained on. Conclusion Today\u0026rsquo;s article covered some of the methods used within the graph factorisation for shallow embedding methods. We looked at LLEs, Laplacian Eigenmaps, GF, and GraRep and briefly covered the maths behind their formulation.\nAppendix: A) Laplacian Matrices: A Laplacian matrix is simply a matrix representation of a graph. They have many different definitions and properties depending on the graph. For simple graphs the Laplacian matrix $L$ is also defined as $L = D - A$, where D is the degree matrix and A is the adjacency matrix of the graph. For Laplacian matrix $L$ with eigenvalues $\\lambda_{0} \\leq \\lambda_{1} \\ldots \\leq \\lambda_{n-1}$. Some of the properties of Laplacian matrices $L$ are:\nL is symmetric. L is positive semi-definite ($\\lambda_{i} \\geq 0 \\forall i). The second smallest eigenvalue is referred to as the algebraic connectivity. It\u0026rsquo;s used to identify clusters in a graph. In general the smaller eigenvalues capture the large-scale global structure. Row and columns sums of L are 0. For any Laplacian matrix, there will always exist one eigenvalue with the value 0. The number of zero eigenvalues for L is the same as the number of connected components of the graph. B) The Standard Eigenvalue Problem: Given a square matrix $A$, the goal is to find non-zero vectors $x$ and scalars $\\lambda$ such that\n$$Ax = \\lambda x$$Vectors which satisfy this problem are called eigenvectors and the scalars are called $eigenvalues$.\nThe Generalised Eigenvalue Problem: Given square matrices $A$ and $B$, the goal is to find non-zero vectors $x$ and scalars $\\lambda$ that satisfy:\n$$Ax = \\lambda B x$$These type of problems appear when the properties of the matrices $A$ and $B$ are required for solving another engineering or mathematical problem.\nReferences: William, W.L. (2022). Graph Representation Learning. Springer Nature.\nKumar, S. (2023). Shallow Embedding Models for Homogeneous Graphs. [online] Reachsumit.com. Available at: https://blog.reachsumit.com/posts/2023/05/shallow-homogeneous-graphs-rep/ [Accessed 20 Aug. 2024].\nRoweis, S.T. (2000). Nonlinear Dimensionality Reduction by Locally Linear Embedding. Science, 290(5500), pp.2323–2326. doi:https://doi.org/10.1126/science.290.5500.2323.\n‌Belkin, M. and Niyogi, P. (2003). Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. Neural Computation, [online] 15(6), pp.1373–1396. doi:https://doi.org/10.1162/089976603321780317.\nGoogle, A., Shervashidze, N., Bangalore, M., Google, V. and Smola, A. (n.d.). Distributed Large-scale Natural Graph Factorization * Shravan Narayanamurthy. [online] Available at: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40839.pdf [Accessed 20 Aug. 2024].\nCao, S., Lu, W. and Xu, Q. (2015). GraRep. Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. doi:https://doi.org/10.1145/2806416.2806512.\n‌\n‌\n‌\n‌\n","permalink":"https://www.mathstoml.com/graph-factorisation-methods-in-shallow-graphs/","tags":["GNN"],"title":"Graph Factorisation Methods in Shallow Graphs"},{"categories":["Graphs"],"contents":" Image by Justin Simmonds\nGenerative Adversarial Networks (GANs) were first introduced in 2014 by Ian Goodfellow in his paper “Generative Adversarial Nets.” This paper presented the GAN framework, which consists of two neural networks called the generator and the discriminator. The generator takes random noise as input and outputs a generated image. The discriminator takes both a generated image and a real image as inputs and tries to determine which is real and which is generated. Their training process can be likened to a ping-pong game, with the generator trying to produce images that fool the discriminator, and the discriminator is trying to identify which images are generated.\nPrior to 2017, normalisation methods within GANs still used Batch Normalisation. This normalisation scheme was initially introduced to help train Convolutional Neural Networks (CNNs) by normalising each of the channels in each batch. In batch normalisation, channel-wise normalisation is performed using the formulas below. This works well in CNNs by ensuring a consistent channel mean and variance, resulting in a more stable training process and improved gradient flow.\nHere, c denotes the channel (also known as the feature map), m is the batch size, and H and W represent the height and width of the image.\nBatch Normalisation in CNNs has been proven to be effective but in GANs it has introduced limitations. An ideal GAN should produce a wide range of outputs. However, when Batch Normalisation is applied, it often results in samples with a similar distribution across the batch, reducing the uniqueness of each sample. This phenomenon is known as mode collapse.\nMode collapse occurs when the generator struggles to produce a diverse range of images. Consequently, the generator learns to produce a limited set of images that can fool the discriminator, rather than focusing on generating a diverse array of images.\nAnother issue with Batch Normalisation is that it creates small dependencies between the sample channels, as each sample impacts the scaling applied to the channel. This results in inconsistent normalisation, which is not optimal for any individual image in the batch.\nTo address these issues, a new type of normalisation is needed. This new normalisation is called Instance Normalisation (also known as Contrast Normalisation). Instance Normalisation was introduced by Dmitry Ulyanov in his paper “Instance Normalization: The Missing Ingredient for Fast Stylization” as a way to improve the performance of artistic style transfer.\nThe goal in artistic style transfer is to recreate the style of one image within the context of another. For example, the generator would take the art style from a painting and apply it to a portrait. This produces a portrait with the given art style. The networks that solve these problems are called Style Transfer Networks. In these networks, the painting is referred to as the style image, and the portrait is the content image.\nTo avoid the effect of samples impacting each other’s output images, Instance Normalisation normalises each channel individually using the following formula:\nHere i denotes the ith channel, and H and W denote the image height and width, respectively.\nThis normalisation is crucial in Style Transfer Networks which feed both the content image and style image into the same network. Without Instance Normalisation the normalisation would be inconsistent as the content image would impact the normalisation of the style images and vice-versa. Dmitry mentioned this in his paper as he found that the larger the batch size was used, there would be a larger drop in performance of the network with Batch Normalisation.\nThe image below should help understand the difference between the normalisation schemes.\nSyncedReview (2018) Facebook Ai proposes group normalization alternative to batch normalization, Medium. Available at: https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7 (Accessed: 22 June 2024).\nThis article covered the introduction of Instance Normalisation in Neural Networks and its preference over batch normalisation in the domain of GANs.\nReferences\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., \u0026amp; Bengio, Y. (2014). Generative Adversarial Nets. https://arxiv.org/pdf/1406.2661\nUlyanov, D., Vedaldi, A., \u0026amp; Lempitsky, V. (n.d.). Instance Normalization: The Missing Ingredient for Fast Stylization. https://arxiv.org/pdf/1607.08022\n‌\n","permalink":"https://www.mathstoml.com/instance-normalisation-within-gans/","tags":["GANs"],"title":"Instance Normalisation within GANs"},{"categories":["Machine Learning","Auto-Encoders"],"contents":" Image by Dan Gold\nMachine Learning (ML) has numerous applications in medicine, including disease diagnosis, drug development, predictive healthcare, and more. One key application of ML in medicine is biomedical image processing. These types of models takes an image as an input and then assigns a class label to each pixel in a process called localisation. Competitions are held annually to advance these ML models for biomedical image processing tasks. For instance, the International Symposium on Biomedical Imaging (ISBI) hosts yearly competitions focused on various biomedical imaging challenges. One notable problems from the ISBI involves segmenting neuronal structures in electron microscopy stacks.\nIn 2012, the winner of this competition employed a sliding-window approach. This method involved using a window to assign each pixel a class based on the information within a patch of the image. The approach had two main advantages. First, it enabled the model to localize effectively. Second, it required less training data because multiple patches could be generated from each image.\nHowever, there were two major disadvantages. Firstly, the network was slow because each image patch had to be processed individually. Secondly, there was a tradeoff between localization accuracy and the use of context. Using a larger patch size provided more context but to manage the increased image size, max-pooling had to be applied for efficiency, which reduced localization accuracy. Conversely, using a smaller patch size provided less context, which also diminished accuracy.\nU-Net provides an alternative approach with a distinctive architecture consisting of two paths: the contracting path and the expansive path, often referred to as the encoder path and decoder path in later papers. The contracting path extracts features from the original image using convolution layers followed by pooling to downsample, effectively reducing the spatial dimensions and capturing the context.\nThe expansive path, or decoder path, focuses on reconstructing the image segmentation from the abstract features obtained in the contracting path. This is achieved through the use of upsampling, which progressively restore the spatial dimensions. This upsampling process allows the decoder to recover fine details and achieve precise localization, resulting in accurate segmentation.\nFigure 1: U-Net Architecture. Source: Ronneberger, O., Fischer, P. and Brox, T. (n.d.). U-Net: Convolutional Networks for Biomedical Image Segmentation. [online] Available at: https://arxiv.org/pdf/1505.04597\nIn the pooling process, the encoder loses a significant amount of fine-grained information. To help the decoder achieve precise localization, skip connections are added between the encoder and decoder layers. These skip connections concatenate the encoder’s layers with the corresponding decoder layers, helping the decoder preserve finer details. Skip connections provide direct pathways to previous layers in the network, which helps maintain the gradient of the parameters and prevents them from vanishing.\nThe U-Net architecture is illustrated in Figure 1. The contracting path is on the left, and the expansive path is on the right. The contracting path closely resembles convolutional networks, with repeated 3x3 convolutions followed by rectified linear units (ReLU). Downsampling is achieved using a 2x2 max pooling layer with a stride of 2, and the number of filters doubles at each downsampling step.\nThe expansive path performs the opposite operations. It uses 2x2 up-convolutions to upsample the image, reducing the number of feature maps accordingly. Following the upsampling, 3x3 convolutions are applied, each followed by a ReLU. Finally, 1x1 convolutions are used to map the final feature maps to the desired number of output classes.\nThe input to the U-Net is a patch of the original image. To provide context for each patch, an overlap-tile strategy is used (figure 2). In this strategy, the centre of the patch is the area of interest for prediction, while the surrounding region provides context for the central section. Every patch inputs into the model overlaps with adjacent patches, ensuring continuous context and accurate predictions.\nFigure 2: Overlapping boundary boxes. Source: Ronneberger, O., Fischer, P. and Brox, T. (n.d.). U-Net: Convolutional Networks for Biomedical Image Segmentation. [online] Available at: https://arxiv.org/pdf/1505.04597\nIn the case the middle section (the area bounded by the yellow box) is on the boundary of the original image a mirroring operation is performed to create context around the boundary.\nAnother challenge the U-net authors encountered was the separation of touching objects of the same class. A weighted loss was introduced to address this issue. This weighted loss adds more weight to pixels to the boundary of two different classes. The weight function is written as\nThis weight function is applied to the energy function. The energy function is a combination of the pixel-wise softmax function and the cross entropy loss function.\nThe softmax function takes the form below: The energy function takes the form below:\nSince its introduction, the U-Net architecture has set new benchmarks in biomedical segmentation tasks, notably excelling in the ISBI challenge for segmenting neuronal structures in electron microscopy stacks. U-Net continues to demonstrate high performance, and its influence is evident in many contemporary machine learning models for biomedical image segmentation. Modern architectures often draw heavily from U-Net’s design principles, showcasing its lasting impact and relevance in the field.\nReferences:\nFor further details on the training and the experimental results of the U-Net model I’d highly encourage reading the paper by Olaf Ronneberger about U-Net.\nRonneberger, O., Fischer, P. and Brox, T. (n.d.). U-Net: Convolutional Networks for Biomedical Image Segmentation. [online] Available at: https://arxiv.org/pdf/1505.04597.\n‌\n","permalink":"https://www.mathstoml.com/u-net/","tags":["Auto-Encoders"],"title":"U-Net"},{"categories":["Machine Learning","Trees"],"contents":" Image by Todd Quackenbush\nFor the past 30 years, tree-based algorithms such as Adaboost and Random Forests have been the go-to methods for solving tabular data problems. While neural networks (NNs) have been used in this context, they have historically struggled to match the performance of tree-based methods. Despite recent advancements in NN capabilities and their success in tasks from computer vision, language translation, and image generation, tree-based algorithms still outperform neural networks when it comes to tabular data. This article will introduce several reasons behind the continued dominance of tree-based methods in this domain.\nSmoothness of Neural Networks: Tabular data often has high dimensionality due to the large number of features each of different types. From these features, many of them have multidimensional relationships which can be complicated to model.\nOne issue with NNs is that they are inherently too smooth for most tabular data target functions (see definition at the end of the paper), which tend to be irregular in order to capture the complex tabular feature relations and structure. NNs are biased towards low-frequency functions, whereas tree-based algorithms, which are based on decision trees, use piecewise continuous functions. This gives trees an advantage in handling the irregularities common in tabular data and helps them model these complexities.\n2. Handling Uninformative Features:\nTree-based algorithms, being greedy, can effectively disregard uninformative features during the split process, using a splitting criteria such as entropy or the Gini index. In contrast, the neural networks initially assign weights to all features (including uninformative ones) by either giving each feature the same weight or assigning a random weight to the features. Even when there are small weights applied to these features, they can negatively impact the model’s performance. Consequently, it takes the NN model longer to learn which features are uninformative and then assign them a small weight which won\u0026rsquo;t impact the model\u0026rsquo;s performance.\n3. Rotational Invariance of Neural Networks:\nIn the paper by Grinsztajn on why trees outperform NNs on tabular data, Grinsztajn stated that Neural networks, particularly MultiLayer Perceptrons (MLPs), are naturally \u0026ldquo;rotationally invariant\u0026rdquo; and do not preserve the natural orientation of the data.\nIn this context, the MLP\u0026rsquo;s rotational invariance corresponds to MLPs being unable to capture the natural orientation and relationships of the features. This must not be confused with the rotational invariance found in Geometric Neural Networks (GNNs) where if the features are rotated by applying a rotation matrix, the model\u0026rsquo;s predictions will remain the same.\nSimilarly in this context, the natural orientation of the feature refers to the inherent structure and relationships between the features. The structure refers to the meaning of the feature, data types, data ranges, and feature distribution. The relationships refer to the interactions between features, correlations dependencies, and the hierarchical or sequential relationships.\nThe NNs initially treats each feature to be independent of one another. Additionally, the NNs do not have built-in mechanisms to preserve the natural relationships between features. For the NN to learn these structures and relationships between the features, more training and data is needed and a change in architecture may be required.\nOn the contrary, tree-based models and their ensembles inherently preserve natural orientation of features. Trees follow a greedy procedure which splits the data on the specific feature which achieves the highest splitting criteria. Each of these splits conserves the original meaning of the feature and its interactions. Consequently, these splits maintain the structure and relationships present in the data. For neural networks, these structures and relationships need to be learnt. This gives the tree-based models an inductive bias.\nConclusion:\nTree-based algorithms are easier and quicker to train compared to neural networks, making them a more practical choice for most tabular data problems. However, this does not mean that neural networks are entirely unsuitable for tabular data; with the right hyperparameters, NNs can potentially outperform tree-based models in specific scenarios.\nReference\nThis summary is based on the following paper, which provides a deeper understanding of the content covered here:\nGrinsztajn, L., Oyallon, E. and Varoquaux, G. (n.d.). Why do tree-based models still outperform deep learning on tabular data? [online] Available at: https://arxiv.org/pdf/2207.08815\nDefinitions:\n• Target Function: The underlying function that maps inputs to outputs, representing the true relationship between features and labels. The goal in practice is to approximate this function as accurately as possible.\n","permalink":"https://www.mathstoml.com/why-do-trees-outperform-neural-networks-on-tabular-data/","tags":["Machine Learning","Trees"],"title":"Why Do Trees Outperform Neural Networks on Tabular Data?"},{"categories":null,"contents":"About Me Hi, I’m Ben, a Foundation ML Engineer based in London, currently working at the startup Pharmovo where we apply AI solutions to forecast the demand of pharmacetutical drugs in the US and UK.\nPrior to this role, I worked as a data scientist at the start up Eligible where we applied data driven approaches to predict the actions of mortgage users. My final work experience was at NatWest Markets where I applied ML methods to determine whether bonds were over or under priced within a portfolio.\nI created this platform to share my research and insights in these areas and to connect with others who share similar interests.\n","permalink":"https://www.mathstoml.com/about_me/","tags":null,"title":"About Me"}]