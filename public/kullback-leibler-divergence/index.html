<!DOCTYPE html>
<html lang="en-uk"
    dir="ltr"><head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, maximum-scale=1">
    <title>
    
    Kullback-Leibler Divergence - MathsToML
    
</title>
    
    
    
    
    
    
    
    
    
    
    <meta name="keywords" content="Kullback-Leibler Divergence , KL Divergence">
    <meta name="description" content="SEO Description Here">
    <link rel="canonical" href="https://www.mathstoml.com/kullback-leibler-divergence/" />
    <link rel="icon" href="/favicon.ico?v=1735249243" type="image/x-icon">
    
<link rel="stylesheet" href="/css/app.min.b80070c428cb0fdb565420914fa58a8c0a534bf1f6aca65496a3fbdd04269510.css" integrity="sha256-uABwxCjLD9tWVCCRT6WKjApTS/H2rKZUlqP73QQmlRA=" crossorigin="anonymous">
    
<script src="/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js" integrity="sha256-I80MfYNyY7nq65buLZzPopadqj&#43;gD6HB/ocBqbhyUaE=" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.13.10/dist/cdn.min.js"></script>

    
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "",
            enable_page_level_ads: true
        });
    </script>
    

    
    
    <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', '');
    </script>
    

    
    
</head><head>
    
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['$', '$']]                  
    }
  };
</script>
    
</head>

<body>
    <div class="
        mx-auto max-w-[calc(120rem)]
        min-h-screen
        2xl:px-[calc(16rem)]
        xl:px-24
        md:px-8
        px-4
    ">

        <div x-data="{ openMenu: false }" class="relative">
    <nav class="flex flex-1 flex-col lg:flex-row items-center justify-between">
        <a href="/">
            <img src="/image/logo.webp" alt="site logo"
                class="w-16 h-16 my-5 p-1 bg-gray-100 rounded-full cursor-pointer hover:scale-110" />
        </a>
        <div class="hidden lg:block" :class="{'hidden': !openMenu}">
            






<ul
    class="flex flex-col lg:flex-row justify-end mt-2 sm:mt-5 mb-5 pb-2 font-light text-xl lg:text-2xl gap-5 lg:gap-1 text-center">
    








<li>
    <a    class="px-5 lg:px-10 py-3 font-light hover:border-b-2 hover:border-red-500"     href="/"  >Home</a>
    
</li>







<li>
    <a    class="px-5 lg:px-10 py-3 font-light hover:border-b-2 hover:border-red-500"     href="/categories/"  >Categories</a>
    
</li>







<li>
    <a    class="px-5 lg:px-10 py-3 font-light hover:border-b-2 hover:border-red-500"     href="/tags/"  >Tags</a>
    
</li>







<li>
    <a    class="px-5 lg:px-10 py-3 font-light hover:border-b-2 hover:border-red-500"     href="/about_me/"  >About Me</a>
    
</li>


</ul>




        </div>
    </nav>
    <div class="absolute top-8 right-5 flex items-center lg:hidden">
        
        <button @click="openMenu = !openMenu" type="button"
            class="relative inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:bg-gray-800 hover:text-gray-100 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-white"
            aria-controls="mobile-menu" aria-expanded="openMenu">
            
            <svg x-show="!openMenu" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5"
                stroke="currentColor" aria-hidden="true">
                <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" />
            </svg>
            
            <svg x-show="openMenu" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5"
                stroke="currentColor" aria-hidden="true">
                <path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12" />
            </svg>
        </button>
    </div>
</div>

        <header class="flex flex-col w-full items-center justify-center text-white pt-8 pb-8">
    <div class="w-full">
        <div class="flex flex-1 flex-row justify-between">
            <h2 class="w-full text-center text-3xl sm:text-5xl font-crimson font-bold tracking-tight text-gray-300">
                <a href="https://www.mathstoml.com/">MathsToML</a>
            </h2>
        </div>
        <p
            class="w-full text-center pl-1 pb-4 sm:pt-3 sm:pb-0 font-crimson font-normal text-xl sm:text-2xl leading-8 text-gray-500">
            My Blog</p>
    </div>

    <div class="relative w-9/12 lg:w-1/2 h-12 my-5">
        <form action="/en/search" method="get">
            <input
                class="bg-gray-800 placeholder:italic placeholder:text-gray-600 w-full h-12 rounded-full mt-1 pl-5 pr-5 border border-gray-800 text-gray-100"
                placeholder='Input Keywords...' type="text" name="q" id="search-query" />

            <button
                class="absolute inset-y-2 right-1 w-28 h-10 font-light bg-gray-900 hover:bg-red-500 text-gray-500 hover:text-gray-100 rounded-full cursor-pointer"
                type="submit">Search</button>
        </form>
    </div>

    
    <div class="w-full flex flex-row justify-start text-gray-500 text-lg px-1 border-l-4 border-l-red-500">
        <ul class="flex flex-row gap-x-2">
            <li class="">
                <a href="/" class="hover:text-gray-100">Home</a>
            </li>
            
            <li>
                &gt;&nbsp;&nbsp;<a href="/kullback-leibler-divergence/" class="hover:text-gray-100">Kullback-Leibler Divergence</a>
            </li>
            
        </ul>
    </div>
    <div class="w-full h-2 border-b border-b-gray-600/50 border-dashed font-light text-gray-300">
    </div>
    

</header>

        <div class="
            flex flex-col overflow-hidden
            xl:px-0
            lg:flex-row lg:space-x-8
        ">
            <main class="w-full overflow-hidden">
                

<article class="single-article">
    
    <div class="group relative">
        <h1 class="text-3xl font-medium leading-10 text-gray-400 hover:text-gray-100">
            <a href="/kullback-leibler-divergence/">
                Kullback-Leibler Divergence
            </a>
        </h1>
        <time datetime="2025-03-16" class="flex items-center py-2 text-xl text-gray-600">
            2024-09-10 00:00
            &nbsp;&nbsp;|&nbsp;&nbsp;7 minute read
        </time>

        <div
            class="mt-1 lg:pb-10 px-2 text-2xl leading-10 font-thin text-gray-500 overflow-hidden break-words article-body">
            <p><img src="/images/daniela-turcanu-wABdckxoj90-unsplash.jpg" alt="Image">
<a href="https://unsplash.com/@protopopica?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Image by Daniela Turcanu</a></p>
<hr>
<p><strong>Introduction</strong></p>
<p>This article will cover the key features of Kullback-Leibler Divergence (KL divergence), a formula invented in 1951 by the mathematicians Soloman Kullback and Richard Leibler. This formula is used in the background of many of the modern day machine learning models focused around probabilistic modelling. These including Variational Autoencoders (VAEs), Generative Models, Reinforcement Learning, and Natural Language Processing. Additionally, this article will cover some of KL divergence&rsquo;s key properties and briefly cover one of its applications.</p>
<p><strong>KL Divergence</strong></p>
<p>In essence, KL divergence gives us a way to tell how much one probability distribution varies from another. KL divergence quantifies this result as the amount of information lost when approximating one distribution from another.</p>
<p>In the discrete setting, for probability distributions $P$ and $Q$ defined over sample space $\mathcal{X}$, the formula takes the form:</p>
$$D_{\text{KL}}(P || Q) = \sum_{x\in\mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}.$$<p>This formula quantifies how close distribution $Q$ is to the approximating distribution $P$. More specifically, it measures the divergence of the distribution $Q$ from distribution $P$. This is interpreted as the amount of extra information required to encode samples from $P$ using distribution $Q$.</p>
<p>KL divergence for continuous probability distributions $P$ and $Q$ follow:</p>
$$D_{\text{KL}}(P || Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx,$$<p>where $p$ and $q$ refer to the probability densities of distribution $P$ and $Q$ respectively.</p>
<p>Before continuing, it is important to mention that in other texts, KL divergence is often referred to relative entropy. This must not be confused with Shannon entropy which measures the uncertainty in just one probability distribution.</p>
<p><strong>Key Properties of KL Divergence</strong></p>
<p><strong>Asymmetry</strong></p>
<p>The first key property of KL divergence is that this metric is asymmetrical,</p>
$$D_{\text{KL}}(P || Q) \neq D_{\text{KL}}(Q || P).$$<p>This indicates that the divergence of distribution $P$ from $Q$ often gives a different result to the divergence of distribution $Q$ from $P$.</p>
<p>The lack of symmetry, along with the fact that the triangle inequality is not satisfied, are key reasons why this measure is not referred to as a distance. Instead, in information theory, it is classified as a type of divergence.</p>
<p>An alternative symmetric measure to quantify the difference between two probability distributions is Jeffery&rsquo;s Divergence. This divergence is defined as the sum of divergences between distributions $P$ and $Q$ in both directions.</p>
$$J(P, Q) = \frac{1}{2} D_{\text{KL}}(P \parallel Q) + \frac{1}{2} D_{\text{KL}}(Q \parallel P)$$<p>Jeffery&rsquo;s measure provides a way to measure the total difference between two distributions.</p>
<p>On initial inspection, Jeffery&rsquo;s may seem more appropriate than KL divergence, however there&rsquo;s a few reasons why Kl divergence may be preferred. First KL divergence has a higher sensitivity to differences in distributions while Jefferys is a more smoothed version of KL divergence, which is less sensitive to these finer differences. The finer tuning ability is often preferred in practice.</p>
<p>Secondly, in situations where one distribution is a &ldquo;target&rdquo; or &ldquo;true&rdquo; distribution and the second is one is a distribution trying to approximate the first. KL divergence encapsulates this problem well, while Jeffery&rsquo;s does not. The reasons for this is because Jeffery&rsquo;s will incorporate information about the divergence of the true distribution from the approximating distribution. This is unwanted information, which hinders are inferences.</p>
<p><strong>Non-Negativity</strong></p>
<p>The second important property of KL divergence is that it is non-negative and equals zero iff (if and only if) the two distributions are identical almost everywhere. In this case, there is no divergence between the two distributions. Mathematically, this non-negativity property is written as $D_{\text{KL}}(P | Q) \geq 0$. This result is known as Gibbs’ inequality.</p>
<p>The term “almost everywhere” refers to the distributions being identical except on a subset of the domain with measure zero. This idea is analogous to the concept of “almost surely” in probability theory. Essentially, KL divergence is zero iff the two distributions are identical everywhere except on a small subset of measure zero.</p>
<p><strong>Convexity</strong></p>
<p>KL divergence is convex for any pair of probability measures $P$ and $Q$. For example, choose $(P_{1}, P_{2})$ and $(Q_{1}, Q_{2})$ to be two pairs of probability measures. Then the following property is satisfied.</p>
$$D_{\text{KL}}(\lambda P_{1} + (1 - \lambda) P_{2} \| \lambda Q_{1} + (1-\lambda)Q_{2}) \leq \lambda D_{\text{KL}}(P_{1} \| Q_{1} ) + (1 - \lambda) D_{\text{KL}}(P_{2} \| Q_{2} ), \\ \text{ for } 0 \leq \lambda \leq 1.$$<p><strong>No upper bound exists</strong></p>
<p>Another important property of KL divergence is that, in general, it does not have an upper bound. However, an upper bound does exist in a specific case: when $P$ and $Q$ are discrete probability distributions over the same discrete set of outcomes. In this situation, a maximum value for $D_{\text{KL}}(P | Q)$ can be determined.</p>
<p><strong>Example:</strong></p>
<p>Given probability distributions $P$ and $Q$ with discrete outcomes from the sample space $\{0,1,2\}$. The distributions&rsquo;s outcome probabilities are in the table below.</p>
<table>
  <thead>
      <tr>
          <th>x</th>
          <th>0</th>
          <th>1</th>
          <th>2</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Distribution P(x)</td>
          <td>1/2</td>
          <td>1/3</td>
          <td>1/6</td>
      </tr>
      <tr>
          <td>Distribution Q(x)</td>
          <td>1/3</td>
          <td>1/3</td>
          <td>1/3</td>
      </tr>
  </tbody>
</table>
<p>The KL divergence for $Q$ from $P$ and $P$ from $Q$ is written as:</p>
$$\begin{aligned} D_{\text{KL}}(P\|Q) &= \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right) \\ &= 1/2 \log(\frac{1/2}{1/3}) + 1/3 \log(\frac{1/3}{1/3}) + 1/6 \log(\frac{1/6}{1/3}) \\ &= 0.038 \\ D_{\text{KL}}(Q\|P) &= \sum_{x \in \mathcal{X}} Q(x) \log \left( \frac{Q(x)}{P(x)} \right) \\ &= 1/3 \log(\frac{1/3}{1/2}) + 1/3 \log(\frac{1/3}{1/3}) + 1/3 \log(\frac{1/3}{1/6}) \\ &= 0.042\end{aligned} $$<p>In many real-world scenarios, the distributions $P$ and $Q$ are derived from frequency distributions. As a result, there may be instances where one distribution contains events that have not been observed in the other. To account for these unobserved events, a smoothing method is applied. This method assigns a small probability to the events missing from both distributions, ensuring that all possible outcomes are represented.</p>
<table>
  <thead>
      <tr>
          <th>x</th>
          <th>0</th>
          <th>1</th>
          <th>2</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Distribution P(x)</td>
          <td>1/2</td>
          <td>1/2</td>
          <td>0</td>
      </tr>
      <tr>
          <td>Distribution Q(x)</td>
          <td>0</td>
          <td>2/3</td>
          <td>1/3</td>
      </tr>
  </tbody>
</table>
<p>Here a small smoothing operator $\epsilon=10^{-3}$ smoothen $P$ and $Q$. This gives us the following table.</p>
<table>
  <thead>
      <tr>
          <th>x</th>
          <th>0</th>
          <th>1</th>
          <th>2</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Distribution P(x)</td>
          <td>1/2 - ε/2</td>
          <td>1/2 - ε/2</td>
          <td>ε</td>
      </tr>
      <tr>
          <td>Distribution Q(x)</td>
          <td>ε</td>
          <td>2/3 - ε/2</td>
          <td>1/3 - ε/2</td>
      </tr>
  </tbody>
</table>
<p>Then the KL divergence of $P$ from $Q$ and $Q$ from $P$ can be calculated using the same formula as above.</p>
<p><strong>Applications of KL</strong></p>
<p>One application of KL divergence is in Variational Inference (VI), a method in Bayesian Machine Learning used to approximate complex posterior distributions. The process involves selecting a simpler family of distributions, $Q$, to approximate the true but intractable posterior distribution, $P$. This approximating distribution $Q$ is referred to as the variational distribution.</p>
<p>This approximation is crucial since directly finding the true posterior distribution is often infeasible. To estimate $P$, we first express the distribution in terms of the latent variable ($Z$) and the observed data ($X$), leading to the following equation:</p>
$$ P(Z|X) = \frac{P(X|Z)P(Z)}{P(X)} = \frac{P(X|Z)P(Z)}{\int_{Z'}P(X|Z') dZ'}.$$<p>The challenge with this formula arises from the marginal term $P(X)$. Integrating over all possible values of $Z$ is often intractable, especially when $Z$ is a high-dimensional vector, as this makes the integration computationally expensive. To address this intractability, we introduce an approximating function $Q$, which serves as a simpler and more tractable version of the true posterior $P$.</p>
<p>The difference between the distributions $Q$ and $P$ is measured by a dissimilarity function, $d(Q, P)$. This function can take various forms, with KL divergence being one of the most commonly used.</p>
<p>As a result, the goal becomes minimizing the divergence between $Q$ and $P$. This optimization problem is typically framed as maximizing the Evidence Lower Bound (ELBO), which can be expressed as follows:</p>
$$\begin{aligned} \text{ELBO} &= \mathbb{E}_{Q(Z, \phi)}\left[\log(P(X,Z)) - \log(Q(Z,\phi))\right]\\ &= D_{KL}(P\|Q(Z, \phi)) - \log(P(X)) \end{aligned} $$<p>where $\phi$ are the parameters of distribution $Q$, these are the parameters which need to be optimised to make distribution $Q(Z, \phi)$ approximate the true posterior. Since the term $\log(P(X)$ is constant, maximising the ELBO is equivalent to minimising the KL divergence.</p>
<p><strong><strong>Conclusion</strong></strong></p>
<p>In conclusion, KL divergence is a useful tool used in information theory, statistics, and even machine learning. This measure of divergence is a crucial for quantifying the difference between two probability distributions. Understanding KL divergence will undoubtedly enhance your ability to interpret probabilistic models and it will serve as a good stepping stone into many key ML models and algorithms used today.</p>
<p><strong><strong>References:</strong></strong></p>
<p>Wikipedia. (2024). <strong>Kullback–Leibler divergence</strong>. [online] Available at: <a href="https://en.wikipedia.org/wiki/Kullback_Leibler_divergence">https://en.wikipedia.org/wiki/Kullback_Leibler_divergence</a> [Accessed 26 Jul. 2024].</p>
<p>Wikipedia. (2022). <strong>Variational Bayesian methods</strong>. [online] Available at: <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">https://en.wikipedia.org/wiki/Variational_Bayesian_methods</a>.</p>
<p>Shannon Entropy and Kullback-Leibler Divergence. (n.d.). Available at: <a href="https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf">https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf</a> [Accessed 23 Apr. 2019].</p>
<p>2.4.8 Kullback-Leibler Divergence. (n.d.). Available at: <a href="https://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf">https://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf</a>.</p>
<p>‌</p>
<p>‌</p>

        </div>
    </div>
    <div class="text-gray-500 text-lg">
        Page link:&nbsp;<a href="https://www.mathstoml.com/kullback-leibler-divergence/"
            class="border-b border-b-gray-500 hover:text-gray-400">https://www.mathstoml.com/kullback-leibler-divergence/</a>
    </div>
    <div class="my-10 py-5 border-t border-dashed border-t-white/10 text-xl text-gray-600">
        
    </div>
</article>


            </main>

            <aside id="sidebar" class="aside-container">

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z" />
        </svg>
        About
    </div>

    <img src="/image/logo.webp?v=1735249243" class="w-80 self-center" alt="Logo" />

    <p class="leading-8 text-center text-lg font-light mt-3">
        
    </p>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M9 12h3.75M9 15h3.75M9 18h3.75m3 .75H18a2.25 2.25 0 0 0 2.25-2.25V6.108c0-1.135-.845-2.098-1.976-2.192a48.424 48.424 0 0 0-1.123-.08m-5.801 0c-.065.21-.1.433-.1.664 0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75 2.25 2.25 0 0 0-.1-.664m-5.8 0A2.251 2.251 0 0 1 13.5 2.25H15c1.012 0 1.867.668 2.15 1.586m-5.8 0c-.376.023-.75.05-1.124.08C9.095 4.01 8.25 4.973 8.25 6.108V8.25m0 0H4.875c-.621 0-1.125.504-1.125 1.125v11.25c0 .621.504 1.125 1.125 1.125h9.75c.621 0 1.125-.504 1.125-1.125V9.375c0-.621-.504-1.125-1.125-1.125H8.25ZM6.75 12h.008v.008H6.75V12Zm0 3h.008v.008H6.75V15Zm0 3h.008v.008H6.75V18Z" />
        </svg>
        Latest Post
    </div>

    <ul class="text-lg">
        
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/xgboost/" class="py-5 hover:text-gray-300">XGBoost</a>
        </li>
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/perpetual-children/" class="py-5 hover:text-gray-300">Perpetual Children</a>
        </li>
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/some-questions-in-probability-i/" class="py-5 hover:text-gray-300">Some Questions in Probability I</a>
        </li>
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/gams/" class="py-5 hover:text-gray-300">GAMs</a>
        </li>
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/the-ant-problem/" class="py-5 hover:text-gray-300">The Ant Problem</a>
        </li>
        
        
    </ul>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M2.25 7.125C2.25 6.504 2.754 6 3.375 6h6c.621 0 1.125.504 1.125 1.125v3.75c0 .621-.504 1.125-1.125 1.125h-6a1.125 1.125 0 0 1-1.125-1.125v-3.75ZM14.25 8.625c0-.621.504-1.125 1.125-1.125h5.25c.621 0 1.125.504 1.125 1.125v8.25c0 .621-.504 1.125-1.125 1.125h-5.25a1.125 1.125 0 0 1-1.125-1.125v-8.25ZM3.75 16.125c0-.621.504-1.125 1.125-1.125h5.25c.621 0 1.125.504 1.125 1.125v2.25c0 .621-.504 1.125-1.125 1.125h-5.25a1.125 1.125 0 0 1-1.125-1.125v-2.25Z" />
        </svg>
        Hot Categories
    </div>
    <ul class="leading-10 text-lg">
        
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/graphs/" class="hover:text-gray-300">Graphs <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">4</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/probability/" class="hover:text-gray-300">Probability <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">4</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/machine-learning/" class="hover:text-gray-300">Machine Learning <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">3</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/time-series/" class="hover:text-gray-300">Time Series <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/programming-tools/" class="hover:text-gray-300">Programming Tools <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/trees/" class="hover:text-gray-300">Trees <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/non-convex-optimisation/" class="hover:text-gray-300">Non-Convex Optimisation <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/statistical-modelling/" class="hover:text-gray-300">Statistical Modelling <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/auto-encoders/" class="hover:text-gray-300">Auto-Encoders <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/diffusion-models/" class="hover:text-gray-300">Diffusion Models <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        
        
        
        

    </ul>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z" />
            <path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z" />
        </svg>
        Top Tags
    </div>
    <div class="flex flex-wrap gap-2 text-lg leading-8 pt-3 pl-1">
        
        
        
        
        <a href="/tags/probability/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Probability&nbsp;&nbsp;4</span></a>
        
        
        
        <a href="/tags/machine-learning/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Machine Learning&nbsp;&nbsp;3</span></a>
        
        
        
        <a href="/tags/statistics/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Statistics&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/auto-encoders/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Auto-Encoders&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/gnn/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GNN&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/trees/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Trees&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/time-series/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Time Series&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/docker/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Docker&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/gams/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GAMs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/gnns/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GNNs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/arma/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">ARMA&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/diffusion-models/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Diffusion Models&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/boosting/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Boosting&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/dynamic-programming/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Dynamic Programming&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/glms/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GLMs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/optimisation/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Optimisation&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/arima/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">ARIMA&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/gans/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GANs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/rest-apis/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">REST APIs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/algorithms/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Algorithms&nbsp;&nbsp;1</span></a>
        
        
        
        
    </div>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="m20.25 7.5-.625 10.632a2.25 2.25 0 0 1-2.247 2.118H6.622a2.25 2.25 0 0 1-2.247-2.118L3.75 7.5M10 11.25h4M3.375 7.5h17.25c.621 0 1.125-.504 1.125-1.125v-1.5c0-.621-.504-1.125-1.125-1.125H3.375c-.621 0-1.125.504-1.125 1.125v1.5c0 .621.504 1.125 1.125 1.125Z" />
        </svg>
        Archive Analytics
    </div>
    <ul class="leading-10 text-lg font-light">
        
        
        
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Sep, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Oct, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">3</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Nov, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">4</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Jun, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Jul, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">4</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Jan, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Dec, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">4</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Aug, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
    </ul>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75" />
        </svg>
        Contact
    </div>
    <div class="flex flex-row gap-2">
        Email：benbradshaw01@outlook.com
    </div>
</aside>
        </div>

        <footer class="p-5 text-xl text-center mt-8 pt-8 pb-8 border-t border-gray-100/10">
    <div class="text-gray-500">
        
        &#xA9; 2019 - 2023 by <a class="hover:text-gray-100" href="https://github.com/guangmean/Niello"
            target="_blank">guangmean</a>. All Rights
        Reserved.
        

        
        | <a class="hover:text-gray-100" href=" /en ">🇬🇧EN</a>
        
    </div>
</footer>

        <div class="cookie-container text-center py-12 text-2xl font-thin text-gray-500">
  <p>
    We use cookies on this website to give you the best experience on our
    site and show you relevant ads. To find out more, read our
    <a href="/privacy/" class="text-red-600">privacy policy</a> and <a href="/cookies/" class="text-red-600">cookie
      policy</a>.
  </p>
  <button class="cookie-btn w-32 h-22 mt-5 py-2 bg-red-600 text-white rounded-full hover:scale-110">
    Okay
  </button>
</div>
<script src="/js/cookie.js"></script>

    </div>
</body>

</html>