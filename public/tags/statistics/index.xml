<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on </title>
    <link>https://www.mathstoml.com/tags/statistics/</link>
    <description>Recent content in Statistics on </description>
    <generator>Hugo</generator>
    <language>en-uk</language>
    <lastBuildDate>Thu, 03 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.mathstoml.com/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Welch&#39;s T-test</title>
      <link>https://www.mathstoml.com/welchs-t-test/</link>
      <pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://www.mathstoml.com/welchs-t-test/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://www.mathstoml.com/images/turtle.jpg&#34; alt=&#34;Wexor Tmg&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@wexor&#34;&gt;Image by Wxor Tmg&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;In hypothesis testing, there are a host of methods that could be used to perform your test. Today we&amp;rsquo;ll look at Welch&amp;rsquo;s T-test. This test excels in situations where two sample means vary significantly, specifically, if the samples have unequal variances or unequal sample sizes.&lt;/p&gt;&#xA;&lt;p&gt;In general, this test should be used when:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Comparing two independent groups.&lt;/li&gt;&#xA;&lt;li&gt;Unequal variances.&lt;/li&gt;&#xA;&lt;li&gt;Unequal sample sizes.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The Welch&amp;rsquo;s &lt;em&gt;t&lt;/em&gt;-test is designed for unequal population variances, a relaxation of the Student&amp;rsquo;s t-test that assumes the two groups have equal variance. But similar to the Student&amp;rsquo;s t-test, Welch&amp;rsquo;s test shares the assumption that the data must follow a normal distribution.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is a Time Series &amp; How can we Model it?</title>
      <link>https://www.mathstoml.com/what-is-a-time-series--how-can-we-model-it/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://www.mathstoml.com/what-is-a-time-series--how-can-we-model-it/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://www.mathstoml.com/images/oguzhan-kiran-wj0l2BJKmkU-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://mathstoml.ghost.io/ghost/#/editor/post/66eb4a6473445600016630bb&#34;&gt;Image by Oğuzhan Kıran&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;This article will cover what is a time series, what does it mean for a time series to be stationary, and how could an autoregressive process or moving average process be used to model the time series.&lt;/p&gt;&#xA;&lt;p&gt;Before starting this article, I would like to mention the Cambridge notes from &lt;a href=&#34;https://www.statslab.cam.ac.uk/~rrw1/timeseries/t.pdf&#34;&gt;here&lt;/a&gt; form the foundation of this content.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-a-time-series&#34;&gt;What is a Time Series?&lt;/h2&gt;&#xA;&lt;p&gt;In short, time series is area of statistics which focuses on describing a set of data points which are collected on regular intervals by fitting low-dimensional models and making forecasts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kullback-Leibler Divergence</title>
      <link>https://www.mathstoml.com/kullback-leibler-divergence/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://www.mathstoml.com/kullback-leibler-divergence/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://www.mathstoml.com/images/daniela-turcanu-wABdckxoj90-unsplash.jpg&#34; alt=&#34;Image&#34;&gt;&#xA;&lt;a href=&#34;https://unsplash.com/@protopopica?utm_source=ghost&amp;amp;utm_medium=referral&amp;amp;utm_campaign=api-credit&#34;&gt;Image by Daniela Turcanu&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;This article will cover the key features of Kullback-Leibler Divergence (KL divergence), a formula invented in 1951 by the mathematicians Soloman Kullback and Richard Leibler. This formula is used in the background of many of the modern day machine learning models focused around probabilistic modelling. These including Variational Autoencoders (VAEs), Generative Models, Reinforcement Learning, and Natural Language Processing. Additionally, this article will cover some of KL divergence&amp;rsquo;s key properties and briefly cover one of its applications.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
