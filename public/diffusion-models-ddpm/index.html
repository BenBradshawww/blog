<!DOCTYPE html>
<html lang="en-uk"
    dir="ltr"><head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, maximum-scale=1">
    <title>
    
    Diffusion Models (DDPM) - MathsToML
    
</title>
    
    
    
    
    
    
    
    
    
    
    <meta name="keywords" content="Diffusion , Forward Process , Backward Process , Auto Encoders">
    <meta name="description" content="SEO Description Here">
    <link rel="canonical" href="https://www.mathstoml.com/diffusion-models-ddpm/" />
    <link rel="icon" href="/favicon.ico?v=1734785111" type="image/x-icon">
    
<link rel="stylesheet" href="/css/app.min.b80070c428cb0fdb565420914fa58a8c0a534bf1f6aca65496a3fbdd04269510.css" integrity="sha256-uABwxCjLD9tWVCCRT6WKjApTS/H2rKZUlqP73QQmlRA=" crossorigin="anonymous">
    
<script src="/js/main.23cd0c7d837263b9eaeb96ee2d9ccfa2969daa3fa00fa1c1fe8701a9b87251a1.js" integrity="sha256-I80MfYNyY7nq65buLZzPopadqj&#43;gD6HB/ocBqbhyUaE=" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.13.10/dist/cdn.min.js"></script>

    
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "",
            enable_page_level_ads: true
        });
    </script>
    

    
    
    <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', '');
    </script>
    

    
    
</head><head>
    
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['$', '$']]                  
    }
  };
</script>
    
</head>

<body>
    <div class="
        mx-auto max-w-[calc(120rem)]
        min-h-screen
        2xl:px-[calc(16rem)]
        xl:px-24
        md:px-8
        px-4
    ">

        <div x-data="{ openMenu: false }" class="relative">
    <nav class="flex flex-1 flex-col lg:flex-row items-center justify-between">
        <a href="/">
            <img src="/image/logo.webp" alt="site logo"
                class="w-16 h-16 my-5 p-1 bg-gray-100 rounded-full cursor-pointer hover:scale-110" />
        </a>
        <div class="hidden lg:block" :class="{'hidden': !openMenu}">
            






<ul
    class="flex flex-col lg:flex-row justify-end mt-2 sm:mt-5 mb-5 pb-2 font-light text-xl lg:text-2xl gap-5 lg:gap-1 text-center">
    








<li>
    <a    class="px-5 lg:px-10 py-3 font-light hover:border-b-2 hover:border-red-500"     href="/"  >Home</a>
    
</li>







<li>
    <a    class="px-5 lg:px-10 py-3 font-light hover:border-b-2 hover:border-red-500"     href="/categories/"  >Categories</a>
    
</li>







<li>
    <a    class="px-5 lg:px-10 py-3 font-light hover:border-b-2 hover:border-red-500"     href="/tags/"  >Tags</a>
    
</li>







<li>
    <a    class="px-5 lg:px-10 py-3 font-light hover:border-b-2 hover:border-red-500"     href="/about_me/"  >About Me</a>
    
</li>


</ul>




        </div>
    </nav>
    <div class="absolute top-8 right-5 flex items-center lg:hidden">
        
        <button @click="openMenu = !openMenu" type="button"
            class="relative inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:bg-gray-800 hover:text-gray-100 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-white"
            aria-controls="mobile-menu" aria-expanded="openMenu">
            
            <svg x-show="!openMenu" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5"
                stroke="currentColor" aria-hidden="true">
                <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" />
            </svg>
            
            <svg x-show="openMenu" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke-width="1.5"
                stroke="currentColor" aria-hidden="true">
                <path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12" />
            </svg>
        </button>
    </div>
</div>

        <header class="flex flex-col w-full items-center justify-center text-white pt-8 pb-8">
    <div class="w-full">
        <div class="flex flex-1 flex-row justify-between">
            <h2 class="w-full text-center text-3xl sm:text-5xl font-crimson font-bold tracking-tight text-gray-300">
                <a href="https://www.mathstoml.com/">MathsToML</a>
            </h2>
        </div>
        <p
            class="w-full text-center pl-1 pb-4 sm:pt-3 sm:pb-0 font-crimson font-normal text-xl sm:text-2xl leading-8 text-gray-500">
            My Blog</p>
    </div>

    <div class="relative w-9/12 lg:w-1/2 h-12 my-5">
        <form action="/en/search" method="get">
            <input
                class="bg-gray-800 placeholder:italic placeholder:text-gray-600 w-full h-12 rounded-full mt-1 pl-5 pr-5 border border-gray-800 text-gray-100"
                placeholder='Input Keywords...' type="text" name="q" id="search-query" />

            <button
                class="absolute inset-y-2 right-1 w-28 h-10 font-light bg-gray-900 hover:bg-red-500 text-gray-500 hover:text-gray-100 rounded-full cursor-pointer"
                type="submit">Search</button>
        </form>
    </div>

    
    <div class="w-full flex flex-row justify-start text-gray-500 text-lg px-1 border-l-4 border-l-red-500">
        <ul class="flex flex-row gap-x-2">
            <li class="">
                <a href="/" class="hover:text-gray-100">Home</a>
            </li>
            
            <li>
                &gt;&nbsp;&nbsp;<a href="/diffusion-models-ddpm/" class="hover:text-gray-100">Diffusion Models (DDPM)</a>
            </li>
            
        </ul>
    </div>
    <div class="w-full h-2 border-b border-b-gray-600/50 border-dashed font-light text-gray-300">
    </div>
    

</header>

        <div class="
            flex flex-col overflow-hidden
            xl:px-0
            lg:flex-row lg:space-x-8
        ">
            <main class="w-full overflow-hidden">
                

<article class="single-article">
    
    <div class="group relative">
        <h1 class="text-3xl font-medium leading-10 text-gray-400 hover:text-gray-100">
            <a href="/diffusion-models-ddpm/">
                Diffusion Models (DDPM)
            </a>
        </h1>
        <time datetime="2025-03-16" class="flex items-center py-2 text-xl text-gray-600">
            2024-08-30 00:00
            &nbsp;&nbsp;|&nbsp;&nbsp;13 minute read
        </time>

        <div
            class="mt-1 lg:pb-10 px-2 text-2xl leading-10 font-thin text-gray-500 overflow-hidden break-words article-body">
            <p><img src="images/justin-lim-tloFnD-7EpI-unsplash.jpg" alt="Image">
[Image by Justin Lim]</p>
<h3 id="introduction">Introduction</h3>
<p>This article will delve into diffusion models, a group of latent variable (see definitions) generative models with applications in image generation, audio synthesis, and denoising. More specifically, this article will mostly focus on the derivations and the ideas behind diffusion models, with a heavy enthuses on the ideas introduced in <a href="https://arxiv.org/pdf/2006.11239">Ho et al.</a> in his Denoising Diffusion Probabilisitic Models paper (DDPMs). The applications of these models will not be covered today.</p>
<h3 id="introduction-to-probabilistic-modelling"><strong><strong>Introduction to probabilistic modelling</strong></strong></h3>
<p>Before delving into diffusion models, it&rsquo;s best to briefly cover some of the common issues in probabilistic modelling. As explained by <a href="https://arxiv.org/pdf/1503.03585">J.Sohl-Dickstein et al.</a>, in probabilistic modelling, there has always been the trade-off between two conflicting objectives. These objectives are tractability and flexibility.</p>
<p>Tractable models are those that fit easily to data and that can be analytically evaluated. However, these models often lack the complexity required to model complex datasets.</p>
<p>On the contrary, flexible models have the ability to model any arbitrary data. For example, any distribution of data $\mathcal{X}$ can be represented by a scoring function $\phi(\bm{x}) \geq 0$, $\forall x \in \mathcal{X}$. To construct a probabilistic distribution from this function, we need to divide by a normalizing constant $C$, where $C = \int \phi(x) \ \delta x$, giving us $p(x) = \phi(x) / C$. The challenge with this method arises from the intractability of this normalisation term. We will explore this limitation further later in this article.</p>
<p>Diffusion models sit in the middle between these two objectives. They provide extreme flexibility while remaining tractable. To achieve this a generative Markov chain is used. This chain iteratively converts a simple known distribution (often a Gaussian distribution) to the target distribution. The model learns in each step by estimating the small perturbations in the sample rather than estimating the target function from a single non-analytically-normalisable potential function.</p>
<p>This idea stems from the fact that a diffusion process exists for any smooth target function. Consequently, a diffusion process can model distributions of arbitrary form.</p>
<h3 id="what-are-diffusion-models"><strong><strong>What are diffusion models?</strong></strong></h3>
<p>Diffusion models are broken down into 2 key processes. The forward process and the reverse process. The forward process gradually adds Gaussian noise to an image, slowly corrupting the image. On the other hand, the reverse process takes this corrupted image and iteratively tries to remove the noise from the corrupted image. In the process of removing the noise, a function (often a neural network) is trained to remove this noise. After a long training process, this new function will have the ability to generate samples from random noise.</p>
<p>Diffusion models are inspired by two key processes. The forward and backward processes in diffusion models are rooted in the concept of diffusion in physics, where particles naturally move from areas of high concentration to low concentration. This is where the term ‚Äúdiffusion‚Äù in the model‚Äôs name comes from.</p>
<p>The second, slightly more complex, inspiration comes from Sequential Monte Carlo (SMC) methods. SMC methods are simulation-based techniques used to approximate the posterior distribution, which is particularly useful for modelling dynamic systems where states evolve over time.</p>
<p>These models have been shown to be competitive and even surpass the performance GANs and VAEs architectures in many high quality image generation tasks. They&rsquo;re also the backbone architecture used by many of the modern day image generation models such as OpenAI Sora, Stable Diffusion, DALL-E 2, Imagen, Latent Diffusion Models, and many more.</p>
<h3 id="forward-process"><strong><strong>Forward Process</strong></strong></h3>
<p>The forward process starts with starting point $\bm{x}_{0} \sim q$, where $\bm{x}_{0}$ is an uncorrupted image and $q$ is the marginal probability distribution. We iteratively add Gaussian noise to the sample image over T steps producing a set of increasing corrupted samples $\{\bm{x}_{1}, \bm{x}_{2}, ... \bm{x}_{T}\}$, with each of these step sizes controlled by a variance scheduler parameter $\{ \beta_{t} \in (0,1)\}^{T}_{t=1}$ . Under this assumption, the marginal probability distribution $q$ given the corrupted image $\bm{x}_{t-1}$ can be modelled with a Gaussian distribution:</p>
$$ q(\bm{x}_{t}|\bm{x}_{t-1}) = \mathcal{N}(\bm{x}_{t}; \sqrt{1 - \beta{t}}\bm{x}_{t-1}, \beta_{t}\bm{I}).$$<p><img src="https://mathstoml.ghost.io/content/images/2024/08/Screenshot-2024-08-06-at-14.21.24-1.png" alt=""></p>
<p>Figure 1. An example forward diffusion process. Source: <a href="https://arxiv.org/pdf/2102.09672">Nichol &amp; Dhariwal</a></p>
<p>The iterative nature of progressively adding noise to the images is inspired by Markovian processes where the subsequent state of an event $\bm{x}_{t+1}$ is only dependent on the current state of the event $\bm{x}_{t}$. This can be shown of mathematically with the probability of getting the sequence of images $\bm{x}_{1}, ..., \bm{x}_{T}$ given start image $\bm{x}_{0}$ is:</p>
$$p(\bm{x}_{1}, ..., \bm{x}_{T} | \bm{x}_{0}) = \prod _{t=0}^{T} p(\bm{x}_{t} | \bm{x}_{t-1}).$$<p>For any given time step t, in order to avoid having to iteratively calculate the previous samples from $\bm{x}_{0}$, to $\bm{x}_{t-1}$ to apply the noise to the sample at step $t$, we can instead use the following reparameterization trick. First let $\alpha_{t} = 1 - \beta_{t}$ and $\overline{\alpha} = \prod_{i=1}^{t}\alpha_{i}$. If we define $\bm{\epsilon}_{t-1}$, &hellip; to be random variables from a normal distribution $\mathcal{N}(\bm{0}, \bm{I})$, then we can write $\bm{x}_{t}$ as a function of $\bm{x}_{0}$:</p>
$$\begin{aligned}\bm{x}_t &= \sqrt{\alpha_t} \bm{x}_{t-1} + \sqrt{1 - \alpha_t} \bm{\epsilon}_{t-1} \\ &= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}\bm{x}_{t-2} + \sqrt{1 - \alpha_{t-1} }\bm{\epsilon}_{t-2}) + \sqrt{1 - \alpha_t} \bm{\epsilon}_{t-1} \\ &= \sqrt{\alpha_t \alpha_{t-1}}\bm{x}_{t-2} + \sqrt{\alpha_t (1 - \alpha_{t-1})}\bm{\epsilon}_{t-2} + \sqrt{1 - \alpha_t} \bm{\epsilon}_{t-1} \\ &= \sqrt{\alpha_t \alpha_{t-1}} \bm{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bm{\bar{\epsilon}}_{t-2} \\ &= \cdots \\ &= \sqrt{\bar{\alpha}_t} \bm{x}_0 + \sqrt{1 - \bar{\alpha}_t} \bm{\epsilon}\end{aligned}$$<p>where $\bm{\epsilon}$ merges the $n$ gaussian distributions and the third to fourth line uses the fact that if $X \sim \mathcal{N}(0, \sigma^{2}_{1} \bm{I})$ and $Y \sim \mathcal{N}(0, \sigma^{2}_{2} \bm{I})$, then $X + Y \sim \mathcal{N}(0, (\sigma^{2}_{1} + \sigma^{2}_{2}) \bm{I})$.</p>
<p>Using this formula, we now have a tractable way to apply the noise for the sample at any time step.</p>
<h3 id="backward-diffusion-process"><strong><strong>Backward Diffusion Process</strong></strong></h3>
<p>Now that we know how to add noise to a sample, we need to know how to remove noise from a sample. This process is called the backward diffusion process. The goal of the backward process is to learn the conditional probability distribution of $q(\bm{x}_{t-1}| \bm{x}_{t})$. However, learning $q(\bm{x}_{t-1}| \bm{x}_{t})$ turns out to be intractable (see the definitions section) and needs to be approximated. To show why $q(\bm{x}_{t-1}| \bm{x}_{t})$ is intractable we&rsquo;ll start by writing the backward process using Bayes theorem:</p>
$$q(\bm{x}_{t-1}| \bm{x}_{t}) = \frac{q(\bm{x}_{t}| \bm{x}_{t-1}) q(\bm{x}_{t-1})}{q(\bm{x}_{t})} = \frac{q(\bm{x}_{t}| \bm{x}_{t-1}) q(\bm{x}_{t-1})}{\int q(\bm{x}_{t}|\bm{x}_{t-1})q(\bm{x}_{t-1}) d\bm{x}_{t-1}}.$$<p>Computing the marginal distribution $q(\bm{x}_{t})$ requires integrating over a high dimensional space which is generally intractable and the distribution $q(\bm{x}_{t})$ is often complex and doesn&rsquo;t have a closed form solution.</p>
<p>Consequently, we need to learn a model $p_{\theta}$ to approximate the conditional distribution $q(\bm{x}_{t-1}| \bm{x}_{t})$ to reverse the noise. This model will take two arguments $\bm{x}_{t}$ and $t$. $\theta$ corresponds to the model&rsquo;s parameters the model will learn.</p>
<p>W .Feller proved in his paper on the theory of stochastic processes that for Gaussian and binomial diffusion processes (with small step sizes $\beta$), the reversal of these diffusion process also has an identical form to the forward process. This forms the foundation of our theory here. If the forward process uses a small enough step sizes $\beta_{t}$, then the reverse process $q(\bm{x}_{t-1} \| x_{t})$ also follows a Gaussian distribution. Consequently, we only need to create a model to find the parameters $\mu_{\theta}(x_{T}, t)$ and $\sum_{\theta}(x_{t}, t))$ to estimate the backward diffusion process.</p>
$$p_{\theta}(\bm(x)_{t-1}|\bm{x}_{t}) = \mathcal{N}(\bm{x}_{t-1}|\mu_{\theta}(x_{t}, t), \sum_{\theta}(x_{t},t)).$$<p>There are many different models we could choose to estimate these parameters. In <a href="https://arxiv.org/pdf/1503.03585">J, Sohl-Dickstein</a> first application of diffusion models, he used a MultiLayer Perceptron, but more of the modern methods tend to use a Neural Network inspired by U-Net. Using either of these models, the next steps are finding the correct parameters of our NN which minimise the divergence of $p_{\theta}(\bm{x}_{0})$ from distribution $q(\bm{x}_{0})$. This is done by borrowing some of the ideas around Variational Inference (VI) from Bayesian Statistics.</p>
<p><img src="https://mathstoml.ghost.io/content/images/2024/08/image-4.png" alt=""></p>
<p>Figure 2. The reverse diffusion process. Source: Ho et al. <a href="https://arxiv.org/pdf/2006.11239">https://arxiv.org/pdf/2006.11239</a></p>
<h3 id="variational-inference"><strong><strong>Variational Inference:</strong></strong></h3>
<p>In short, VI revolves around a family of techniques used for approximating intractable integrals arising in Bayesian statistics. The goal of variational inference is to find a way to approximate the posterior probability of unobserved variables by using the Evidence lower bound (ELBO). The ELBO provides us with a lower bound on the loss of our approximate distribution. This lower bound is often known as the variational free energy. We won&rsquo;t delve any more into this lower bound here but more details can be found <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">here</a>. The ELBO inequality applied to our situation gives:</p>
$$\ln p_{\theta}(\bm{x}_{0}) \geq \mathbb{E}_{\bm{x}_{1:T} \sim q(\cdot | \bm{x}_{0})} \left[ \ln p_{\theta} (\bm{x}_{0:T}) - \ln q(\bm{x}_{1:T} | \bm{x}_{0}) \right] .$$<p>Taking expectation wrt to $x_{0}$ gives:</p>
$$\mathbb{E}_{\bm{x}_{0} \sim q} \left[ \ln p_{\theta} (\bm{x}_{0}) \right] \geq \mathbb{E}_{\bm{x}_{0:T} \sim q(\cdot \mid \bm{x}_{0})} \left[ \ln p_{\theta} (\bm{x}_{0:T}) - \ln q(\bm{x}_{1:T} \| \bm{x}_{0}) \right]$$<p>Since the term on the left is fixed, we now have an upper bound. Consequently, we want to maximise the terms on the right by minimising the divergence of $\ln p_{\theta} (\bm{x}_{0:T}) $ from $q(\bm{x}_{1:T} | \bm{x}_{0})$. Using this formula, a loss function can be created by multiplying the formula above by negative 1.</p>
$$L(\theta) = -\mathbb{E}_{\bm{x}_{0:T} \sim q} \left[ \ln p_{\theta} (\bm{x}_{0:T}) - q(\bm{x}_{1:T} | \bm{x}_{0}) \right].$$<p>Minimising this loss function is equivalent to minimising the divergence between the two functions. This can be re-written to:</p>
$$L(\theta) = \sum_{t=1}^{T} \mathbb{E}_{\bm{x}_{t}, \bm{x}_{t-1} \sim q} \left[ -\ln p{\theta} (\bm{x}_{t-1} | \bm{x}_{t}) \right] + \mathbb{E} \left[ D_{\text{KL}} (q(\bm{x}_{T} | \bm{x}_{0}) | p_{\theta} (\bm{x}_{T})) \right].$$<p>Since the term $p_{\theta} (\bm{x}_{T})$ is completely independent of any parameters, this KL divergence term can be ignored. This gives the simpler loss function to be minimised:</p>
$$L(\theta) = \sum_{t=1}^{T} L_{t} \ \text{ where } L_{t} = \mathbb{E}_{\bm{x}_{t-1}, \bm{x}_{t} \sim q} \left[ -\ln p_{\theta} (\bm{x}_{t-1} | \bm{x}_{t}) \right]. $$<p>Some form of the gradient descent algorithm will be applied to this function in hopes to minimise the loss (and consequently be able to identify the noise). Many lines of code are skipped in this proof (for my sanity), however, I&rsquo;d recommend reading <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">this</a> GitHub post which covers the proofs behind the whole process.</p>
<p>Much of this section follows the work from <a href="https://en.wikipedia.org/wiki/Diffusion_model">here</a>.</p>
<h3 id="back-to-the-backward-diffusion-step"><strong><strong>Back to the backward diffusion step</strong></strong></h3>
<p>Now that we know what we are trying to minimise, the next step is determining how we&rsquo;re going to do this.</p>
<p>Deriving a formula for $q(\bm{x}_{t-1} | \bm{x}_{t})$ is impossible because this distribution relies on the value $\bm{x} _{0}$ which is not available during inference. If you think about this intuitively, it will be incredibly difficult to predict the noise of an image without knowing the end image with no noise. However, if we were to condition the distribution on $\bm{x}_{0}$, we will now have a tractable way to predict the noise. Most the proof for the formulas below will be skipped for now but can be found <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">here</a>.</p>
$$q(\bm{x}_{t-1}|\bm{x}_{t}, \bm{x}_{0}) = q(\bm{x}_{t}|\bm{x}_{t-1}, \bm{x}_{0}) \frac{q(\bm{x}_{t-1} | \bm{x}_{0})}{q(\bm{x}_{t} | \bm{x}_{0}) }$$<p>Since each of these distributions on the left can be expressed as a normal distribution, they can be multiplied together creating another normal distribution with parameters $\tilde{\bm{\mu}}_{t}(\bm{x}_{t}, \bm{x}_{0})$ and $\tilde{\beta_{t}}$.</p>
$$\begin{align*} q(\bm{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) &= \mathcal{N}\left(\bm{x}_{t-1}; \tilde{\bm{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t \mathbf{I}\right) \\ \tilde{\beta}_t &= \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t \\ \tilde{\bm{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) &= \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t \end{align*}$$<p>The only issue with conditioning on $\bm{x}_{0}$ is that it defeats the process of generating samples from random noise. When you generate samples in practice you won&rsquo;t have the clean sample $\bm{x}_{0}$ . Consequently, we can&rsquo;t use the true sample of $\bm{x}_{0}$, however it can be estimated. Using the simplified formula from the forward diffusion process we can write $\bm{x}_{0}$ as a function of $\bm{x}_{t}$.</p>
$$\begin{aligned} \bm{x}_{t} &= \sqrt{\bar{\alpha}_t} \bm{x}_{0} + \sqrt{1 - \bar{\alpha}_t} \bm{\epsilon} \\ \bm{x}_{0} &= \frac{\bm{x}_{t} - \sqrt{1 - \bar{\alpha}_t} \bm{\epsilon}}{\sqrt{\bar{\alpha}_t}}. \end{aligned}$$<p>Substituting this estimation of $\bm{x}_{0}$ into the function $\mu$ gives:</p>
$$\tilde{\mu}_t(\mathbf{x}_t) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \bm{\epsilon} \right).$$<p>Next we will write $\bm{\epsilon}$ as a function of $\mathbf{x}_{t}$ and $t$:</p>
$$\tilde{\mu}_{\theta}(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_{\theta}(\mathbf{x}_t, t) \right).$$<p>Substituting these terms into our loss function from earlier. We get:</p>
$$\begin{aligned}L_t &= \mathbb{E}_{\mathbf{x}_0, t, \epsilon} \left[ \frac{1}{2 \|\Sigma_{\theta}(\mathbf{x}_t, t)\|_2^2} \|\tilde{\mu}_t - \mu_{\theta}(\mathbf{x}_t, t)\|_2^2 \right] \\ &= \mathbb{E}_{\mathbf{x}_0, t, \epsilon} \left[ \frac{\beta_t^2}{2 \alpha_t (1 - \bar{\alpha}_t) \|\Sigma_{\theta}\|_2^2} \|\epsilon_t - \epsilon_{\theta}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t)\|_2^2 \right]\end{aligned}$$<p>With these substitutions and rearrangements, we have simplified the problem. Now we only need to estimate the variance term ($\Sigma_{\theta}$) and the noise term ($\bm{\epsilon}$) term for every timestep $t \in \left[1, T\right]$.</p>
<p>These loss functions was further simplified in the paper by <a href="https://arxiv.org/abs/2006.11239">Ho et al.</a>, where he removed the weighting term giving the final loss function:</p>
$$L_{t}^{\text{simple}} = \mathbb{E}_{x_{0},t,\epsilon} \left[ \left| \epsilon - \epsilon_{\theta} \left( \sqrt{\bar{\alpha}t} \bm{x}_{0} + \sqrt{1 - \bar{\alpha}_{t}} \bm{\epsilon}, t \right) \right|^{2} \right].$$<p>Note that of the math in this section follows the website <a href="https://theaisummer.com/diffusion-models/">here</a>.</p>
<h3 id="ddpm-specifics"><strong><strong>DDPM Specifics</strong></strong></h3>
<p>During testing <a href="https://arxiv.org/abs/2006.11239">Ho et al.</a> found that when they tried estimating the variance term $\beta_{t}$, model training became unstable and model sample quality deteriorated. <a href="https://arxiv.org/abs/2006.11239">Ho et al.</a> observed better performance when $\sum_{\theta} (\bm{x}_{t}, t) = \sigma^{2}_{t} \bm{I}$ and $ \sigma^{2} _{t}$ were fixed to the hyper-parameter $\beta_{t}$.</p>
<p>To ensure the reverse process could be modelled as a gaussian distribution <a href="https://arxiv.org/abs/2006.11239">Ho et al.</a> decided to use a linearly increasing step size $\beta_{t}$ from $\beta_{0} = 10^{-4}$ to $\beta_{T} = 0.02$ with $T$ chosen to be 1000 steps. It&rsquo;s worth mentioning many believe these choices of $\beta$ were quite low given the fact that the image pixel values were in the interval $\left[-1, 1\right]$. This parameter was changed on the next iterations of this model.</p>
<p>To estimate the noise $\bm{\epsilon}_{t}$ at each timestep $t \in \left[1, T\right]$ a U-Net inspired architecture from <a href="https://arxiv.org/pdf/1505.04597">Ronneberger et al.</a> was used.</p>
<p><img src="https://mathstoml.ghost.io/content/images/2024/08/image-6.png" alt=""></p>
<p>Figure 3. The U-Net Architecture. Source: <a href="https://arxiv.org/pdf/1505.04597">Ronneberger et al.</a></p>
<p>However, there were a few key differences to the generic U-Net model.</p>
<ul>
<li><strong><strong>Residual Blocks</strong></strong>: The first difference were in the type of convolution blocks used. Unlike typical U-Net blocks which do not incorporate skip-connections, Ho et al. added skip-connections to these blocks. Skip connections are often used to help improve gradient flow and avoid the vanishing or exploding gradient problem.</li>
<li><strong><strong>Attention Mechanisms</strong></strong>: Second, self-attention layers were incorporated into the U-Net. Attention mechanism help the model capture long-term relationships.</li>
<li><strong><strong>Group Normalization</strong></strong>: Instead of Batch Normalization, Group Normalization was used to stabilize the training process. In most situations, Batch Normalisation should be used instead of Group Normalisation. However in the setting where the input data is not IID, then group normalisation is preferred. A common example where the data is not IID occurs when the data comes from different classes.</li>
<li><strong><strong>Sinusoidal Position Embeddings</strong></strong>: Finally a sinusoidal position embedding is used to encode the timesteps. This information is then added to the input of every residual block. This of course takes inspiration from the positional encodings used in Transformers.</li>
</ul>
<h3 id="conclusion"><strong><strong>Conclusion</strong></strong></h3>
<p>This article introduced the idea of diffusion models and delved into their mathematical foundation. The diffusion model DDPM was used as an example.</p>
<h3 id="definitions"><strong><strong>Definitions</strong></strong></h3>
<p><strong><strong>Intractability:</strong></strong> A problem which can be solved in theory (given large enough but finite resources, and time) but requires too many resources to be useful. These group of problems are known as intractable. On contrary, a problem that can be solved in practice is known as a tractable problem.</p>
<p><strong><strong>Latent Variables:</strong></strong> Latent variables are inspired by the latin word &ldquo;lateo&rdquo; which means &ldquo;lie hidden&rdquo;. These are a class of variables that are not directly observed or measured. They are only inferred through the observed variables. It&rsquo;s best thinking of these variables as hidden variables.</p>
<p><strong><strong>References:</strong></strong></p>
<p>Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S. and Edu, S. (n.d.).¬†<strong>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</strong>. [online] Available at: <a href="https://arxiv.org/pdf/1503.03585">https://arxiv.org/pdf/1503.03585</a>.</p>
<p>Ho, J., Jain, A. and Abbeel, P. (2020).¬†<strong>Denoising Diffusion Probabilistic Models</strong>. [online] Available at: c.</p>
<p>Adaloglou, S. K. . N. (2022, September 29). How diffusion models work: the math from scratch | AI Summer.¬†<strong>AI Summer</strong>.¬†https://theaisummer.com/diffusion-models/</p>
<p>Wikipedia contributors. (2024, July 25).¬†<strong>Diffusion model</strong>. Wikipedia.¬†https://en.wikipedia.org/wiki/Diffusion_model</p>
<p>Weng, L. (2021, July 11). What are Diffusion Models?¬†<strong>Lil‚ÄôLog</strong>.¬†https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</p>
<p>‚Äå</p>
<p>‚Äå</p>

        </div>
    </div>
    <div class="text-gray-500 text-lg">
        Page link:&nbsp;<a href="https://www.mathstoml.com/diffusion-models-ddpm/"
            class="border-b border-b-gray-500 hover:text-gray-400">https://www.mathstoml.com/diffusion-models-ddpm/</a>
    </div>
    <div class="my-10 py-5 border-t border-dashed border-t-white/10 text-xl text-gray-600">
        
    </div>
</article>


            </main>

            <aside id="sidebar" class="aside-container">

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z" />
        </svg>
        About
    </div>

    <img src="/image/logo.webp?v=1734785111" class="w-80 self-center" alt="Logo" />

    <p class="leading-8 text-center text-lg font-light mt-3">
        
    </p>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M9 12h3.75M9 15h3.75M9 18h3.75m3 .75H18a2.25 2.25 0 0 0 2.25-2.25V6.108c0-1.135-.845-2.098-1.976-2.192a48.424 48.424 0 0 0-1.123-.08m-5.801 0c-.065.21-.1.433-.1.664 0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75 2.25 2.25 0 0 0-.1-.664m-5.8 0A2.251 2.251 0 0 1 13.5 2.25H15c1.012 0 1.867.668 2.15 1.586m-5.8 0c-.376.023-.75.05-1.124.08C9.095 4.01 8.25 4.973 8.25 6.108V8.25m0 0H4.875c-.621 0-1.125.504-1.125 1.125v11.25c0 .621.504 1.125 1.125 1.125h9.75c.621 0 1.125-.504 1.125-1.125V9.375c0-.621-.504-1.125-1.125-1.125H8.25ZM6.75 12h.008v.008H6.75V12Zm0 3h.008v.008H6.75V15Zm0 3h.008v.008H6.75V18Z" />
        </svg>
        Latest Post
    </div>

    <ul class="text-lg">
        
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/gams/" class="py-5 hover:text-gray-300">GAMs</a>
        </li>
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/the-ant-problem/" class="py-5 hover:text-gray-300">The Ant Problem</a>
        </li>
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/non-convex-optimisation-learning-rate-scheduling/" class="py-5 hover:text-gray-300">Non-Convex Optimisation Learning Rate Scheduling</a>
        </li>
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/martingale-theory/" class="py-5 hover:text-gray-300">Martingale Theory</a>
        </li>
        
        <li class="leading-10 line-clamp-1 mb-3 font-light border-b border-b-white/10 border-dashed">
            <a href="/docker/" class="py-5 hover:text-gray-300">Docker</a>
        </li>
        
        
    </ul>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M2.25 7.125C2.25 6.504 2.754 6 3.375 6h6c.621 0 1.125.504 1.125 1.125v3.75c0 .621-.504 1.125-1.125 1.125h-6a1.125 1.125 0 0 1-1.125-1.125v-3.75ZM14.25 8.625c0-.621.504-1.125 1.125-1.125h5.25c.621 0 1.125.504 1.125 1.125v8.25c0 .621-.504 1.125-1.125 1.125h-5.25a1.125 1.125 0 0 1-1.125-1.125v-8.25ZM3.75 16.125c0-.621.504-1.125 1.125-1.125h5.25c.621 0 1.125.504 1.125 1.125v2.25c0 .621-.504 1.125-1.125 1.125h-5.25a1.125 1.125 0 0 1-1.125-1.125v-2.25Z" />
        </svg>
        Hot Categories
    </div>
    <ul class="leading-10 text-lg">
        
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/graphs/" class="hover:text-gray-300">Graphs <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">4</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/time-series/" class="hover:text-gray-300">Time Series <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/machine-learning/" class="hover:text-gray-300">Machine Learning <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/programming-tools/" class="hover:text-gray-300">Programming Tools <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/probability/" class="hover:text-gray-300">Probability <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/statistical-modelling/" class="hover:text-gray-300">Statistical Modelling <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/statistics/" class="hover:text-gray-300">Statistics <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/trees/" class="hover:text-gray-300">Trees <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/diffusion-models/" class="hover:text-gray-300">Diffusion Models <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-gray-900 border-dashed font-light">
            <a href="/categories/algorithms/" class="hover:text-gray-300">Algorithms <span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        
        
        
        

    </ul>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M9.568 3H5.25A2.25 2.25 0 0 0 3 5.25v4.318c0 .597.237 1.17.659 1.591l9.581 9.581c.699.699 1.78.872 2.607.33a18.095 18.095 0 0 0 5.223-5.223c.542-.827.369-1.908-.33-2.607L11.16 3.66A2.25 2.25 0 0 0 9.568 3Z" />
            <path stroke-linecap="round" stroke-linejoin="round" d="M6 6h.008v.008H6V6Z" />
        </svg>
        Top Tags
    </div>
    <div class="flex flex-wrap gap-2 text-lg leading-8 pt-3 pl-1">
        
        
        
        
        <a href="/tags/statistics/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Statistics&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/machine-learning/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Machine Learning&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/time-series/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Time Series&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/probability/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Probability&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/auto-encoders/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Auto-Encoders&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/gnn/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GNN&nbsp;&nbsp;2</span></a>
        
        
        
        <a href="/tags/docker/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Docker&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/dynamic-programming/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Dynamic Programming&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/algorithms/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Algorithms&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/arma/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">ARMA&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/glms/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GLMs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/arima/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">ARIMA&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/gams/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GAMs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/gnns/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GNNs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/diffusion-models/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Diffusion Models&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/gans/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">GANs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/optimisation/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Optimisation&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/rest-apis/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">REST APIs&nbsp;&nbsp;1</span></a>
        
        
        
        <a href="/tags/trees/"><span
                class="inline-block p-0 bg-gray-800 hover:bg-gray-900 border border-gray-800 text-gray-500 hover:text-gray-300 font-light mb-1 px-5 rounded-full hover:scale-105">Trees&nbsp;&nbsp;1</span></a>
        
        
        
        
    </div>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="m20.25 7.5-.625 10.632a2.25 2.25 0 0 1-2.247 2.118H6.622a2.25 2.25 0 0 1-2.247-2.118L3.75 7.5M10 11.25h4M3.375 7.5h17.25c.621 0 1.125-.504 1.125-1.125v-1.5c0-.621-.504-1.125-1.125-1.125H3.375c-.621 0-1.125.504-1.125 1.125v1.5c0 .621.504 1.125 1.125 1.125Z" />
        </svg>
        Archive Analytics
    </div>
    <ul class="leading-10 text-lg font-light">
        
        
        
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Sep, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Oct, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">3</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Nov, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">4</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Jun, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Jul, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">4</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Jan, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Dec, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">1</span></a>
        </li>
        
        
        
        <li class="mb-1 border-b border-b-white/10 border-dashed">
            <a href="/en/archives">Aug, 2024<span
                    class="ml-2 px-2 bg-gray-800 rounded-full text-gray-500">2</span></a>
        </li>
        
        
        
    </ul>

    
    <div class="aside-section-title">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor"
            class="size-6">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75" />
        </svg>
        Contact
    </div>
    <div class="flex flex-row gap-2">
        EmailÔºöbenbradshaw01@outlook.com
    </div>
</aside>
        </div>

        <footer class="p-5 text-xl text-center mt-8 pt-8 pb-8 border-t border-gray-100/10">
    <div class="text-gray-500">
        
        &#xA9; 2019 - 2023 by <a class="hover:text-gray-100" href="https://github.com/guangmean/Niello"
            target="_blank">guangmean</a>. All Rights
        Reserved.
        

        
        | <a class="hover:text-gray-100" href=" /en ">üá¨üáßEN</a>
        
    </div>
</footer>

        <div class="cookie-container text-center py-12 text-2xl font-thin text-gray-500">
  <p>
    We use cookies on this website to give you the best experience on our
    site and show you relevant ads. To find out more, read our
    <a href="/privacy/" class="text-red-600">privacy policy</a> and <a href="/cookies/" class="text-red-600">cookie
      policy</a>.
  </p>
  <button class="cookie-btn w-32 h-22 mt-5 py-2 bg-red-600 text-white rounded-full hover:scale-110">
    Okay
  </button>
</div>
<script src="/js/cookie.js"></script>

    </div>
</body>

</html>